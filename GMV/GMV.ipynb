{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GMV.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[{"file_id":"1dmHfrpWA_YW9-x0kQneHUKv65z1lqRo0","timestamp":1530693433814}],"collapsed_sections":["Xm4VhoMbZa7Q","-EsJLc3_aEHo","AJkKD7_2cCwy","HLEukOO9cJsQ","fKqMxZ1h9WNK","36nQ6mxG_WxC","OacrptOJT4Fu","IdxkPymwI5To","OD8TXyTQcide","FxDGailmlJWY","iIQKp985kNRq","X-84L8bUkTtk","La8b6sTKPmIg","4OvRGNL9ccAu","RX3ksEp5ZmFs","WvxE2WOHMZN6"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"0gnZNAum_AWK","colab_type":"text"},"cell_type":"markdown","source":["# Importations et constantes"]},{"metadata":{"id":"Xm4VhoMbZa7Q","colab_type":"text"},"cell_type":"markdown","source":["## Installations et importations des modules"]},{"metadata":{"id":"BB5shcgzZHQW","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["!pip install -q keras\n","\n","!pip install -q tqdm\n","from tqdm import tqdm\n","from tqdm import tqdm_notebook\n","\n","import random as rd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","!pip install -q panda\n","import pandas as pd\n","!pip install -q openpyxl\n","import tensorflow as tf\n","import timeit\n","from __future__ import print_function\n","from PIL import Image"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Bh8RTpLJaUx0","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":36},"outputId":"45b843f0-90d6-4cca-c6e3-51cd4bfde8d4","executionInfo":{"status":"ok","timestamp":1530688878740,"user_tz":-120,"elapsed":968,"user":{"displayName":"Camille Méketyn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"116567900876735498800"}}},"cell_type":"code","source":["#KERAS\n","import keras\n","from keras import activations, initializers, regularizers, constraints,metrics\n","from keras.legacy import interfaces\n","from keras.engine import InputSpec, Layer\n","from keras.layers import Input, BatchNormalization, concatenate, Reshape, Conv2DTranspose, Activation\n","from keras.layers.core import Reshape, Dense, Dropout, Flatten\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras.layers.convolutional import Conv2D, UpSampling2D, MaxPooling2D\n","from keras.models import Model, Sequential\n","from keras.datasets import mnist\n","from keras.optimizers import Adam, Adagrad\n","from keras import backend as K\n","K.set_image_dim_ordering('tf')\n","from keras.utils import np_utils\n","from keras.utils.generic_utils import func_dump, func_load, deserialize_keras_object\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"XS9qNAC9aR4Q","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  print('GPU device not found')\n","else:\n","  print('Found GPU at: {}'.format(device_name))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VpxUqvzILagI","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2_q_5IK2NJLK","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["!mkdir -p drive -v\n","!google-drive-ocamlfuse drive"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lWaZc1Kz9wLi","colab_type":"text"},"cell_type":"markdown","source":["## Constantes"]},{"metadata":{"id":"slyJEx5M9wi8","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#Bases de donnees :\n","#MNIST\n","nb_classes_MNIST = 10\n","dimensions_MNIST = (28,28,1)\n","#3Dchairs\n","nb_classes_3Dchairs = 1393\n","nb_images_par_classes_3Dchairs = 62\n","dimensions_3Dchairs = (64,64,3)\n","\n","randomVectorSizeC = 10\n","randomVectorSize = 15\n","\n","adagrad_1 = Adagrad(lr=0.001, epsilon=None, decay=0.0)\n","adagrad_2 = Adagrad(lr=0.001, epsilon=None, decay=0.0)\n","\n","adam_1 = Adam(lr=0.001, beta_1=0.5)  #par defaut pour le generateur et le gan\n","adam_2 = Adam(lr=0.00005, beta_1=0.5)  #par defaut pour le discriminateur\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-EsJLc3_aEHo","colab_type":"text"},"cell_type":"markdown","source":["## Importations des bases de données"]},{"metadata":{"id":"6TmBVXbZaEb6","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#MNIST\n","\n","from keras.datasets import mnist\n","# input image dimensions\n","img_rows, img_cols = 28, 28\n","nb_classes_MNIST =10\n","\n","##### Chargement des donnees\n","\n","# the data, shuffled and split between train and test sets\n","(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n","\n","print (K.image_dim_ordering())\n","\n","if K.image_dim_ordering() == 'th':\n","    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n","    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n","    input_shape = (1, img_rows, img_cols)\n","else:\n","    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n","    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n","    input_shape = (img_rows, img_cols, 1)\n","\n","\n","X_train = X_train.astype('float32')\n","X_test = X_test.astype('float32')\n","X_train = X_train /127.5 -1\n","X_test = X_test /127.5 -1\n","print('X_train shape:', X_train.shape)\n","print('X_test shape:', X_test.shape)\n","print(X_train.shape[0], 'train samples')\n","print(X_test.shape[0], 'test samples')\n","\n","# convert class vectors to binary class matrices\n","y_train = np_utils.to_categorical(Y_train, nb_classes_MNIST)\n","y_test = np_utils.to_categorical(Y_test, nb_classes_MNIST)\n","print('y_train shape:', y_train.shape)\n","print('y_test shape:', y_test.shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AJkKD7_2cCwy","colab_type":"text"},"cell_type":"markdown","source":["# Fonctions"]},{"metadata":{"id":"HLEukOO9cJsQ","colab_type":"text"},"cell_type":"markdown","source":["## Base de données"]},{"metadata":{"id":"K1RUcUPXO1_I","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def faire_des_paires(bdd, new=True, nb_paires=0, X_train_2=[], X_train=X_train, Y_train=Y_train, test=False):\n","    \n","  if bdd == \"MNIST\":\n","    global nb_classes_MNIST\n","    nb_classes = nb_classes_MNIST\n","    global dimensions_MNIST\n","    dim = dimensions_MNIST\n","    if test:\n","      global X_test\n","      X_train = X_test\n","      global Y_test\n","      Y_train = Y_test\n","    X = []\n","    for n in range(nb_classes):\n","      X_0=X_train[np.where(Y_train==n)]\n","      X.append(X_0)\n","\n","    if new:\n","      X_train_2 = np.zeros((nb_paires, 2, dim[0], dim[1], dim[2]))\n","    nb_paires = len(X_train_2)\n","    nb_paires_par_classes = int(nb_paires / nb_classes)\n","    for i in range(nb_paires_par_classes):\n","      for j in range(nb_classes):\n","        k1=rd.randint(0,len(X[j])-1)\n","        k2=rd.randint(0,len(X[j])-1)\n","        while k1==k2:\n","          k2=rd.randint(0,len(X[j])-1)\n","        img1=X[j][k1]\n","        img2=X[j][k2]\n","        X_train_2[nb_classes*i+j][0] = img1\n","        X_train_2[nb_classes*i+j][1] = img2\n","  \n","  elif bdd == \"3Dchairs\":\n","    global nb_classes_3Dchairs\n","    nb_classes = nb_classes_3Dchairs\n","    global nb_images_par_classes_3Dchairs\n","    nb_images_par_classes = nb_images_par_classes_3Dchairs\n","    global dimensions_3Dchairs\n","    dim = dimensions_3Dchairs\n","    if test:\n","      print(\"Pas de bdd de test pour 3Dchairs\")\n","      print(\"Une bdd d'entrainement va etre generee\")\n","    if new:\n","      X_train_2 = np.zeros((nb_paires, 2, dim[0], dim[1], dim[2]))\n","    path = \"drive/\"+bdd+\"/chair\"\n","    nb_paires = len(X_train_2)\n","    nb_folders = nb_classes\n","    nb_paires_par_classes = int(nb_paires / nb_classes)\n","    if nb_paires < nb_folders:\n","      nb_folders = nb_paires\n","      nb_paires_par_classes = 1\n","    f = 0\n","    c = 0\n","    for _ in tqdm(range(nb_folders)):\n","      for im in range(nb_paires_par_classes):\n","        k1=np.random.randint(0,nb_images_par_classes-1)\n","        k2=np.random.randint(0,nb_images_par_classes-1)\n","        while k1==k2:\n","          k2=rd.randint(0,nb_images_par_classes-1)\n","\n","        name = path+str(f)+\"/\"\n","        name1 = name+str(k1)+\".png\"\n","        name2 = name+str(k2)+\".png\"\n","        X_train_2[nb_paires_par_classes*c+im] = [np.asarray(Image.open(name1)), np.asarray(Image.open(name2))]\n","      c += 1\n","      if nb_folders < nb_classes:\n","        f = np.random.randint(0, nb_classes-1)\n","      else: f = f + 1\n","    X_train_2 = (X_train_2 / float(127.5)) - 1 #pour normer entre -1 et 1\n","        \n","  #np.random.shuffle(X_train_2)\n","  return(X_train_2)\n","\n","def bdd_pandas(bdd, nb_classes_par_panel):\n","  if bdd == \"3Dchairs\":\n","    \n","    #constantes\n","    global nb_classes_3Dchairs\n","    nb_classes = nb_classes_3Dchairs\n","    global nb_images_par_classes_3Dchairs\n","    nb_images_par_classes = nb_images_par_classes_3Dchairs\n","    global dimensions_3Dchairs\n","    dim = dimensions_3Dchairs\n","\n","    nb_de_panels = int(nb_classes/float(nb_classes_par_panel))\n","    reste = nb_classes - nb_de_panels*nb_classes_par_panel\n","    if reste != 0:\n","      nb_de_panels +=1\n","      \n","    nb_total_images_par_panel = nb_images_par_classes*nb_classes_par_panel\n","    \n","    X_train = []\n","    \n","    #initialisation d'un panel\n","    data = np.random.rand(nb_total_images_par_panel*dim[2], dim[0], dim[1])\n","    p = pd.Panel(data)\n","    \n","    #pour chaque panel\n","    for j in range(nb_de_panels):\n","      print(\"Images de chair\"+str(j*nb_classes_par_panel)+\" a chair\"+str(nb_classes_par_panel*(j+1)-1))\n","      \n","      if j == nb_de_panels - 1 and reste != 0:\n","        #pour chaque classe du panel\n","        i=0\n","        for _ in tqdm(range(reste)):\n","          path = \"drive/3Dchairs/chair\"+str(nb_classes_par_panel*j+i)+\"/\"\n","\n","          #pour chaque image de la classe\n","          for k in range(nb_images_par_classes):\n","            name = path + str(k) + \".png\"\n","            img = np.asarray(Image.open(name))\n","            img = img /127.5 -1\n","\n","            #copie de tous les canaux de l'image\n","            for a in range(dim[2]):\n","              p[nb_images_par_classes*i+dim[2]*k+a] = img[:,:,a]\n","          i+=1\n","          \n","      else:\n","        #pour chaque classe du panel\n","        i=0\n","        for _ in tqdm(range(nb_classes_par_panel)):\n","          path = \"drive/3Dchairs/chair\"+str(nb_classes_par_panel*j+i)+\"/\"\n","\n","          #pour chaque image de la classe\n","          for k in range(nb_images_par_classes):\n","            name = path + str(k) + \".png\"\n","            img = np.asarray(Image.open(name))\n","            img = img /127.5 -1\n","\n","            #copie de tous les canaux de l'image\n","            for a in range(dim[2]):\n","              p[nb_images_par_classes*i+dim[2]*k+a] = img[:,:,a]\n","          i+=1\n","      X_train.append(p)\n","    return(X_train)\n","        "],"execution_count":0,"outputs":[]},{"metadata":{"id":"fKqMxZ1h9WNK","colab_type":"text"},"cell_type":"markdown","source":["## Modèles"]},{"metadata":{"id":"3BxuX73n0ini","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def create_model(model, bdd, opt_G = adam_1, opt_D = adam_2, opt_P = adam_3, nb_sorties = 1, loss = 'mse'):\n","  #model=\"GMV\" ou \"CGMV\" ; nb_sorties=1 ou 2\n","  \n","  if bdd == \"MNIST\":\n","    if model == \"GMV\" or model == \"CGMV\":\n","      # Generator\n","      g_input_c = Input(shape=(randomVectorSize,),name=\"g_input_c\")\n","      g_input_v = Input(shape=(randomVectorSize,),name=\"g_input_v\")\n","      x = concatenate([g_input_c,g_input_v])\n","      x = Dense(7*7*128) (x)\n","      x = Reshape((7,7,128)) (x)\n","      x = Conv2DTranspose(64,kernel_size=(5,5),strides=(2,2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (x)\n","      #x = BatchNormalization() (x)\n","      x = Activation('relu') (x)\n","      #x = Dropout(0.3) (x)\n","      x = Conv2DTranspose(1,kernel_size=(5,5), strides=(2,2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (x)\n","      #x = BatchNormalization() (x)\n","      g_prediction = Activation('tanh') (x)\n","\n","      generator = Model(input = [g_input_c,g_input_v], output = g_prediction)\n","      generator.compile(optimizer=opt_G, loss=loss)\n","\n","      #Discriminateur    \n","      d_input_1 = Input(shape=(28,28,1),name=\"d_input_1\")\n","      d_input_2 = Input(shape=(28,28,1),name=\"d_input_2\")\n","      x = concatenate([d_input_1, d_input_2])\n","      x = Reshape((28,28,2)) (x)\n","      x = Conv2D(64, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (x)\n","      x = LeakyReLU(0.2)                                                 (x)\n","      #x = Dropout(0.3)                                                   (x)\n","      x = Conv2D(128, kernel_size=(4, 4), strides=(2, 2), padding='same')(x)\n","      #x = BatchNormalization()                                           (x)\n","      x = LeakyReLU(0.2)                                                 (x)\n","      #x = Conv2D(256, kernel_size=(4, 4), strides=(2, 2), padding='same')(x)\n","      #x = BatchNormalization()                                           (x)\n","      #x = LeakyReLU(0.2)                                                 (x)\n","      #x = Dropout(0.3)                                                   (x)\n","      x = Flatten()                                                      (x)\n","      d_prediction = Dense(nb_sorties, activation='sigmoid', name='d_output')     (x)\n","\n","      discriminator = Model(input = [d_input_1,d_input_2], output = d_prediction)\n","      discriminator.compile(optimizer=opt_D, loss=loss)\n","\n","      if model == \"GMV\":\n","\n","        #GAN\n","        discriminator.trainable = False\n","        gan_input_c = Input(shape=(randomVectorSize,))\n","        gan_input_v1 = Input(shape=(randomVectorSize,))\n","        gan_input_v2 = Input(shape=(randomVectorSize,))\n","        x = generator ([gan_input_c, gan_input_v1])\n","        y = generator ([gan_input_c, gan_input_v2])\n","        gan_prediction = discriminator ([x, y])\n","\n","        gan = Model(input = [gan_input_c, gan_input_v1, gan_input_v2], output = gan_prediction)\n","        gan.compile(optimizer=opt_G, loss=loss)\n","\n","      elif model == \"CGMV\":\n","\n","        #Encodeur\n","        e_input = Input(shape=(28,28,1),name=\"e_input\")\n","        x = Conv2D(32, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (e_input)\n","        x = LeakyReLU(0.2)                                                 (x)\n","        x = Conv2D(64, kernel_size=(4, 4), strides=(2, 2), padding='same')(x)\n","        x = LeakyReLU(0.2)                                                 (x)\n","        x = Flatten()                                                      (x)\n","        e_prediction = Dense(randomVectorSize, name='e_output')     (x)\n","\n","        encodeur = Model(input = e_input, output = e_prediction)\n","        encodeur.compile(optimizer=opt_G, loss=loss)\n","\n","        #GAN CGMV (Encodeur -> Generateur -> Discriminateur)\n","\n","        discriminator.trainable = False\n","\n","        gan_input_x1 = Input(shape=(28,28,1)) #image qui est utilisee deux fois par le generateur\n","        gan_input_x2 = Input(shape=(28,28,1)) #image dont une autre vue sera generee\n","        gan_input_v1 = Input(shape=(randomVectorSize,))\n","        gan_input_v2 = Input(shape=(randomVectorSize,))\n","        gan_input_v3 = Input(shape=(randomVectorSize,))\n","\n","        c1 = encodeur(gan_input_x1)\n","        c2 = encodeur(gan_input_x2)\n","\n","        x = generator ([c1, gan_input_v1])\n","        y = generator ([c1, gan_input_v2])\n","        z = generator ([c2, gan_input_v3])\n","\n","        gan_prediction_1 = discriminator ([x, y])\n","        gan_prediction_2 = discriminator ([gan_input_x2, z])\n","\n","        gan = Model(input = [gan_input_x1, gan_input_x2, gan_input_v1, gan_input_v2, gan_input_v3], output = [gan_prediction_1, gan_prediction_2])\n","        gan.compile(optimizer=opt_G, loss=loss)\n","        \n","        \n","    elif model == \"AECP\":\n","      \n","      #Encodeur\n","\n","      e_input = Input(shape=(28,28,1),name=\"e_input\")\n","      x = Conv2D(32, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (e_input)\n","      x = LeakyReLU(0.2)                                                 (x)\n","      x = Conv2D(64, kernel_size=(4, 4), strides=(2, 2), padding='same') (x)\n","      x = LeakyReLU(0.2)                                                 (x)\n","      x = Flatten()                                           (x)\n","      e_prediction = Dense(randomVectorSizeC+randomVectorSize) (x)\n","\n","      encodeur = Model(input = e_input, output = e_prediction)\n","      encodeur.compile(optimizer=opt_G, loss=loss)\n","      \n","      #Vector_c\n","      \n","      vector_c_input = Input(shape=(randomVectorSizeC+randomVectorSize,), name=\"vector_c_input\")\n","      vector_c_output = Dense(randomVectorSizeC, name=\"vector_c_output\") (vector_c_input)  #activation=\"sigmoid\"\n","      \n","      vector_c = Model(input = vector_c_input, output = vector_c_output)\n","      vector_c.compile(optimizer=opt_G, loss=loss)\n","      \n","      #Vector_v\n","      \n","      vector_v_input = Input(shape=(randomVectorSizeC+randomVectorSize,), name=\"vector_v_input\")\n","      vector_v_output = Dense(randomVectorSize, name=\"vector_v_output\") (vector_v_input)  #activation=\"sigmoid\"\n","      \n","      vector_v = Model(input = vector_v_input, output = vector_v_output)\n","      vector_v.compile(optimizer=opt_G, loss=loss)\n","      \n","      \n","      #Generateur\n","\n","      g_input_c = Input(shape=(randomVectorSizeC,),name=\"g_input_c\")\n","      g_input_v = Input(shape=(randomVectorSize,),name=\"g_input_v\")\n","      x  = Reshape((randomVectorSizeC,)) (g_input_c)\n","      y = Reshape((randomVectorSize,)) (g_input_v)\n","      x = concatenate([x,y])\n","      x = Dense(7*7*128) (x)\n","      x = Reshape((7,7,128)) (x)\n","      x = Conv2DTranspose(64,kernel_size=(5,5),strides=(2,2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (x)\n","      x = Activation('relu') (x)\n","      x = Conv2DTranspose(1,kernel_size=(5,5), strides=(2,2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (x)\n","      g_prediction = Activation('tanh') (x)\n","\n","      generator = Model(input = [g_input_c, g_input_v], output = g_prediction)\n","      generator.compile(optimizer=opt_G, loss=loss)\n","      \n","      #Predicteur\n","\n","      p_input = Input(shape=(28,28,1),name=\"p_input\")\n","      x = Conv2D(32, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (p_input)\n","      x = LeakyReLU(0.2)                                                 (x)\n","      x = Conv2D(64, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (x)\n","      x = LeakyReLU(0.2)                                                 (x)\n","      x = Flatten()                                                      (x)\n","      p_prediction = Dense(randomVectorSize, name='p_prediction')        (x)\n","\n","      predicteur = Model(input = p_input, output = p_prediction)\n","      predicteur.compile(optimizer=opt_P, loss=loss)\n","      \n","      #Classifieur\n","\n","      c_input = Input(shape=(28,28,1), name='c_input')\n","      x = Conv2D(32, kernel_size=(3,3), kernel_initializer=initializers.RandomNormal(stddev=0.02))(c_input)\n","      x = Activation('relu')                                        (x)\n","      x = MaxPooling2D(pool_size=(2,2))                             (x)\n","      x = Conv2D(32, kernel_size=(3,3), kernel_initializer=initializers.RandomNormal(stddev=0.02))(x)\n","      x = Activation('relu')                                        (x)\n","      x = MaxPooling2D(pool_size=(2,2))                             (x)\n","      x = Dropout(0.3)                                              (x)\n","      x = Flatten()                                                 (x)\n","      x = Dense(100)                                                (x)\n","      x = Activation('relu')                                        (x)\n","      x = Dropout(0.3)                                              (x)\n","      x = Dense(10)                                                 (x)\n","      c_prediction = Activation('softmax', name='c_prediction')     (x)\n","\n","      classifieur = Model(input = c_input, output = c_prediction)\n","      classifieur.compile(optimizer=opt_D, loss=loss)\n","      \n","      #Discriminateur  \n","      \n","      d_input_x = Input(shape=(28,28,1),name=\"d_input_x\")\n","      d_input_class = Input(shape=(28,28,10),name=\"d_input_class\")\n","      x = concatenate([d_input_x, d_input_class])\n","      x = Conv2D(64, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (x)\n","      x = LeakyReLU(0.2)                                                 (x)\n","      x = Conv2D(128, kernel_size=(4, 4), strides=(2, 2), padding='same')(x)\n","      x = LeakyReLU(0.2)                                                 (x)\n","      x = Flatten()                                                      (x)\n","      d_prediction = Dense(1, activation='sigmoid', name='d_output')     (x)\n","\n","      discriminator = Model(input = [d_input_x,d_input_class], output = d_prediction)\n","      discriminator.compile(optimizer=opt_D, loss=loss)\n","      \n","      \n","      #Auto-encodeur AE\n","            \n","      AE_input_x = Input(shape=(28,28,1), name='AE_input_x')\n","      \n","      CV = encodeur(AE_input_x) \n","      C = vector_c(CV)\n","      V = vector_v(CV)\n","\n","      AE_prediction = generator([C, V])\n","\n","      AE = Model(input = AE_input_x, output = AE_prediction)\n","      AE.compile(optimizer=opt_G, loss=loss)\n","      \n","      #Auto-encodeur + Classifieur AEC\n","      \n","      discriminator.trainable = False\n","      #encodeur.trainable = False\n","      #vector_c.trainable = False\n","      \n","      AEC_input_x = Input(shape=(28,28,1), name='AEC_input_x')           #image\n","      AEC_input_v = Input(shape=(randomVectorSize,), name='AEC_input_v') #bruit\n","      AEC_input_class = Input(shape=(28,28,10), name='AEC_input_class')\n","      \n","      CV = encodeur(AEC_input_x)\n","      C = vector_c(CV)\n","      img_generees = generator([C, AEC_input_v])\n","      AEC_prediction = discriminator([img_generees, AEC_input_class])\n","      \n","      AEC = Model(input = [AEC_input_x, AEC_input_v, AEC_input_class], output = AEC_prediction)\n","      AEC.compile(optimizer=opt_G, loss=loss)\n","      \n","      #Auto-encodeur + Predicteur AEP\n","      #encodeur.trainable = False\n","      #vector_c.trainable = False\n","      predicteur.trainable = False\n","      AEP_input_x = Input(shape=(28,28,1), name='AEP_input_x')             #image\n","      AEP_input_v = Input(shape=(randomVectorSize,), name='AEP_input_v')   #bruit\n","      CV = encodeur(AEP_input_x)\n","      C = vector_c(CV)\n","      y = generator([C, AEP_input_v])\n","      AEP_prediction = predicteur(y)\n","\n","      AEP = Model(input = [AEP_input_x, AEP_input_v], output = AEP_prediction)\n","      AEP.compile(optimizer=opt_G, loss=loss)\n","      \n","      print(\"Modele \"+model+\" genere pour \"+bdd)\n","      print(\"list_models = (encodeur, vector_c, vector_v, generator, discriminator, predicteur, AE, AEC, AEP)\")\n","      return(encodeur, vector_c, vector_v, generator, discriminator, predicteur, AE, AEC, AEP)\n","      \n","      \n","  elif bdd == \"3Dchairs\":\n","    \n","    # Generator\n","\n","    g_input_c = Input(shape=(randomVectorSize,),name=\"g_input_c\")\n","    g_input_v = Input(shape=(randomVectorSize,),name=\"g_input_v\")\n","    x = concatenate([g_input_c,g_input_v])\n","    x = Dense(4*4*400) (x)\n","    x = Reshape((4,4,400)) (x)\n","    x = Conv2DTranspose(250,kernel_size=(4,4),strides=(2,2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (x)\n","    x = BatchNormalization() (x)\n","    x = Activation('relu') (x)\n","    #x = Dropout(0.3) (x)\n","    x = Conv2DTranspose(125,kernel_size=(4,4), strides=(2,2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (x)\n","    x = BatchNormalization() (x)\n","    x = Activation('relu') (x)\n","    x = Conv2DTranspose(60,kernel_size=(4,4), strides=(2,2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (x)\n","    x = BatchNormalization() (x)\n","    x = Activation('relu') (x)\n","    x = Conv2DTranspose(3,kernel_size=(4,4), strides=(2,2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (x)\n","    g_prediction = Activation('tanh') (x)\n","\n","    generator = Model(input = [g_input_c,g_input_v], output = g_prediction)\n","    generator.compile(optimizer=opt_G, loss=loss)\n","    \n","    # Discriminator\n","\n","    d_input_1 = Input(shape=(64,64,3),name=\"d_input_1\")\n","    d_input_2 = Input(shape=(64,64,3),name=\"d_input_2\")\n","    x = concatenate([d_input_1, d_input_2])\n","    x = Reshape((64,64,6)) (x)\n","    x = Conv2D(64, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (x)\n","    x = LeakyReLU(0.2)                                                 (x)\n","    #x = Dropout(0.3)                                                   (x)\n","    x = Conv2D(128, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02))(x)\n","    x = BatchNormalization()                                           (x)\n","    x = LeakyReLU(0.2)                                                 (x)\n","    x = Conv2D(256, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02))(x)\n","    x = BatchNormalization()                                           (x)\n","    x = LeakyReLU(0.2)                                                 (x)\n","    x = Conv2D(512, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02))(x)\n","    x = BatchNormalization()                                           (x)\n","    #x = Dropout(0.3)                                                   (x)\n","    x = Flatten()                                                      (x)\n","    d_prediction = Dense(nb_sorties, activation='sigmoid', name='d_output')     (x)\n","\n","    discriminator = Model(input = [d_input_1,d_input_2], output = d_prediction)\n","    discriminator.compile(optimizer=opt_D, loss=loss)\n","    \n","    if model == \"GMV\":\n","\n","      #GAN\n","\n","      discriminator.trainable = False\n","      gan_input_c = Input(shape=(randomVectorSize,))\n","      gan_input_v1 = Input(shape=(randomVectorSize,))\n","      gan_input_v2 = Input(shape=(randomVectorSize,))\n","      x = generator ([gan_input_c, gan_input_v1])\n","      y = generator ([gan_input_c, gan_input_v2])\n","      gan_prediction = discriminator ([x, y])\n","\n","      gan = Model(input = [gan_input_c, gan_input_v1, gan_input_v2], output = gan_prediction)\n","      gan.compile(optimizer=opt_G, loss=loss)\n","      \n","    elif model == \"CGMV\":\n","      print(\"Modele pas implemente:(\")\n","      #Encodeur\n","      \n","      \n","      #GAN\n","      \n","      \n","  print(\"Modele \"+model+\" genere pour \"+bdd)\n","  if model == \"GMV\":\n","    return(generator, discriminator, gan)\n","  elif model == \"CGMV\":\n","    return(encodeur, generator, discriminator, gan)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"36nQ6mxG_WxC","colab_type":"text"},"cell_type":"markdown","source":["## Entraînement"]},{"metadata":{"id":"_NvzuwZh_iCM","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def train(model, bdd, list_models, X_train_2=[], X_train=[], y_train=[], epochs=10, batchSize=128, epochs_regen=5, data_discriminator=False, data_predicteur=False, nb_sorties=1):\n","  if model == \"GMV\":\n","    return(train_GMV(bdd, list_models, X_train_2, epochs=epochs, batchSize=batchSize, epochs_regen=epochs_regen, data_discriminator=data_discriminator, nb_sorties=nb_sorties))\n","  elif model == \"CGMV\":\n","    return(train_CGMV(bdd, list_models, X_train_2, epochs=epochs, batchSize=batchSize, epochs_regen=epochs_regen, data_discriminator=data_discriminator))\n","  elif model == \"AECP\":\n","    return(train_AECP(bdd, list_models, X_train=X_train, y_train=y_train, epochs=epochs, batchSize=batchSize, data_discriminator=data_discriminator, data_predicteur=data_predicteur))\n","\n","def train_GMV(bdd, list_models, X_train_2, epochs=100, batchSize=128, epochs_regen=5, data_discriminator=False, nb_sorties=1):\n","  \n","  generator, discriminator, gan = list_models\n","  if bdd == \"MNIST\":\n","    global dimensions_MNIST\n","    dim = dimensions_MNIST\n","  elif bdd == \"3Dchairs\":\n","    global dimensions_3Dchairs\n","    dim = dimensions_3Dchairs\n","  \n","  batchCount = int(len(X_train_2) / batchSize)\n","  print ('Epochs :', epochs)\n","  print ('Batch size :', batchSize)\n","  print ('Batches per epoch :', batchCount)\n","  d_losses = []\n","  g_losses = []\n","  if data_discriminator:\n","    d_moy_pos=[]\n","    d_moy_neg=[]\n","  for e in range(1, epochs+1):\n","    if e%epochs_regen==0:\n","      print(\"Regeneration de la base de donnees...\")\n","      X_train_2 = faire_des_paires(bdd, new=False, X_train_2=X_train_2)\n","        \n","    print ('\\n','-'*15, 'Epoch %d' % e, '-'*15)\n","    dloss=0\n","    gloss=0\n","\n","    for _ in tqdm(range(batchCount)):\n","      # DISCRIMINATOR\n","      #generation des vecteurs de bruit\n","      vector_c = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","      vector_v1 = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","      vector_v2 = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","      \n","      #passage dans le generateur\n","      generatedImages_1 = generator.predict([vector_c,vector_v1])\n","      generatedImages_2 = generator.predict([vector_c,vector_v2])\n","      \n","      #entraînement du discriminateur\n","      indices=np.random.randint(0, len(X_train_2), size=batchSize)\n","      imageBatch_1 = np.zeros((len(indices),dim[0],dim[1],dim[2]))\n","      imageBatch_2 = np.zeros((len(indices),dim[0],dim[1],dim[2]))\n","      for i in range(len(indices)):\n","        imageBatch_1[i]=X_train_2[indices[i]][0]\n","        imageBatch_2[i]=X_train_2[indices[i]][1]\n","      XDis1=np.concatenate((imageBatch_1,generatedImages_1))\n","      XDis2=np.concatenate((imageBatch_2,generatedImages_2))\n","      if nb_sorties==1:\n","        yDis = np.zeros(2*batchSize)\n","        yDis[:batchSize] = 0.9\n","        yDis[batchSize:] = 0.1\n","      elif nb_sorties==2:\n","        yDis = np.zeros((2*batchSize,2))\n","        yDis[:batchSize] = [1,0]\n","        yDis[batchSize:]=[0,1]\n","      \n","      if data_discriminator:\n","        #Recuperation des valeurs des predictions du discriminateur\n","        predictions=discriminator.predict([XDis1, XDis2])\n","        if nb_sorties==1:\n","          moy_pos=0\n","          moy_neg=0\n","          for z in range(len(predictions)):\n","            if z < batchSize:\n","              moy_pos +=predictions[z]\n","            else:\n","              moy_neg +=predictions[z]\n","\n","          moy_pos=2*float(moy_pos)/len(predictions)\n","          moy_neg=2*float(moy_neg)/len(predictions)\n","        elif nb_sorties==2:\n","          moy_pos=[0,0]\n","          moy_neg=[0,0]\n","          for z in range(len(predictions)):\n","            if z < batchSize:\n","              moy_pos[0]+=predictions[z][0]\n","              moy_pos[1]+=predictions[z][1]\n","            else:\n","              moy_neg[0]+=predictions[z][0]\n","              moy_neg[1]+=predictions[z][1]\n","\n","          moy_pos[0]=2*float(moy_pos[0])/len(predictions)\n","          moy_pos[1]=2*float(moy_pos[1])/len(predictions)\n","          moy_neg[0]=2*float(moy_neg[0])/len(predictions)\n","          moy_neg[1]=2*float(moy_neg[1])/len(predictions)\n","        d_moy_pos.append(moy_pos)\n","        d_moy_neg.append(moy_neg)\n","        \n","      dloss += discriminator.train_on_batch([XDis1,XDis2], yDis)\n","\n","      # GENERATOR\n","      vector_c = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","      vector_v1 = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","      vector_v2 = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","      if nb_sorties==1:\n","        yGen=np.zeros(batchSize)\n","        yGen[:]=0.9\n","      elif nb_sorties==2:\n","        yGen = np.zeros((batchSize,2))\n","        yGen[:]=[1,0]\n","      discriminator.trainable = False\n","      gloss += gan.train_on_batch([vector_c,vector_v1,vector_v2], yGen)\n","    \n","    \n","    # TESTS ET EVALUATIONS\n","    d_losses.append(dloss)\n","    g_losses.append(gloss)\n","    \n","    print (\"Losses : DISCRIMINATOR \", d_losses[-1],\" GENERATOR \", g_losses[-1])\n","    \n","  if data_discriminator:\n","    return(g_losses,d_losses,d_moy_pos,d_moy_neg)\n","  else:\n","    return(g_losses, d_losses)\n","\n","  \n","def train_CGMV(bdd, list_models, X_train_2, epochs=10, batchSize=128, epochs_regen=5, data_discriminator=False):\n","  \n","  encodeur, generator, discriminator, gan = list_models\n","  \n","  if bdd == \"MNIST\":\n","    global dimensions_MNIST\n","    dim = dimensions_MNIST\n","  elif bdd == \"3Dchairs\":\n","    global dimensions_3Dchairs\n","    dim = dimensions_3Dchairs\n","    \n","  batchCount = int(len(X_train_2) / batchSize)\n","  print ('Epochs :', epochs)\n","  print ('Batch size :', batchSize)\n","  print ('Batches per epoch :', batchCount)\n","  d_losses = []\n","  g_losses = []\n","  if data_discriminator:\n","    d_moy_pos=[]\n","    d_moy_neg=[]\n","    \n","  for e in range(1, epochs+1):\n","    if e%epochs_regen==0:\n","      print(\"Regeneration de la base de donnees...\")\n","      #creation des couples d'images\n","      X_train_2 = faire_des_paires(bdd, new=False, X_train_2=X_train_2)\n","          \n","    print ('-'*15, 'Epoch %d' % e, '-'*15)\n","    dloss=0\n","    gloss=[0,0]\n","    for _ in tqdm(range(batchCount)):\n","      # DISCRIMINATOR\n","      #generation des vecteurs de bruit\n","      vector_v1 = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","      vector_v2 = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","      vector_v3 = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","      \n","      #choix des images\n","      img_x1=np.zeros((batchSize, dim[0], dim[1], dim[2]))\n","      img_x2=np.zeros((batchSize, dim[0], dim[1], dim[2]))\n","      for k in range(batchSize):\n","        indice = rd.randint(0,len(X_train_2)-1)\n","        img_x1[k]=X_train_2[indice][0]\n","        img_x2[k]=X_train_2[indice][1]\n","        \n","      #passage dans l'encodeur\n","      vector_c1=encodeur.predict(img_x1)\n","      \n","      #passage dans le generateur\n","      generatedImages_1 = generator.predict([vector_c1,vector_v1])\n","      generatedImages_2 = generator.predict([vector_c1,vector_v2])\n","      generatedImages_3 = generator.predict([vector_c1,vector_v3])\n","      \n","      #entraînement du discriminateur\n","      XDis1=np.concatenate((img_x1,generatedImages_1,img_x1))\n","      XDis2=np.concatenate((img_x2,generatedImages_2,generatedImages_3))\n","      yDis = np.zeros(3*batchSize)\n","      yDis[:batchSize] = 0.9\n","      \n","      if data_discriminator:\n","        #Recuperation des valeurs des predictions du discriminateur\n","        predictions=discriminator.predict([XDis1, XDis2])\n","        if nb_sorties==1:\n","          moy_pos=0\n","          moy_neg=0\n","          for z in range(len(predictions)):\n","            if z < batchSize:\n","              moy_pos +=predictions[z]\n","            else:\n","              moy_neg +=predictions[z]\n","\n","          moy_pos=2*float(moy_pos)/len(predictions)\n","          moy_neg=2*float(moy_neg)/len(predictions)\n","        elif nb_sorties==2:\n","          moy_pos=[0,0]\n","          moy_neg=[0,0]\n","          for z in range(len(predictions)):\n","            if z < batchSize:\n","              moy_pos[0]+=predictions[z][0]\n","              moy_pos[1]+=predictions[z][1]\n","            else:\n","              moy_neg[0]+=predictions[z][0]\n","              moy_neg[1]+=predictions[z][1]\n","\n","          moy_pos[0]=2*float(moy_pos[0])/len(predictions)\n","          moy_pos[1]=2*float(moy_pos[1])/len(predictions)\n","          moy_neg[0]=2*float(moy_neg[0])/len(predictions)\n","          moy_neg[1]=2*float(moy_neg[1])/len(predictions)\n","        d_moy_pos.append(moy_pos)\n","        d_moy_neg.append(moy_neg)\n","      \n","      dloss += discriminator.train_on_batch([XDis1,XDis2], yDis)\n","\n","      # GENERATEUR et ENCODEUR\n","      vector_v1 = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","      vector_v2 = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","      vector_v3 = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","      \n","      yGen1 = np.ones(batchSize)\n","      yGen2 = np.ones(batchSize)\n","      discriminator.trainable = False\n","      val = gan.train_on_batch([img_x1,img_x1,vector_v1,vector_v2,vector_v3], [yGen1,yGen2])\n","      val = gan.train_on_batch([img_x1,img_x1,vector_v1,vector_v2,vector_v3], [yGen1,yGen2])\n","      gloss[0] = gloss[0] + val[0]\n","      gloss[1] = gloss[1] + val[1]\n","      \n","    # TESTS ET EVALUATIONS\n","    d_losses.append(dloss)\n","    g_losses.append(gloss)\n","    print (\"Losses : DISCRIMINATOR \", d_losses[-1],\" GENERATOR \", g_losses[-1])\n","    \n","  if data_discriminator:\n","    return(g_losses,d_losses,d_moy_pos,d_moy_neg)\n","  else:\n","    return(g_losses, d_losses)\n","\n","  \n","  \n","def train_AECP(bdd, list_models, X_train, y_train, epochs=10, batchSize=128, data_discriminator=False, data_predicteur=False):\n","  \n","  if bdd != \"MNIST\":\n","    print(\"Fonction pas implementee pour la base de donnees \"+bdd+\" :(\")\n","    return([[],[],[]])\n","  if len(list_models) != 9:\n","    print(\"Liste des modeles incorrecte\")\n","    return([[],[],[]])\n","  \n","  encodeur, vector_c, vector_v, generator, discriminator, predicteur, AE, AEC, AEP = list_models\n","  \n","  global dimensions_MNIST\n","  dim = dimensions_MNIST\n","  global nb_classes_MNIST\n","  nb_classes = nb_classes_MNIST\n","  \n","  batchCount = int(len(X_train) / batchSize)\n","  print ('Epochs :', epochs)\n","  print ('Batch size :', batchSize)\n","  print ('Batches per epoch :', batchCount)\n","  ae_losses = []\n","  d_losses = []\n","  aec_losses = []\n","  aep_losses = []\n","  p_losses = []\n","  if data_discriminator:\n","    data = np.zeros((epochs*batchCount, 3, nb_classes))  #batchs, [nb_img, pred_T, pred_F], classes\n","  if data_predicteur:\n","    data_p = []\n","  for e in range(1, epochs+1):\n","    print ('-'*15, 'Epoch %d' % e, '-'*15)\n","    ae_loss=0\n","    aec_loss=0\n","    aep_loss=0\n","    d_loss=0\n","    p_loss=0\n","    for batch in tqdm(range(batchCount)):\n","            \n","      indices = np.random.randint(0, len(X_train), 2*batchSize)\n","      img_x = X_train[indices]\n","      class_x = y_train[indices]\n","      vector_random = np.random.normal(0, 1, size=[2*batchSize, randomVectorSize])\n","      \n","      #Auto-encodeur AE\n","      \n","      #ae_loss += AE.train_on_batch(img_x, img_x)\n","      \n","      C = vector_c.predict(encodeur.predict(img_x))\n","      img_generees = generator.predict([C, vector_random])\n","      \n","      \n","      #Discriminateur\n","      classes = np.zeros((2*batchSize, 28, 28, 10))\n","      for k in range(2*batchSize):\n","        classes[k,:,:] = class_x[k]\n","      \n","      img_xd = img_x[:batchSize]\n","      img_genereesd = img_generees[:batchSize]\n","      classesd = classes[:batchSize]\n","      XDis = np.concatenate((img_xd, img_genereesd))\n","      CDis = np.concatenate((classesd, classesd))\n","      YDis = np.zeros(batchSize*2)\n","      YDis[:batchSize] = 0.9\n","      \n","      if data_discriminator:\n","        pred = discriminator.predict([XDis, CDis])\n","        pred_T = pred[:batchSize]\n","        pred_F = pred[batchSize:]\n","        num_batch = (e-1)*batchCount+batch\n","        \n","        for k in range(nb_classes):\n","          nb_img = 0\n","          moy_T = 0\n","          moy_F = 0\n","          for I in range(batchSize):\n","            if class_x[I][k] == 1:\n","              nb_img += 1\n","              moy_T += pred_T[I]\n","              moy_F += pred_F[I]\n","          data[num_batch,0,k] = nb_img\n","          if nb_img != 0:\n","            data[num_batch,1,k] = moy_T / float(nb_img)\n","            data[num_batch,2,k] = moy_F / float(nb_img)\n","              \n","      d_loss += discriminator.train_on_batch([XDis,CDis],YDis)\n","      \n","      #Auto-encodeur + Classifieur AEC\n","      Y_AEC = np.zeros(2*batchSize)\n","      Y_AEC[:] = 0.9\n","      AEC.train_on_batch([img_x, vector_random, classes], Y_AEC)\n","      aec_loss += AEC.train_on_batch([img_x, vector_random, classes], Y_AEC)\n","      \n","      #Predicteur\n","      \n","      predicteur.train_on_batch(img_generees, vector_random)\n","      p_loss += predicteur.train_on_batch(img_generees, vector_random)\n","      \n","      if data_predicteur:\n","        examplesp = batchSize\n","        indicesp = np.random.randint(0, len(X_train), examplesp)\n","        img_xp = X_train[indicesp]\n","        Cp = vector_c.predict(encodeur.predict(img_xp))\n","        Vp = np.random.normal(0, 1, size=[examplesp, randomVectorSize])\n","        img_genp = generator.predict([Cp, Vp])\n","        V_pred = predicteur.predict(img_genp)\n","        erreurs = np.zeros((examplesp))\n","        for f in range(examplesp):\n","          err = 0\n","          for g in range(randomVectorSize):\n","            err += (V_pred[f,g] - Vp[f,g])**2\n","          erreurs[f] = err\n","        err_moy = np.sum(erreurs) / erreurs.shape[0]\n","        data_p.append(err_moy)\n","      \n","      #Auto-encodeur + Predicteur AEP\n","      \n","      AEP.train_on_batch([img_x, vector_random], vector_random)\n","      aep_loss += AEP.train_on_batch([img_x, vector_random], vector_random)\n","      \n","    \n","    ae_losses.append(ae_loss)\n","    aec_losses.append(aec_loss)\n","    aep_losses.append(aep_loss)\n","    d_losses.append(d_loss)\n","    p_losses.append(p_loss)\n","    print (\"Losses : AE \", ae_losses[-1],\"D \", d_losses[-1],\" AEC \", aec_losses[-1], \"P \", p_losses[-1], \" AEP \", aep_losses[-1])\n","    retour = [ae_losses, d_losses, aec_losses, p_losses, aep_losses]\n","  \n","  if data_discriminator:\n","    retour.append(data)\n","  if data_predicteur:\n","    data_p = np.asarray(data_p)\n","    retour.append(data_p)\n","  return(retour)  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"R1HBDHZvZC_Q","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def change_lr(model, list_models, name, value):\n","  if name == \"discriminator\":\n","    if model == \"GMV\":\n","      discriminator = list_models[1]\n","      lr = K.get_value(discriminator.optimizer.lr)\n","      K.set_value(discriminator.optimizer.lr, lr*value)\n","      print(\"Learning rate du discriminateur :\")\n","      print(\"changed to {}\".format(lr*value))\n","      return(list_models[0], discriminator, list_models[2])\n","    \n","    elif model == \"CGMV\":\n","      discriminator = list_models[2]\n","      lr = K.get_value(discriminator.optimizer.lr)\n","      K.set_value(discriminator.optimizer.lr, lr*value)\n","      print(\"Learning rate du discriminateur :\")\n","      print(\"changed to {}\".format(lr*value))\n","      return(list_models[0], list_models[1], discriminator, list_models[3])\n","    \n","    if model == \"AECP\":\n","      discriminator = list_models[4]\n","      lr = K.get_value(discriminator.optimizer.lr)\n","      K.set_value(discriminator.optimizer.lr, lr*value)\n","      print(\"Learning rate du discriminateur :\")\n","      print(\"changed to {}\".format(lr*value))\n","      return(list_models[0], list_models[1], list_models[2], list_models[3], discriminator, list_models[5], list_models[6], list_models[7], list_models[8])\n","    \n","  elif name == \"generator\" or name == \"gan\" or name == \"encodeur\":\n","    if model == \"GMV\":\n","      generator, gan = list_models[0], list_models[2]\n","      lr = K.get_value(generator.optimizer.lr)\n","      K.set_value(generator.optimizer.lr, lr*value)\n","      K.set_value(gan.optimizer.lr, lr*value)\n","      print(\"Learning rate du generateur et du gan :\")\n","      print(\"changed to {}\".format(lr*value))\n","      return(generator, list_models[1], gan)\n","    \n","    elif model == \"CGMV\":\n","      encodeur, generator, gan = list_models[0], list_models[1], list_models[3]\n","      lr = K.get_value(generator.optimizer.lr)\n","      K.set_value(encodeur.optimizer.lr, lr*value)\n","      K.set_value(generator.optimizer.lr, lr*value)\n","      K.set_value(gan.optimizer.lr, lr*value)\n","      print(\"Learning rate de l'encodeur, du generateur et du gan :\")\n","      print(\"changed to {}\".format(lr*value))\n","      return(encodeur, generator, list_models[2], gan)\n","    \n","    if model == \"AECP\":\n","      encodeur, vector_c, vector_v, generator, discriminator, predicteur, AE, AEC, AEP = list_models\n","      \n","      lr = K.get_value(encodeur.optimizer.lr)\n","      K.set_value(encodeur.optimizer.lr, lr*value)\n","      K.set_value(vector_c.optimizer.lr, lr*value)\n","      K.set_value(vector_v.optimizer.lr, lr*value)\n","      K.set_value(generator.optimizer.lr, lr*value)\n","      K.set_value(AE.optimizer.lr, lr*value)\n","      K.set_value(AEC.optimizer.lr, lr*value)\n","      K.set_value(AEP.optimizer.lr, lr*value)\n","      print(\"Learning rate de l'encodeur, du generateur, etc. :\")\n","      print(\"changed to {}\".format(lr*value))\n","      return(encodeur, vector_c, vector_v, generator, discriminator, predicteur, AE, AEC, AEP)\n","    \n","  elif name == \"predicteur\":\n","    if model != \"AECP\":\n","      print(\"Pas de predicteur dans ce modele !\")\n","    else:\n","      encodeur, vector_c, vector_v, generator, discriminator, predicteur, AE, AEC, AEP = list_models\n","      \n","      lr = K.get_value(predicteur.optimizer.lr)\n","      K.set_value(predicteur.optimizer.lr, lr*value)\n","      print(\"Learning rate du predicteur :\")\n","      print(\"changed to {}\".format(lr*value))\n","      return(encodeur, vector_c, vector_v, generator, discriminator, predicteur, AE, AEC, AEP)\n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"OacrptOJT4Fu","colab_type":"text"},"cell_type":"markdown","source":["## Graphiques"]},{"metadata":{"id":"mwFUWW8ST4j2","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def graph_loss(model, g_losses=[], d_losses=[], ae_losses=[], aec_losses=[], p_losses=[], aep_losses=[]):\n","  if model == \"GMV\":\n","    A=np.linspace(1,len(d_losses),len(d_losses))\n","    plt.figure(figsize=(10,10))\n","    plt.subplot(2,1,1)\n","    plt.plot(A,d_losses,color='blue',label=\"Discriminateur\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend()\n","    plt.subplot(2,1,2)\n","    plt.plot(A,g_losses, color='green',label=\"Generateur\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend()\n","    \n","  elif model == \"CGMV\":\n","    A=np.linspace(1,len(d_losses),len(d_losses))\n","    g_losses_C_1 = []\n","    g_losses_C_2 = []\n","    for i in range(len(g_losses)):\n","      g_losses_C_1.append(g_losses[i][0])\n","      g_losses_C_2.append(g_losses[i][1])\n","\n","    plt.figure(figsize=(10,10))\n","    plt.subplot(3,1,1)\n","    plt.plot(A,d_losses,color='blue',label=\"Discriminateur\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend()\n","    plt.subplot(3,1,2)\n","    plt.plot(A,g_losses_C_1, color='red',label=\"Generateur et Encodeur : 2 images generees\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend()\n","    plt.subplot(3,1,3)\n","    plt.plot(A,g_losses_C_2, color='green',label=\"Generateur et Encodeur : 1 image generee\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend()\n","    \n","  elif model == \"AECP\":\n","    A=np.linspace(1,len(ae_losses),len(ae_losses))\n","    plt.figure(figsize=(10,10))\n","    plt.subplot(5,1,1)\n","    plt.plot(A,ae_losses,color='dodgerblue',label=\"AE\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend()\n","    plt.subplot(5,1,2)\n","    plt.plot(A,d_losses, color='limegreen',label=\"Discriminateur\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend()\n","    plt.subplot(5,1,3)\n","    plt.plot(A,aec_losses, color='orangered',label=\"AEC\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend()\n","    plt.subplot(5,1,4)\n","    plt.plot(A,p_losses, color='midnightblue',label=\"Predicteur\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend()\n","    plt.subplot(5,1,5)\n","    plt.plot(A,aep_losses, color='darkmagenta',label=\"AEP\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JtuFJjneVsvA","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def graph_discriminator(d_moy_pos, d_moy_neg, nb_sorties):\n","  A=np.linspace(1,len(d_moy_pos),len(d_moy_pos))\n","  plt.figure(figsize=(12,12))\n","\n","  if nb_sorties==1:\n","    Vrai=[]\n","    Fake=[]\n","    for i in range(len(d_moy_pos)):\n","      Vrai.append(d_moy_pos[i])\n","      Fake.append(d_moy_neg[i])\n","\n","    plt.subplot(2,1,1)\n","    plt.plot(A,Vrai,color='blue')\n","    plt.ylabel(\"Predictions du discriminateur\")\n","    plt.xlabel(\"Batch\")\n","    plt.title(\"Exemples True\")\n","    plt.subplot(2,1,2)\n","    plt.plot(A,Fake,color='green')\n","    plt.ylabel(\"Predictions du discriminateur\")\n","    plt.xlabel(\"Batch\")\n","    plt.title(\"Exemples Fake\")\n","    plt.show()\n","\n","\n","  elif nb_sorties==2:\n","    True_0=[]\n","    True_1=[]\n","    Fake_0=[]\n","    Fake_1=[]\n","    for i in range(len(d_moy_pos)):\n","      True_0.append(d_moy_pos[i][0])\n","      True_1.append(d_moy_pos[i][1])\n","      Fake_0.append(d_moy_neg[i][0])\n","      Fake_1.append(d_moy_neg[i][1])\n","\n","    plt.subplot(2,1,1)\n","    plt.plot(A,True_0,color='blue',label=\"1ere valeur\")\n","    plt.plot(A,True_1,color='green',label=\"2eme valeur\")\n","    plt.ylabel(\"Predictions du discriminateur\")\n","    plt.xlabel(\"Batch\")\n","    plt.title(\"Exemples True\")\n","    plt.legend()\n","    plt.subplot(2,1,2)\n","    plt.plot(A,Fake_0,color='blue',label=\"1ere valeur\")\n","    plt.plot(A,Fake_1,color='green',label=\"2eme valeur\")\n","    plt.ylabel(\"Predictions du discriminateur\")\n","    plt.xlabel(\"Batch\")\n","    plt.title(\"Exemples Fake\")\n","    plt.legend()\n","    plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EHgSNshvsdLa","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def data_discriminator_AECP(data, figsize=(10,10)):\n","  #print(\"data.shape : \",data.shape)\n","  A = np.linspace(1,data.shape[0], data.shape[0])\n","  nb_classes = data.shape[2]\n","  colors = [\"yellow\", \"orange\", \"orangered\", \"red\", \"deeppink\", \"fuchsia\", \"darkmagenta\", \"midnightblue\", \"dodgerblue\", \"limegreen\"]\n","  plt.figure(figsize=figsize)\n","  plt.subplot(3,1,1)\n","  for k in range(nb_classes):\n","    x=[]\n","    for b in range(data.shape[0]):\n","      x.append(data[b,1,k])\n","    #print(\"x : \",x)\n","    plt.plot(A,x, label=str(k), color=colors[k])\n","  plt.ylabel(\"Predictions du discriminateur\")\n","  plt.xlabel(\"Batch\")\n","  plt.title(\"Exemples True\")\n","  plt.legend()\n","  plt.subplot(3,1,2)\n","  for k in range(nb_classes):\n","    x=[]\n","    for b in range(data.shape[0]):\n","      x.append(data[b,2,k])\n","    plt.plot(A,x, label=str(k), color=colors[k])\n","  plt.ylabel(\"Predictions du discriminateur\")\n","  plt.xlabel(\"Batch\")\n","  plt.title(\"Exemples Fake\")\n","  plt.legend()\n","  \n","  moy_T = []\n","  moy_F = []\n","  for b in range(data.shape[0]):\n","    m_T = 0\n","    m_F = 0\n","    N = 0\n","    for c in range(nb_classes):\n","      n = data[b,0,c]\n","      N += n\n","      m_T += n*data[b,1,c]\n","      m_F += n*data[b,2,c]\n","    m_T = m_T / float(N)\n","    m_F = m_F / float(N)\n","    moy_T.append(m_T)\n","    moy_F.append(m_F)\n","  #print(\"moy_T : \",moy_T)\n","  plt.subplot(3,1,3)\n","  plt.plot(A, moy_T, label=\"True\")\n","  plt.plot(A, moy_F, label=\"Fake\")\n","  plt.legend()\n","  plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"taF2oovyPoDQ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def graph_predicteur(data_p, figsize=(10,10)):\n","  A=np.linspace(1,len(data_p),len(data_p))\n","  plt.figure(figsize=figsize)\n","  plt.plot(A, data_p, color=\"red\", label=\"Erreur du predicteur\")\n","  plt.xlabel(\"Batch\")\n","  plt.legend()\n","  plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IdxkPymwI5To","colab_type":"text"},"cell_type":"markdown","source":["## Sauvegarde et restauration des poids"]},{"metadata":{"id":"LKgRRwHtI9kq","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def sauvegarde(model, bdd, list_models, name_enc=\"encodeur\", name_gen=\"generator\", name_dis=\"discriminator\", name_gan=\"gan\", name_vec_c=\"vector_c\", name_vec_v=\"vector_v\", name_class=\"classifieur\", name_pred=\"predicteur\", name_AE=\"AE\", name_AEC=\"AEC\", name_AEP=\"AEP\"):\n","  path = 'drive/Sauvegardes_poids/'+bdd+'/'+model+'/'\n","  if model == \"GMV\":\n","    generator, discriminator, gan = list_models\n","    \n","    generator.save(path+name_gen+'.h5')\n","    discriminator.save(path+name_dis+'.h5')\n","    gan.save(path+name_gan+'.h5')\n","    print(\"Poids sauvegardes dans le Drive, dans le dossier Sauvegardes_poids/\"+bdd+\"/\"+model+\", sous le nom :\")\n","    print(name_gen+\".h5\")\n","    print(name_dis+\".h5\")\n","    print(name_gan+\".h5\")\n","    \n","  elif model == \"CGMV\":\n","    encodeur, generator, discriminator, gan = list_models\n","    \n","    encodeur.save(path+name_enc+'.h5')\n","    generator.save(path+name_gen+'.h5')\n","    discriminator.save(path+name_dis+'.h5')\n","    gan.save(path+name_gan+'.h5')\n","    print(\"Poids sauvegardes dans le Drive, dans le dossier Sauvegardes_poids/\"+bdd+\"/\"+model+\", sous le nom :\")\n","    print(name_enc+\".h5\")\n","    print(name_gen+\".h5\")\n","    print(name_dis+\".h5\")\n","    print(name_gan+\".h5\")\n","    \n","  elif model == \"AECP\":\n","    encodeur, vector_c, vector_v, generator, discriminator, predicteur, AE, AEC, AEP = list_models\n","    \n","    encodeur.save(path+name_enc+'.h5')\n","    vector_c.save(path+name_vec_c+'.h5')\n","    vector_v.save(path+name_vec_v+'.h5')\n","    generator.save(path+name_gen+'.h5')\n","    discriminator.save(path+name_dis+'.h5')\n","    predicteur.save(path+name_pred+'.h5')\n","    AE.save(path+name_AE+'.h5')\n","    AEC.save(path+name_AEC+'.h5')\n","    AEP.save(path+name_AEP+'.h5')\n","    print(\"Poids sauvegardes dans le Drive, dans le dossier Sauvegardes_poids/\"+bdd+\"/\"+model+\", sous le nom :\")\n","    print(name_enc+\".h5\")\n","    print(name_vec_c+\".h5\")\n","    print(name_vec_v+\".h5\")\n","    print(name_gen+\".h5\")\n","    print(name_dis+\".h5\")\n","    print(name_pred+\".h5\")\n","    print(name_AE+\".h5\")\n","    print(name_AEC+\".h5\")\n","    print(name_AEP+\".h5\")\n","\n","def restauration_poids(model, bdd, list_models, name_enc=\"encodeur\", name_gen=\"generator\", name_dis=\"discriminator\", name_gan=\"gan\", name_vec_c=\"vector_c\", name_vec_v=\"vector_v\", name_class=\"classifieur\", name_pred=\"predicteur\", name_AE=\"AE\", name_AEC=\"AEC\", name_AEP=\"AEP\"):\n","  path = 'drive/Sauvegardes_poids/'+bdd+'/'+model+'/'\n","  from keras.models import load_model\n","  if model == \"GMV\":\n","    generator, discriminator, gan = list_models\n","    \n","    generator = load_model(path+name_gen+'.h5')\n","    discriminator = load_model(path+name_dis+'.h5')\n","    gan = load_model(path+name_gan+'.h5')\n","    print(\"Poids restaures depuis le Drive, du dossier Sauvegardes_poids/\"+bdd+\"/\"+model+\", avec les noms :\")\n","    print(name_gen+\".h5\")\n","    print(name_dis+\".h5\")\n","    print(name_gan+\".h5\")\n","    return(generator, discriminator, gan)\n","  \n","  \n","  elif model == \"CGMV\":\n","    encodeur, generator, discriminator, gan = list_models\n","    \n","    encodeur = load_model(path+name_enc+'.h5')\n","    generator = load_model(path+name_gen+'.h5')\n","    discriminator = load_model(path+name_dis+'.h5')\n","    gan = load_model(path+name_gan+'.h5')\n","    print(\"Poids restaures depuis le Drive, du dossier Sauvegardes_poids/\"+bdd+\"/\"+model+\", avec les noms :\")\n","    print(name_enc+\".h5\")\n","    print(name_gen+\".h5\")\n","    print(name_dis+\".h5\")\n","    print(name_gan+\".h5\")\n","    return(encodeur, generator, discriminator, gan)\n","\n","  elif model == \"AECP\":\n","    encodeur, vector_c, vector_v, generator, discriminator, predicteur, AE, AEC, AEP = list_models\n","    \n","    encodeur = load_model(path+name_enc+'.h5')\n","    vector_c = load_model(path+name_vec_c+'.h5')\n","    vector_v = load_model(path+name_vec_v+'.h5')\n","    generator = load_model(path+name_gen+'.h5')\n","    discriminator = load_model(path+name_dis+'.h5')\n","    predicteur = load_model(path+name_pred+'.h5')\n","    AE = load_model(path+name_AE+'.h5')\n","    AEC = load_model(path+name_AEC+'.h5')\n","    AEP = load_model(path+name_AEP+'.h5')\n","    \n","    print(\"Poids restaures depuis le Drive, du dossier Sauvegardes_poids/\"+bdd+\"/\"+model+\", avec les noms :\")\n","    print(name_enc+\".h5\")\n","    print(name_vec_c+\".h5\")\n","    print(name_vec_v+\".h5\")\n","    print(name_gen+\".h5\")\n","    print(name_dis+\".h5\")\n","    print(name_pred+\".h5\")\n","    print(name_AE+\".h5\")\n","    print(name_AEC+\".h5\")\n","    print(name_AEP+\".h5\")\n","    return(encodeur, vector_c, vector_v, generator, discriminator, predicteur, AE, AEC ,AEP)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OD8TXyTQcide","colab_type":"text"},"cell_type":"markdown","source":["## Tests"]},{"metadata":{"id":"CXqv_dErcidq","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#Affichage de 10 couples d'images generees\n","\n","def renormalise(bdd, image): #plt.imshow() veut des valeurs entre 0 et 1 et pas entre -1 et 1 lorsque les images sont en couleurs\n","  if bdd == \"MNIST\": #en noir et blanc\n","    return(image)\n","  elif bdd == \"3Dchairs\": #en couleurs\n","    return((image+1)/float(2))\n","\n","def plotGeneratedImages(model, bdd, list_models, examples=10, figsize=(15, 15)):\n","  \n","  if bdd == \"MNIST\":\n","    global dimensions_MNIST\n","    dim = dimensions_MNIST\n","  elif bdd == \"3Dchairs\":\n","    global dimensions_3Dchairs\n","    dim = dimensions_3Dchairs\n","    \n","  if model == \"GMV\":\n","    generator = list_models[0]\n","    vector_c = np.random.normal(0, 1, size=[examples,randomVectorSize])\n","    vector_v1 = np.random.normal(0, 1, size=[examples,randomVectorSize])\n","    vector_v2 = np.random.normal(0, 1, size=[examples,randomVectorSize])\n","    if dim[2] == 1:\n","      generatedImages_1 = generator.predict([vector_c,vector_v1]).reshape(examples, dim[0], dim[1])\n","      generatedImages_2 = generator.predict([vector_c,vector_v2]).reshape(examples, dim[0], dim[1])\n","    else:\n","      generatedImages_1 = generator.predict([vector_c,vector_v1]).reshape(examples, dim[0], dim[1], dim[2])\n","      generatedImages_2 = generator.predict([vector_c,vector_v2]).reshape(examples, dim[0], dim[1], dim[2])\n","    print(\"Couples d'images generees par le GMV :\")\n","    plt.figure(figsize=figsize)\n","    for i in range(examples):\n","        plt.subplot(examples, 2, 2*i+1)\n","        plt.imshow(renormalise(bdd, generatedImages_1[i]), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","        plt.subplot(examples, 2, 2*i+2)\n","        plt.imshow(renormalise(bdd, generatedImages_2[i]), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","    plt.tight_layout()\n","    plt.show()\n","    #plt.savefig('gan_generated_image.png')\n","    \n","    \n","  elif model == \"CGMV\":\n","    encodeur, generator = list_models[:2]\n","    vector_v1 = np.random.normal(0, 1, size=[examples,randomVectorSize])\n","    vector_v2 = np.random.normal(0, 1, size=[examples,randomVectorSize])\n","\n","    #choix des images\n","    img_x1=np.zeros((examples, dim[0], dim[1], dim[2]))\n","    for k in range(examples):\n","      indice1=rd.randint(0,len(X_train_2)-1)\n","      img_x1[k]=X_train_2[indice1][0]\n","\n","    #passage dans l'encodeur\n","    vector_c1=encodeur.predict(img_x1)\n","\n","    #passage dans le generateur\n","    generatedImages_1 = generator.predict([vector_c1,vector_v1]).reshape(examples, dim[0], dim[1])\n","    generatedImages_2 = generator.predict([vector_c1,vector_v2]).reshape(examples, dim[0], dim[1])\n","    print(\"image d'origine - image generee 1 - image generee 2\")\n","    plt.figure(figsize=figsize)\n","    for i in range(examples):\n","        plt.subplot(examples, 3, 3*i+1)\n","        plt.imshow(renormalise(bdd, img_x1[i].reshape((dim[0], dim[1]))), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","        plt.subplot(examples, 3, 3*i+2)\n","        plt.imshow(renormalise(bdd, generatedImages_1[i]), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","        plt.subplot(examples, 3, 3*i+3)\n","        plt.imshow(renormalise(bdd, generatedImages_2[i]), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","    plt.tight_layout()\n","    plt.show()\n","    #plt.savefig('gan_generated_image.png')\n","    \n","    \n","  elif model == \"AECP\":\n","    \n","    encodeur, vector_c, vector_v, generator, discriminator, predicteur, AE, AEC, AEP = list_models\n","    \n","    indices = np.random.randint(0, len(X_train), examples)\n","    img_x = X_train[indices]\n","    class_x = y_train[indices]\n","    vector_random = np.random.normal(0, 1, size=[examples, randomVectorSize])\n","\n","    CV = encodeur.predict(img_x)\n","    C = vector_c.predict(CV)\n","    V = vector_v.predict(CV)\n","    img_AE = generator.predict([C,V]).reshape((examples, dim[0], dim[1]))\n","    img_AEV = generator.predict([C, vector_random]).reshape((examples, dim[0], dim[1]))\n","    \n","    \"\"\"pred_class = AEC.predict([img_x, vector_random])\n","    score_AEC = 0\n","    for k in range(examples):\n","      if argmax(pred_class[k]) == class_x[k]:\n","        score_AEC += 1\"\"\"\n","    \n","    print(\"Image d'origine - image generee avec AE - image generee avec variations\")\n","    plt.figure(figsize=figsize)\n","    for i in range(examples):\n","        plt.subplot(examples, 3, 3*i+1)\n","        plt.imshow(renormalise(bdd, img_x[i].reshape((dim[0], dim[1]))), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","        plt.subplot(examples, 3, 3*i+2)\n","        plt.imshow(renormalise(bdd, img_AE[i]), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","        plt.subplot(examples, 3, 3*i+3)\n","        plt.imshow(renormalise(bdd, img_AEV[i]), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","    plt.tight_layout()\n","    plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hdosYI9DcieE","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def tableau(model, bdd, list_models, examples=10, figsize=(10,10)):\n","  \n","  if bdd == \"MNIST\":\n","    global dimensions_MNIST\n","    dim = dimensions_MNIST\n","    global nb_classes_MNIST\n","    nb_classes = nb_classes_MNIST\n","    global X_train, Y_train\n","    X = []\n","    for n in range(nb_classes):\n","      X_0=X_train[np.where(Y_train==n)]\n","      X.append(X_0)\n","  elif bdd == \"3Dchairs\":\n","    global dimensions_3Dchairs\n","    dim = dimensions_3Dchairs\n","    global nb_classes_3Dchairs\n","    nb_classes = nb_classes_3Dchairs\n","    \n","  if model == \"GMV\":\n","    generator = list_models[0]\n","    vector_c = np.random.normal(0, 1, size=[examples,randomVectorSize])\n","    vector_v = np.random.normal(0, 1, size=[examples,randomVectorSize])\n","    plt.figure(figsize=figsize)\n","    for i in range(examples):\n","      C=np.zeros((examples,randomVectorSize))\n","      C[:]=vector_c[i]\n","      generatedImages=generator.predict([C,vector_v]).reshape(examples, dim[0], dim[1])\n","      for j in range(examples):\n","        plt.subplot(examples, examples, 10*i+j+1)\n","        plt.imshow(renormalise(bdd, generatedImages[j]), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","    plt.tight_layout()\n","    plt.show()\n","    \n","    \n","  elif model == \"CGMV\":\n","    encodeur, generator = list_models[:2]\n","\n","    #choix des images\n","    img_x=np.zeros((nb_classes,dim[0], dim[1], dim[2]))\n","    for v in range(nb_classes):\n","      img_x[v]=X[v][np.random.randint(0,len(X[v]))]\n","\n","    #generation des vecteurs c et v\n","    vector_c = encodeur.predict(img_x)\n","    vector_v = np.random.normal(0, 1, size=[examples,randomVectorSize])\n","    if nb_classes > examples:\n","      nb_classes = examples\n","\n","    plt.figure(figsize=figsize)\n","    for i in range(nb_classes):\n","      C=np.zeros((examples,randomVectorSize))\n","      C[:]=vector_c[i]\n","      generatedImages=generator.predict([C,vector_v]).reshape(examples, dim[0], dim[1])\n","      plt.subplot(nb_classes, examples+1, (examples+1)*i+1)\n","      plt.imshow(renormalise(bdd, img_x[i].reshape(dim[0], dim[1])))\n","      plt.axis('off')\n","      for j in range(examples):\n","        plt.subplot(nb_classes, examples+1, (examples+1)*i+j+2)\n","        plt.imshow(renormalise(bdd, generatedImages[j]), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","    plt.tight_layout()\n","    plt.show()\n","    \n","  elif model == \"AECP\":\n","    encodeur, vector_c, vector_v, generator, discriminator, predicteur, AE, AEC, AEP = list_models\n","\n","\n","    #choix des images\n","    img_x=np.zeros((nb_classes, dim[0], dim[1], dim[2]))\n","    for v in range(nb_classes):\n","      img_x[v]=X[v][np.random.randint(0,len(X[v]))]\n","\n","    #generation des vecteurs c et v\n","    CC = vector_c.predict(encodeur.predict(img_x))\n","    V = np.random.normal(0, 1, size=[examples,randomVectorSize])\n","    if nb_classes > examples:\n","      nb_classes = examples\n","\n","    plt.figure(figsize=figsize)\n","    for i in range(nb_classes):\n","      C=np.zeros((examples,randomVectorSizeC))\n","      C[:]=CC[i]\n","      generatedImages=generator.predict([C,V]).reshape(examples, dim[0], dim[1])\n","      plt.subplot(nb_classes, examples+1, (examples+1)*i+1)\n","      plt.imshow(renormalise(bdd, img_x[i].reshape(dim[0], dim[1])))\n","      plt.axis('off')\n","      for j in range(examples):\n","        plt.subplot(nb_classes, examples+1, (examples+1)*i+j+2)\n","        plt.imshow(renormalise(bdd, generatedImages[j]), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","    plt.tight_layout()\n","    plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aY8cNvFEciea","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#Test de l'accuracy du discriminateur et du generateur\n","\n","def test(model, bdd, list_models, X_test_2=[], X_test=[], y_test=[], seuil=0.45, nb_sorties=1, batchSize=100):\n","  \n","  if bdd == \"MNIST\":\n","    global dimensions_MNIST\n","    dim = dimensions_MNIST\n","    global nb_classes_MNIST\n","    nb_classes = nb_classes_MNIST\n","  elif bdd == \"3Dchairs\":\n","    global dimensions_3Dchairs\n","    dim = dimensions_3Dchairs\n","    global nb_classes_3Dchairs\n","    nb_classes = nb_classes_3Dchairs\n","  \n","  if model == \"GMV\":\n","    generator, discriminator, gan = list_models\n","    #generation des vecteurs de bruit\n","    vector_c = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","    vector_v1 = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","    vector_v2 = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","\n","    #passage dans le generateur\n","    generatedImages_1 = generator.predict([vector_c,vector_v1])\n","    generatedImages_2 = generator.predict([vector_c,vector_v2])\n","\n","    #test du discriminateur\n","    indices=np.random.randint(0, len(X_test_2), size=batchSize)\n","    imageBatch_1 = np.zeros((len(indices), dim[0], dim[1], dim[2]))\n","    imageBatch_2 = np.zeros((len(indices), dim[0], dim[1], dim[2]))\n","    for i in range(len(indices)):\n","      imageBatch_1[i]=X_test_2[indices[i]][0]\n","      imageBatch_2[i]=X_test_2[indices[i]][1]\n","    XDis1=np.concatenate((imageBatch_1,generatedImages_1))\n","    XDis2=np.concatenate((imageBatch_2,generatedImages_2))\n","    predictions = discriminator.predict([XDis1,XDis2])\n","    score_d=0\n","    if nb_sorties==1:\n","      for j in range(len(predictions)):\n","        if j<batchSize and predictions[j]>seuil:\n","          score_d+=1\n","        elif j>= batchSize and predictions[j]<seuil:\n","          score_d+=1\n","    elif nb_sorties==2:\n","      for j in range(len(predictions)):\n","        if j<batchSize and np.argmax(predictions[j])==0:\n","          score_d+=1\n","        elif j>= batchSize and np.argmax(predictions[j])==1:\n","          score_d+=1\n","    score_d=float(score_d)/len(predictions)\n","    print(\"Seuil = \",seuil)\n","    print(\"Score du discriminateur : \"+str(score_d*100)+\"%\")\n","\n","    #test du generateur\n","    predictions_g = gan.predict([vector_c,vector_v1,vector_v2])\n","    score_g=0\n","    if nb_sorties==1:\n","      for j in range(len(predictions_g)):\n","        if predictions_g[j]>seuil:\n","          score_g+=1\n","    elif nb_sorties==2:\n","      for j in range(len(predictions_g)):\n","        if np.argmax(predictions_g[j])==0:\n","          score_g+=1\n","    score_g=float(score_g)/len(predictions_g)\n","    print(\"Score du generateur : \"+str(score_g*100)+\"%\")\n","\n","    #Affichage d'exemples\n","    if dim[2] == 1:\n","      generatedImages_1.reshape(batchSize, dim[0], dim[1])      \n","      generatedImages_2.reshape(batchSize, dim[0], dim[1])\n","      imageBatch_1.reshape(batchSize, dim[0], dim[1])\n","      imageBatch_2.reshape(batchSize, dim[0], dim[1])\n","    else:\n","      generatedImages_1.reshape(batchSize, dim[0], dim[1], dim[2])      \n","      generatedImages_2.reshape(batchSize, dim[0], dim[1], dim[2])\n","      \n","    plt.figure(figsize=(10,10))\n","    for i in range(4):\n","        plt.subplot(8, 2, 2*i+1)\n","        plt.imshow(renormalise(bdd, generatedImages_1[i]), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","        plt.subplot(8, 2, 2*i+2)\n","        plt.imshow(renormalise(bdd, generatedImages_2[i]), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","    for i in range(4):\n","        plt.subplot(8, 2, 2*i+9)\n","        plt.imshow(renormalise(bdd, imageBatch_1[i]), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","        plt.subplot(8, 2, 2*i+10)\n","        plt.imshow(renormalise(bdd, imageBatch_2[i]), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","    plt.tight_layout()\n","    plt.show()\n","    print(\"Predictions pour les images generees : \",predictions[batchSize:batchSize+4])\n","    print(\"Predictions pour les images de \"+bdd+\" : \",predictions[:4])\n","    print(\"Predictions du gan : \", predictions_g[:4])\n","    \n","    \n","    \n","  elif model == \"CGMV\":\n","    encodeur, generator, discriminator, gan = list_models\n","\n","    #generation des vecteurs de bruit\n","    vector_v1 = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","    vector_v2 = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","    vector_v3 = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","\n","    #choix des images\n","    img_x1=np.zeros((batchSize, dim[0], dim[1], dim[2]))\n","    img_x2=np.zeros((batchSize, dim[0], dim[1], dim[2]))\n","    for k in range(batchSize):\n","      indice1=rd.randint(0,len(X_test_2)-1)\n","      #indice2=rd.randint(0,len(X_test_2)-1)\n","      img_x1[k]=X_test_2[indice1][0]\n","      img_x2[k]=X_test_2[indice1][1]\n","\n","    #passage dans l'encodeur\n","    vector_c1=encodeur.predict(img_x1)\n","    vector_c2=encodeur.predict(img_x2)\n","\n","    #passage dans le generateur\n","    generatedImages_1 = generator.predict([vector_c1,vector_v1])\n","    generatedImages_2 = generator.predict([vector_c1,vector_v2])\n","    generatedImages_3 = generator.predict([vector_c1,vector_v3])\n","\n","    #test du discriminateur\n","    \"\"\"indices=np.random.randint(0, len(X_test_2), size=batchSize)\n","    imageBatch_1 = np.zeros((batchSize, dim[0], dim[1], dim[2]))\n","    imageBatch_2 = np.zeros((batchSize, dim[0], dim[1], dim[2]))\n","    for i in range(len(indices)):\n","      imageBatch_1[i]=X_test_2[indices[i]][0]\n","      imageBatch_2[i]=X_test_2[indices[i]][1]\"\"\"\n","    XDis1=np.concatenate((img_x1,generatedImages_1,img_x1))\n","    XDis2=np.concatenate((img_x2,generatedImages_2,generatedImages_3))\n","    predictions = discriminator.predict([XDis1,XDis2])\n","    score_d=0\n","    for j in range(len(predictions)):\n","      if j<batchSize and predictions[j]>seuil:\n","        score_d+=1\n","      elif j>= batchSize and predictions[j]<seuil:\n","        score_d+=1\n","    score_d=float(score_d)/len(predictions)\n","    print(\"Seuil = \",seuil)\n","    print(\"Score du discriminateur : \"+str(score_d*100)+\"%\")\n","    moy=0\n","    for pred in predictions:\n","      moy+=pred\n","    moy=float(moy)/len(predictions)\n","    M=max(predictions)\n","    m=min(predictions)\n","    print(\"Moyenne : \",moy)\n","    print(\"Maximum : \",M)\n","    print(\"Minimum : \",m)\n","\n","    #test du generateur\n","    predictions_g = gan.predict([img_x1,img_x1,vector_v1,vector_v2,vector_v3])\n","    score_g=0\n","    #print(predictions_g)\n","    for j in range(len(predictions_g)):\n","      if predictions_g[j][0]>seuil:\n","        score_g+=1\n","      if predictions_g[j][1]>seuil:\n","        score_g+=1\n","    score_g=float(score_g)/(2*len(predictions_g))\n","    print(\"Score du generateur : \"+str(score_g*100)+\"%\")\n","\n","    #Affichage d'exemples\n","    plt.figure(figsize=(10,10))\n","    for i in range(4):\n","        plt.subplot(8, 2, 2*i+1)\n","        plt.imshow(np.reshape(renormalise(bdd, generatedImages_1[i]),(dim[0], dim[1])), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","        plt.subplot(8, 2, 2*i+2)\n","        plt.imshow(np.reshape(renormalise(bdd, generatedImages_2[i]),(dim[0], dim[1])), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","    for i in range(4):\n","        plt.subplot(8, 2, 2*i+9)\n","        plt.imshow(np.reshape(renormalise(bdd, img_x1[i]),(dim[0], dim[1])), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","        plt.subplot(8, 2, 2*i+10)\n","        plt.imshow(np.reshape(renormalise(bdd, img_x2[i]),(dim[0], dim[1])), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","    plt.tight_layout()\n","    plt.show()\n","    print(\"Predictions pour les images generees : \",predictions[batchSize:batchSize+4])\n","    print(\"Predictions pour les images de \"+bdd+\" : \",predictions[:4])\n","    print(\"Valeurs du gan : \", predictions_g[0][:4])\n","    print(predictions_g[1][:4])\n","    \n","  elif model == \"AECP\":\n","    encodeur, vector_c, vector_v, generator, discriminator, predicteur, AE, AEC, AEP = list_models\n","    \n","    N = len(X_test)\n","    img_x = X_test\n","    class_x = y_test\n","    vector_random = np.random.normal(0, 1, size=[N, randomVectorSize])\n","\n","    CV = encodeur.predict(img_x)\n","    C = vector_c.predict(CV)\n","    V = vector_v.predict(CV)\n","    img_AEV = generator.predict([C, vector_random])\n","    \n","    classes = np.zeros((N, 28, 28, 10))\n","    for k in range(N):\n","      classes[k,:,:] = class_x[k]\n","    \n","    Pred_T = discriminator.predict([img_x, classes])\n","    Pred_F = discriminator.predict([img_AEV, classes])\n","    \n","    score_dis = 0\n","    for k in range(N):\n","      if Pred_T[k] > seuil:\n","        score_dis += 1\n","      if Pred_F[k] < seuil:\n","        score_dis += 1\n","    score_dis = score_dis / float(2*N)\n","    print(\"Score du discriminateur : \"+str(100*score_dis)+\"%\")\n","    \n","    pred_AEC = AEC.predict([img_x, vector_random, classes])\n","    score_AEC = 0\n","    for k in range(N):\n","      if pred_AEC[k] > seuil:\n","        score_AEC += 1\n","    score_AEC = score_AEC / float(N)\n","    print(\"Score de AEC : \"+str(100*score_AEC)+\"%\")\n","    \n","    examples = 10\n","    print(\"Image d'origine - image generee avec variations\")\n","    plt.figure(figsize=(15,15))\n","    for i in range(examples):\n","      plt.subplot(examples, 2, 2*i+1)\n","      plt.imshow(renormalise(bdd, img_x[i].reshape((dim[0], dim[1]))), interpolation='nearest', cmap='gray_r')\n","      plt.axis('off')\n","      plt.subplot(examples, 2, 2*i+2)\n","      plt.imshow(renormalise(bdd, img_AEV[i].reshape((dim[0], dim[1]))), interpolation='nearest', cmap='gray_r')\n","      plt.axis('off')\n","    plt.tight_layout()\n","    plt.show()    \n","    \n","    for k in range(examples):\n","      print(\"Image n°\"+str(k+1)+\"; Score du discriminateur :\")\n","      print(\"True / Fake\")\n","      print(Pred_T[k], \" / \", Pred_F[k])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"thYR72g0ciey","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def interpolation_lineaire(v1, v2, n=10):\n","  #n = nombre de points, v1 et v2 inclus\n","  vecteur=np.zeros((n,len(v1)))\n","  vecteur[0]=v1\n","  vecteur[n-1]=v2\n","  for p in range(1,n-1):\n","    x=np.zeros(len(v1))\n","    for q in range(len(v1)):\n","      x[q]=v1[q]+(float(p)/float(n-1))*(v2[q]-v1[q])\n","    vecteur[p]=x\n","  return(vecteur)\n","  \n","\n","def interpolation(model, bdd, list_models, n1, n2, examples=10, figsize=(10,10)):\n","  \n","  if bdd == \"MNIST\":\n","    global dimensions_MNIST\n","    global X_train, Y_train\n","    global nb_classes_MNIST\n","  \n","  if model == \"GMV\":\n","    print(\"pas d'interpolation pour ce modele :(\")\n","  elif model == \"CGMV\":\n","    encodeur, generator = list_models[:2]\n","    \n","    if bdd == \"MNIST\":\n","      dim = dimensions_MNIST\n","    elif bdd == \"3Dchairs\":\n","      global dimensions_3Dchairs\n","      dim = dimensions_3Dchairs\n","    \n","    if bdd == \"MNIST\":\n","\n","      nb_classes = nb_classes_MNIST\n","      X = []\n","      for n in range(nb_classes):\n","        X_0=X_train[np.where(Y_train==n)]\n","        X.append(X_0)\n","      k1=rd.randint(0, len(X[n1]))\n","      k2=rd.randint(0, len(X[n2]))\n","      img_x1=X[n1][k1]\n","      img_x2=X[n2][k2]\n","\n","      plt.figure(figsize=(3,3))\n","      plt.subplot(1,2,1)\n","      plt.imshow(img_x1.reshape(28,28))\n","      plt.axis('off')\n","      plt.subplot(1,2,2)\n","      plt.imshow(img_x2.reshape(28,28))\n","      plt.axis('off')\n","\n","\n","    #passage dans l'encodeur et interpolations\n","    images=np.zeros((2,dim[0], dim[1], dim[2]))\n","    images[0]=img_x1\n","    images[1]=img_x2\n","    vector_c1, vector_c2 = encodeur.predict(images)\n","    Vecteur_c=interpolation_lineaire(vector_c1, vector_c2, n=examples)\n","\n","    vector_v1 = np.random.normal(0, 1, size=randomVectorSize)\n","    vector_v2 = np.random.normal(0, 1, size=randomVectorSize)\n","    Vecteur_v = interpolation_lineaire(vector_v1, vector_v2, n=examples)\n","\n","    #Affichage\n","    plt.figure(figsize=figsize)\n","    for i in range(examples):\n","      C=np.zeros((examples,randomVectorSize))\n","      C[:]=Vecteur_c[i]\n","      generatedImages=generator.predict([C,Vecteur_v]).reshape(examples, dim[0], dim[1])\n","      for j in range(examples):\n","        plt.subplot(examples, examples, examples*i+j+1)\n","        plt.imshow(renormalise(bdd, generatedImages[j]), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","    plt.tight_layout()\n","    plt.show()\n","    \n","    \n","  elif model == \"AECP\":\n","    encodeur, vector_c, vector_v, generator, discriminator, predicteur, AE, AEC, AEP = list_models\n","    \n","  if bdd == \"MNIST\":\n","\n","    dim = dimensions_MNIST\n","\n","    nb_classes = nb_classes_MNIST\n","    X = []\n","    for n in range(nb_classes):\n","      X_0=X_train[np.where(Y_train==n)]\n","      X.append(X_0)\n","    k1=rd.randint(0, len(X[n1]))\n","    k2=rd.randint(0, len(X[n2]))\n","    img_x1=X[n1][k1]\n","    img_x2=X[n2][k2]\n","\n","    plt.figure(figsize=(3,3))\n","    plt.subplot(1,2,1)\n","    plt.imshow(img_x1.reshape(28,28))\n","    plt.axis('off')\n","    plt.subplot(1,2,2)\n","    plt.imshow(img_x2.reshape(28,28))\n","    plt.axis('off')\n","    \n","    #passage dans l'encodeur et interpolations    \n","    images=np.zeros((2,dim[0], dim[1], dim[2]))\n","    images[0]=img_x1\n","    images[1]=img_x2\n","    vector_c1, vector_c2 = vector_c.predict(encodeur.predict(images))\n","    Vecteur_c=interpolation_lineaire(vector_c1, vector_c2, n=examples)\n","\n","    vector_v1 = np.random.normal(0, 1, size=randomVectorSize)\n","    vector_v2 = np.random.normal(0, 1, size=randomVectorSize)\n","    Vecteur_v = interpolation_lineaire(vector_v1, vector_v2, n=examples)\n","    \n","    #Affichage\n","    plt.figure(figsize=figsize)\n","    for i in range(examples):\n","      C=np.zeros((examples,randomVectorSizeC))\n","      C[:]=Vecteur_c[i]\n","      generatedImages=generator.predict([C,Vecteur_v]).reshape(examples, dim[0], dim[1])\n","      for j in range(examples):\n","        plt.subplot(examples, examples, examples*i+j+1)\n","        plt.imshow(renormalise(bdd, generatedImages[j]), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","    plt.tight_layout()\n","    plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FxDGailmlJWY","colab_type":"text"},"cell_type":"markdown","source":["# Entraînement"]},{"metadata":{"id":"IrT0p1B-dHNS","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["model = \"GMV\"\n","bdd = \"MNIST\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"qY8GFAfRMrHm","colab_type":"text"},"cell_type":"markdown","source":["**Génération de la base de données**\n","\n","faire_des_paires(bdd, new=True, nb_paires=0, X_train_2=[], test=False)\n","\n","*   bdd = \"MNIST\" ou \"3Dchairs\"\n","*   new = True => X_train_2 va être entièrement créé avec le nombre de paires indiqué\n","*   new = False => X_train_2 va juste être réactualisé avec de nouvelles paires\n","*   nb_paires : seulement utile si new = True\n","*   X_train_2 : seulement utile si new = False\n","*   test = True => génère ou actualise X_test_2 (que pour MNIST)\n","\n","return(X_train_2)\n","    "]},{"metadata":{"id":"3lM5idT6Mriw","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["X_train_2 = faire_des_paires(bdd, new=True, nb_paires=40000, X_train_2=[], test=False) "],"execution_count":0,"outputs":[]},{"metadata":{"id":"zK5SD54jOcYw","colab_type":"text"},"cell_type":"markdown","source":["**Initialisation du modèle**\n","\n","create_model(model, bdd, opt_G = adam_1, opt_D = adam_2, nb_sorties = 1, loss = 'mse')\n","\n","*   model = \"GMV\" ou \"CGMV\"\n","*   bdd = \"MNIST\" ou \"3Dchairs\"\n","*   opt_G et opt_D : optimizers du générateur et du discriminateur\n","*   nb_sorties = 1 ou 2\n","*   loss : la loss de tous les réseaux\n","\n","return((encodeur,) generator, discriminator, gan)"]},{"metadata":{"id":"o5igTfcmH578","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["list_models = create_model(model, bdd, opt_G = adam_1, opt_D = adam_2, opt_P = adam_3, nb_sorties = 1, loss = 'mse')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iIQKp985kNRq","colab_type":"text"},"cell_type":"markdown","source":["### **Restauration des poids des réseaux**"]},{"metadata":{"id":"jMjm7JCOQL36","colab_type":"text"},"cell_type":"markdown","source":["restauration_poids(model, bdd, list_models, name_enc=\"encodeur\", name_gen=\"generator\", name_dis=\"discriminator\", name_gan=\"gan\")\n","\n","*   model = \"GMV\" ou \"CGMV\"\n","*   bdd = \"MNIST\" ou \"3Dchairs\"\n","*   list_models = [(encodeur,)generator,discriminator,gan]\n","*   name_enc = nom donné à la sauvegarde des poids de l'encodeur (ne pas mettre d'extension)\n","\n","return((encodeur,) generator, discriminator, gan)"]},{"metadata":{"id":"_kjGJa91QMG8","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["list_models = restauration_poids(model, bdd, list_models)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"X-84L8bUkTtk","colab_type":"text"},"cell_type":"markdown","source":["### **Entraînement**"]},{"metadata":{"id":"CJuXIhtBPFnK","colab_type":"text"},"cell_type":"markdown","source":["train(model, bdd, list_models, X_train_2, epochs=10, batchSize=128, epochs_regen=5, data_discriminator=False)\n","\n","*   model = \"GMV\" ou \"CGMV\"\n","*   bdd = \"MNIST\" ou \"3Dchairs\"\n","*   list_models = [(encodeur,)generator,discriminator,gan]\n","*   epochs_regen = nombre d'epochs separant chaque regeneration de la base de donnees\n","*   data_discriminator = True => recuperation des valeurs moyennes des predictions du discriminateur\n","\n","return(g_losses,d_losses(,d_moy_pos,d_moy_neg))\n","ou return(ae_losses, c_losses, aec_losses, aep_losses)"]},{"metadata":{"id":"NZZ0URZQlJx4","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["g_losses1, d_losses1 = train(model, bdd, list_models, X_train=X_train, y_train=y_train, epochs=3, batchSize=128, epochs_regen=5, nb_sorties=1, data_discriminator=True)    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"La8b6sTKPmIg","colab_type":"text"},"cell_type":"markdown","source":["### **Sauvegarde des poids**"]},{"metadata":{"id":"FlWQ8Yzzh9-4","colab_type":"text"},"cell_type":"markdown","source":["sauvegarde(model, bdd, list_models, name_enc=\"encodeur\", name_gen=\"generator\", name_dis=\"discriminator\", name_gan=\"gan\")\n","\n","*   model = \"GMV\" ou \"CGMV\"\n","*   bdd = \"MNIST\" ou \"3Dchairs\"\n","*   list_models = [(encodeur,)generator,discriminator,gan]\n","*   name_enc = nom donné à la sauvegarde des poids de l'encodeur (ne pas mettre d'extension)\n"]},{"metadata":{"id":"XUBJKKM7OYN2","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["sauvegarde(model, bdd, list_models)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4OvRGNL9ccAu","colab_type":"text"},"cell_type":"markdown","source":["### Graphiques"]},{"metadata":{"id":"pLlBex54_iXO","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#Nouvel entrainement\n","g_losses = g_losses1\n","d_losses = d_losses1"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6TCJvOwids9q","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#Continuite\n","g_losses = np.concatenate((g_losses, g_losses1))\n","d_losses = np.concatenate((d_losses, d_losses1))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pwK2o4KwPdg0","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["graph_loss(model, g_losses=g_losses, d_losses=d_losses)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YCitu3XVxHH8","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["data_discriminator_AECP(data, figsize=(15,15))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RX3ksEp5ZmFs","colab_type":"text"},"cell_type":"markdown","source":["# Tests"]},{"metadata":{"id":"WvxE2WOHMZN6","colab_type":"text"},"cell_type":"markdown","source":["## Tests GMV"]},{"metadata":{"id":"cLKE8COlMZjA","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["plotGeneratedImages(model, bdd, list_models, examples=10, figsize=(15,15)) #Affichage de 10 couples d'images generees"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wlOyMFt8-ApO","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["X_test_2 = faire_des_paires(bdd, new=True, nb_paires=50, test=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eViWVvd8MmXa","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["test(model, bdd, list_models, X_test=X_test, y_test=y_test, seuil=0.45, batchSize=10, nb_sorties=1) # Calcul de l'accuracy du generateur et du discriminateur"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WIeL8T2-q_Ji","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["tableau(model, bdd, list_models)"],"execution_count":0,"outputs":[]}]}