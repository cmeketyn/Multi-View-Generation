{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GMV.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[{"file_id":"1dmHfrpWA_YW9-x0kQneHUKv65z1lqRo0","timestamp":1530693433814}],"collapsed_sections":["NIluNpTeiGef","VOrDvQRFiGep","6LdKkwf3iGgV","1zuRUOPEiGgg","cXDI9B4ZiGhB","nr4_KfdiiGhG","UUV2A_EiiGhS","u6wXxH5ZiGhg","U9jEJCKJiGhy","PAKEJRPSiGiZ","gJEq8znDiGio","FxDGailmlJWY","iIQKp985kNRq","X-84L8bUkTtk","La8b6sTKPmIg","V95IKh7DZ9AQ","4OvRGNL9ccAu","RX3ksEp5ZmFs"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"NIluNpTeiGef","colab_type":"text"},"cell_type":"markdown","source":["# Importations et constantes"]},{"metadata":{"id":"VOrDvQRFiGep","colab_type":"text"},"cell_type":"markdown","source":["## Installations et importations des modules"]},{"metadata":{"id":"M6jHDS-hiGe1","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["!pip install -q keras\n","\n","!pip install -q tqdm\n","from tqdm import tqdm\n","from tqdm import tqdm_notebook\n","\n","import random as rd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","!pip install -q panda\n","import pandas as pd\n","!pip install -q openpyxl\n","import tensorflow as tf\n","import timeit\n","from __future__ import print_function\n","from PIL import Image\n","import csv"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jciaf7S3iGfD","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/"},"outputId":"56cf7fc9-aac3-4a03-e5e6-a6219410ee0e","executionInfo":{"status":"ok","timestamp":1533020021219,"user_tz":-120,"elapsed":667,"user":{"displayName":"Camille Méketyn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"116567900876735498800"}}},"cell_type":"code","source":["#KERAS\n","import keras\n","from keras import activations, initializers, regularizers, constraints,metrics\n","from keras.legacy import interfaces\n","from keras.engine import InputSpec, Layer\n","from keras.layers import Input, BatchNormalization, concatenate, Reshape, Conv2DTranspose, Activation\n","from keras.layers.core import Reshape, Dense, Dropout, Flatten\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras.layers.convolutional import Conv2D, UpSampling2D, MaxPooling2D\n","from keras.models import Model, Sequential\n","from keras.datasets import mnist\n","from keras.optimizers import Adam, Adagrad\n","from keras import backend as K\n","K.set_image_dim_ordering('tf')\n","from keras.utils import np_utils\n","from keras.utils.generic_utils import func_dump, func_load, deserialize_keras_object\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"YOjf6gfXiGfV","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":36},"outputId":"a31471cc-fff8-4dba-c748-98618045fdd8","executionInfo":{"status":"ok","timestamp":1533020023043,"user_tz":-120,"elapsed":1636,"user":{"displayName":"Camille Méketyn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"116567900876735498800"}}},"cell_type":"code","source":["device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  print('GPU device not found')\n","else:\n","  print('Found GPU at: {}'.format(device_name))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"metadata":{"id":"jMJJi2M5iGfi","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":112},"outputId":"61906360-cb28-4074-f337-54653dfd95b8","executionInfo":{"status":"ok","timestamp":1533020112293,"user_tz":-120,"elapsed":17332,"user":{"displayName":"Camille Méketyn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"116567900876735498800"}}},"cell_type":"code","source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\r\n","··········\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","Please enter the verification code: Access token retrieved correctly.\n"],"name":"stdout"}]},{"metadata":{"id":"swtV7kqZiGfr","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":36},"outputId":"cd6079e9-826e-4517-bdcd-95deaa1d10bb","executionInfo":{"status":"ok","timestamp":1533020115466,"user_tz":-120,"elapsed":3079,"user":{"displayName":"Camille Méketyn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"116567900876735498800"}}},"cell_type":"code","source":["!mkdir -p drive -v\n","!google-drive-ocamlfuse drive"],"execution_count":0,"outputs":[{"output_type":"stream","text":["mkdir: created directory 'drive'\r\n"],"name":"stdout"}]},{"metadata":{"id":"RZ7ZY9p8yY1E","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":36},"outputId":"0c480b47-6a2b-4ec9-a453-35219b3dcda8","executionInfo":{"status":"ok","timestamp":1533020118318,"user_tz":-120,"elapsed":2753,"user":{"displayName":"Camille Méketyn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"116567900876735498800"}}},"cell_type":"code","source":["!ls drive"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1A  2A\t3Dchairs  Autres  Colab Notebooks  HS  Lycée _ Prépa  Sauvegardes_poids\r\n"],"name":"stdout"}]},{"metadata":{"id":"pEmQukQ7IX-k","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":56},"outputId":"7e182852-da85-4d25-b464-2bee77b0dc22","executionInfo":{"status":"ok","timestamp":1533020118876,"user_tz":-120,"elapsed":483,"user":{"displayName":"Camille Méketyn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"116567900876735498800"}}},"cell_type":"code","source":["\"\"\"\n","Quelques commandes pour Colab :\n","\n","  - Installations :\n","!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n","\n","  - Creer un nouveau dossier :\n","!mkdir -p drive -v\n","(-p : creer aussi toute l'arborescence si elle n'existait pas, -v : verbosite)\n","\n","  - Definir le chemin par defaut :\n","os.chdir(\"drive/app\")\n","\n","  - Mount Google drive :\n","!mkdir -p drive -v\n","!google-drive-ocamlfuse drive\n","Cette commande lie Colab et le drive. Les modifications dans l'un se retrouvent dans l'autre.\n","\n","\"\"\""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nQuelques commandes pour Colab :\\n\\n  - Installations :\\n!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\\n!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\\n!apt-get update -qq 2>&1 > /dev/null\\n!apt-get -y install -qq google-drive-ocamlfuse fuse\\nfrom google.colab import auth\\nauth.authenticate_user()\\nfrom oauth2client.client import GoogleCredentials\\ncreds = GoogleCredentials.get_application_default()\\nimport getpass\\n!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\\nvcode = getpass.getpass()\\n!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\\n\\n  - Creer un nouveau dossier :\\n!mkdir -p drive -v\\n(-p : creer aussi toute l\\'arborescence si elle n\\'existait pas, -v : verbosite)\\n\\n  - Definir le chemin par defaut :\\nos.chdir(\"drive/app\")\\n\\n  - Mount Google drive :\\n!mkdir -p drive -v\\n!google-drive-ocamlfuse drive\\nCette commande lie Colab et le drive. Les modifications dans l\\'un se retrouvent dans l\\'autre.\\n\\n'"]},"metadata":{"tags":[]},"execution_count":7}]},{"metadata":{"id":"6LdKkwf3iGgV","colab_type":"text"},"cell_type":"markdown","source":["## Constantes"]},{"metadata":{"id":"3Lk2Cz1SiGgX","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#Bases de donnees :\n","#MNIST\n","nb_classes_MNIST = 10\n","dimensions_MNIST = (28,28,1)\n","#3Dchairs\n","nb_classes_3Dchairs = 1393\n","nb_images_par_classes_3Dchairs = 62\n","dimensions_3Dchairs = (64,64,3)\n","\n","#taille des vecteurs latents\n","#Vecteur C contenu\n","randomVectorSizeC = 10\n","#Vecteur V variation\n","randomVectorSize = 5\n","\n","#adagrad_1 = Adagrad(lr=0.001, epsilon=None, decay=0.0)\n","#adagrad_2 = Adagrad(lr=0.001, epsilon=None, decay=0.0)\n","\n","adam_1 = Adam(lr=0.0001, beta_1=0.5)  #par defaut pour le generateur, l'encodeur et le gan\n","adam_2 = Adam(lr=0.000001, beta_1=0.5)  #par defaut pour le discriminateur\n","adam_3 = Adam(lr=0.0001, beta_1=0.5) #predicteur\n","\n","#adam_1 = 0.001\n","#adam_2 = 0.00005"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1zuRUOPEiGgg","colab_type":"text"},"cell_type":"markdown","source":["## Importations des bases de données"]},{"metadata":{"id":"rvHHKF5EiGgi","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":187},"outputId":"db780ce3-cde2-4aab-b64b-a57df8217a13","executionInfo":{"status":"ok","timestamp":1533020122668,"user_tz":-120,"elapsed":2782,"user":{"displayName":"Camille Méketyn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"116567900876735498800"}}},"cell_type":"code","source":["#MNIST\n","\n","from keras.datasets import mnist\n","# input image dimensions\n","img_rows, img_cols = 28, 28\n","nb_classes_MNIST =10\n","\n","##### Chargement des donnees\n","\n","# the data, shuffled and split between train and test sets\n","(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n","\n","print (K.image_dim_ordering())\n","\n","if K.image_dim_ordering() == 'th':\n","    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n","    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n","    input_shape = (1, img_rows, img_cols)\n","else:\n","    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n","    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n","    input_shape = (img_rows, img_cols, 1)\n","\n","\n","X_train = X_train.astype('float32')\n","X_test = X_test.astype('float32')\n","X_train = X_train /127.5 -1\n","X_test = X_test /127.5 -1\n","print('X_train shape:', X_train.shape)\n","print('X_test shape:', X_test.shape)\n","print(X_train.shape[0], 'train samples')\n","print(X_test.shape[0], 'test samples')\n","\n","# convert class vectors to binary class matrices\n","y_train = np_utils.to_categorical(Y_train, nb_classes_MNIST)\n","y_test = np_utils.to_categorical(Y_test, nb_classes_MNIST)\n","print('y_train shape:', y_train.shape)\n","print('y_test shape:', y_test.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n","11493376/11490434 [==============================] - 1s 0us/step\n","tf\n","X_train shape: (60000, 28, 28, 1)\n","X_test shape: (10000, 28, 28, 1)\n","60000 train samples\n","10000 test samples\n","y_train shape: (60000, 10)\n","y_test shape: (10000, 10)\n"],"name":"stdout"}]},{"metadata":{"id":"ZLfSFG2oRMLm","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":56},"outputId":"5913ed30-5c2d-4806-c0c5-a76d79d8756f","executionInfo":{"status":"ok","timestamp":1533020124516,"user_tz":-120,"elapsed":444,"user":{"displayName":"Camille Méketyn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"116567900876735498800"}}},"cell_type":"code","source":["\"\"\"nb_classes = 10\n","batchSize = 2\n","nb_images_par_classe = 62\n","class_x = np.random.randint(0, nb_classes, 2*batchSize)\n","imgs = np.random.randint(0, nb_images_par_classe, 2*batchSize)\n","images = [[class_x[g], imgs[g]] for g in range(2*batchSize)]\n","X_train, classes_x = bdd_chaises(\"3Dchairs\", images)\"\"\""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'nb_classes = 10\\nbatchSize = 2\\nnb_images_par_classe = 62\\nclass_x = np.random.randint(0, nb_classes, 2*batchSize)\\nimgs = np.random.randint(0, nb_images_par_classe, 2*batchSize)\\nimages = [[class_x[g], imgs[g]] for g in range(2*batchSize)]\\nX_train, classes_x = bdd_chaises(\"3Dchairs\", images)'"]},"metadata":{"tags":[]},"execution_count":12}]},{"metadata":{"id":"cg16FI19Zvw-","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":36},"outputId":"2f85df7b-c451-40ce-bb11-b92566fac9a3","executionInfo":{"status":"ok","timestamp":1533020125099,"user_tz":-120,"elapsed":398,"user":{"displayName":"Camille Méketyn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"116567900876735498800"}}},"cell_type":"code","source":["\"\"\"print(X_train.shape)\n","plt.imshow(renormalise(\"3Dchairs\",X_train[0]))\"\"\""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'print(X_train.shape)\\nplt.imshow(renormalise(\"3Dchairs\",X_train[2]))'"]},"metadata":{"tags":[]},"execution_count":13}]},{"metadata":{"id":"cXDI9B4ZiGhB","colab_type":"text"},"cell_type":"markdown","source":["# Fonctions"]},{"metadata":{"id":"nr4_KfdiiGhG","colab_type":"text"},"cell_type":"markdown","source":["## Base de données"]},{"metadata":{"id":"YwVJSOFNiGhI","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def faire_des_paires(bdd, new=True, nb_paires=0, X_train_2=[], X_train=[], Y_train=[], test=False):\n","    \n","  if bdd == \"MNIST\":\n","    global nb_classes_MNIST\n","    nb_classes = nb_classes_MNIST\n","    global dimensions_MNIST\n","    dim = dimensions_MNIST\n","    if test:\n","      global X_test\n","      X_train = X_test\n","      global Y_test\n","      Y_train = Y_test\n","    X = []\n","    for n in range(nb_classes):\n","      X_0=X_train[np.where(Y_train==n)]\n","      X.append(X_0)\n","\n","    if new:\n","      X_train_2 = np.zeros((nb_paires, 2, dim[0], dim[1], dim[2]))\n","    nb_paires = len(X_train_2)\n","    nb_paires_par_classes = int(nb_paires / nb_classes)\n","    for i in range(nb_paires_par_classes):\n","      for j in range(nb_classes):\n","        k1=rd.randint(0,len(X[j])-1)\n","        k2=rd.randint(0,len(X[j])-1)\n","        while k1==k2:\n","          k2=rd.randint(0,len(X[j])-1)\n","        img1=X[j][k1]\n","        img2=X[j][k2]\n","        X_train_2[nb_classes*i+j][0] = img1\n","        X_train_2[nb_classes*i+j][1] = img2\n","  \n","  elif bdd == \"3Dchairs\":\n","    global nb_classes_3Dchairs\n","    nb_classes = nb_classes_3Dchairs\n","    global nb_images_par_classes_3Dchairs\n","    nb_images_par_classes = nb_images_par_classes_3Dchairs\n","    global dimensions_3Dchairs\n","    dim = dimensions_3Dchairs\n","    if test:\n","      print(\"Pas de bdd de test pour 3Dchairs\")\n","      print(\"Une bdd d'entrainement va etre generee\")\n","    if new:\n","      X_train_2 = np.zeros((nb_paires, 2, dim[0], dim[1], dim[2]))\n","    path = \"drive/\"+bdd+\"/chair\"\n","    nb_paires = len(X_train_2)\n","    nb_folders = nb_classes\n","    nb_paires_par_classes = int(nb_paires / nb_classes)\n","    if nb_paires < nb_folders:\n","      nb_folders = nb_paires\n","      nb_paires_par_classes = 1\n","    f = 0\n","    c = 0\n","    for _ in tqdm(range(nb_folders)):\n","      for im in range(nb_paires_par_classes):\n","        k1=np.random.randint(0,nb_images_par_classes-1)\n","        k2=np.random.randint(0,nb_images_par_classes-1)\n","        while k1==k2:\n","          k2=rd.randint(0,nb_images_par_classes-1)\n","\n","        name = path+str(f)+\"/\"\n","        name1 = name+str(k1)+\".png\"\n","        name2 = name+str(k2)+\".png\"\n","        X_train_2[nb_paires_par_classes*c+im] = [np.asarray(Image.open(name1)), np.asarray(Image.open(name2))]\n","      c += 1\n","      if nb_folders < nb_classes:\n","        f = np.random.randint(0, nb_classes-1)\n","      else: f = f + 1\n","    X_train_2 = (X_train_2 / float(127.5)) - 1 #pour normer entre -1 et 1\n","        \n","  np.random.shuffle(X_train_2)\n","  return(X_train_2)\n","\n","\n","\n","def bdd_chaises(bdd, images):\n","  if bdd == \"3Dchairs\":\n","    print(\"Generation d'une bdd...\")\n","    \n","    #images = [[num_chaise, num_img], [num_chaise, num_img]]\n","    indices = np.zeros((len(images)))\n","    for b in range(len(images)):\n","      couple = images[b]\n","      A = (couple[0]*62+couple[1])*3*64 +1\n","      indices[b] = A\n","    indices = np.sort(indices)\n","    #indices contient les numeros des lignes du debut de chaque image de images\n","    # +1 car la premiere ligne ne contient que des indices\n","    X_train = np.zeros((len(images),64,64,3))\n","    \n","    with open('drive/3Dchairs/chaises.csv', 'r') as f:\n","      reader = csv.reader(f)\n","      \n","      ligne = 0 #compteur de ligne\n","      ind = 0 #parcourt les indices\n","      reste = 3*64 #nombre de lignes a copier\n","      d = 0 #lignes deja copiees\n","      I = []\n","      \n","      for row in reader:\n","        \n","        if ind<len(indices) and ligne == indices[ind]+d and reste!=0: #si on doit copier cette ligne\n","          I.append(row)\n","          reste = reste-1\n","          d+=1\n","          \n","        if reste == 0 and d == 3*64: #si on a fini de copier l'image\n","          \n","          #string -> float\n","          for c in range(len(I)):\n","            for b in range(len(I[c])):\n","              I[c][b] = float(I[c][b])\n","          \n","          #list -> array\n","          IMG = np.zeros((64,64,3))\n","          for c in range(64*3):\n","            if c<64:\n","              IMG[c,:,0] = I[c]\n","            elif c<2*64:\n","              IMG[c-64,:,1] = I[c]\n","            else:\n","              IMG[c-2*64,:,2] = I[c]\n","          X_train[ind] = IMG\n","          I = []\n","          reste = 3*64\n","          d = 0\n","          ind+=1\n","        ligne+=1\n","        \n","        if X_train.shape[0] == len(indices)+1:\n","          break\n","        \n","    #shuffle des images    \n","    permut = np.linspace(0,len(images)-1,len(images), dtype='int')    \n","    np.random.shuffle(permut)\n","    X_train = X_train[permut]\n","    indices = indices[permut]\n","    classes = np.zeros((len(indices)))\n","    for j in range(len(indices)):\n","      B = indices[j]\n","      B = ((B-1)/(3*64)) / float(62)\n","      B = int(B)\n","      classes[j] = B\n","    classes = np_utils.to_categorical(classes, 1393)\n","    print(\"bdd generee\")\n","    return(X_train, classes)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nGP_-5EI9aBU","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def random_vector(batchSize, randomVectorSize=randomVectorSize):\n","  V = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","  V[:, 0] = np.random.uniform(0,1)\n","  w = np.random.uniform(0, 2*np.pi, size=batchSize)\n","  V[:, 1] = np.cos(w[:])\n","  V[:, 2] = np.sin(w[:])\n","  return(V)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UUV2A_EiiGhS","colab_type":"text"},"cell_type":"markdown","source":["## Modèles"]},{"metadata":{"id":"9tVcnD-EiGhT","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def create_model(model, bdd, opt_G = adam_1, opt_D = adam_2, opt_P = adam_3, nb_sorties = 1, loss = 'mse'):\n","  #model=\"GMV\", \"CGMV\" ou \"AECP\" ; nb_sorties=1 ou 2\n","  \n","  if bdd == \"MNIST\":\n","    if model == \"GMV\" or model == \"CGMV\":\n","      # Generator\n","      g_input_c = Input(shape=(randomVectorSize,),name=\"g_input_c\")\n","      g_input_v = Input(shape=(randomVectorSize,),name=\"g_input_v\")\n","      x = concatenate([g_input_c,g_input_v])\n","      x = Dense(7*7*128) (x)\n","      x = Reshape((7,7,128)) (x)\n","      x = Conv2DTranspose(64,kernel_size=(5,5),strides=(2,2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (x)\n","      #x = BatchNormalization() (x)\n","      x = Activation('relu') (x)\n","      #x = Dropout(0.3) (x)\n","      x = Conv2DTranspose(1,kernel_size=(5,5), strides=(2,2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (x)\n","      #x = BatchNormalization() (x)\n","      g_prediction = Activation('tanh') (x)\n","\n","      generator = Model(input = [g_input_c,g_input_v], output = g_prediction)\n","      generator.compile(optimizer=opt_G, loss=loss)\n","\n","      #Discriminateur    \n","      d_input_1 = Input(shape=(28,28,1),name=\"d_input_1\")\n","      d_input_2 = Input(shape=(28,28,1),name=\"d_input_2\")\n","      x = concatenate([d_input_1, d_input_2])\n","      x = Reshape((28,28,2)) (x)\n","      x = Conv2D(64, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (x)\n","      x = LeakyReLU(0.2)                                                 (x)\n","      #x = Dropout(0.3)                                                   (x)\n","      x = Conv2D(128, kernel_size=(4, 4), strides=(2, 2), padding='same')(x)\n","      #x = BatchNormalization()                                           (x)\n","      x = LeakyReLU(0.2)                                                 (x)\n","      #x = Conv2D(256, kernel_size=(4, 4), strides=(2, 2), padding='same')(x)\n","      #x = BatchNormalization()                                           (x)\n","      #x = LeakyReLU(0.2)                                                 (x)\n","      #x = Dropout(0.3)                                                   (x)\n","      x = Flatten()                                                      (x)\n","      d_prediction = Dense(nb_sorties, activation='sigmoid', name='d_output')     (x)\n","\n","      discriminator = Model(input = [d_input_1,d_input_2], output = d_prediction)\n","      discriminator.compile(optimizer=opt_D, loss=loss)\n","\n","      if model == \"GMV\":\n","\n","        #GAN\n","        discriminator.trainable = False\n","        gan_input_c = Input(shape=(randomVectorSize,))\n","        gan_input_v1 = Input(shape=(randomVectorSize,))\n","        gan_input_v2 = Input(shape=(randomVectorSize,))\n","        x = generator ([gan_input_c, gan_input_v1])\n","        y = generator ([gan_input_c, gan_input_v2])\n","        gan_prediction = discriminator ([x, y])\n","\n","        gan = Model(input = [gan_input_c, gan_input_v1, gan_input_v2], output = gan_prediction)\n","        gan.compile(optimizer=opt_G, loss=loss)\n","\n","      elif model == \"CGMV\":\n","\n","        #Encodeur\n","        e_input = Input(shape=(28,28,1),name=\"e_input\")\n","        x = Conv2D(32, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (e_input)\n","        x = LeakyReLU(0.2)                                                 (x)\n","        x = Conv2D(64, kernel_size=(4, 4), strides=(2, 2), padding='same')(x)\n","        x = LeakyReLU(0.2)                                                 (x)\n","        x = Flatten()                                                      (x)\n","        e_prediction = Dense(randomVectorSize, name='e_output')     (x)\n","\n","        encodeur = Model(input = e_input, output = e_prediction)\n","        encodeur.compile(optimizer=opt_G, loss=loss)\n","\n","        #GAN CGMV (Encodeur -> Generateur -> Discriminateur)\n","\n","        discriminator.trainable = False\n","\n","        gan_input_x1 = Input(shape=(28,28,1)) #image qui est utilisee deux fois par le generateur\n","        gan_input_x2 = Input(shape=(28,28,1)) #image dont une autre vue sera generee\n","        gan_input_v1 = Input(shape=(randomVectorSize,))\n","        gan_input_v2 = Input(shape=(randomVectorSize,))\n","        gan_input_v3 = Input(shape=(randomVectorSize,))\n","\n","        c1 = encodeur(gan_input_x1)\n","        c2 = encodeur(gan_input_x2)\n","\n","        x = generator ([c1, gan_input_v1])\n","        y = generator ([c1, gan_input_v2])\n","        z = generator ([c2, gan_input_v3])\n","\n","        gan_prediction_1 = discriminator ([x, y])\n","        gan_prediction_2 = discriminator ([gan_input_x2, z])\n","\n","        gan = Model(input = [gan_input_x1, gan_input_x2, gan_input_v1, gan_input_v2, gan_input_v3], output = [gan_prediction_1, gan_prediction_2])\n","        gan.compile(optimizer=opt_G, loss=loss)\n","        \n","        \n","    elif model == \"AECP\":\n","      \n","      #Encodeur\n","\n","      e_input = Input(shape=(28,28,1),name=\"e_input\")\n","      x = Conv2D(32, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (e_input)\n","      x = LeakyReLU(0.2)                                                 (x)\n","      x = Conv2D(64, kernel_size=(4, 4), strides=(2, 2), padding='same') (x)\n","      x = LeakyReLU(0.2)                                                 (x)\n","      x = Flatten()                                           (x)\n","      e_prediction = Dense(randomVectorSizeC+randomVectorSize) (x)\n","\n","      encodeur = Model(input = e_input, output = e_prediction)\n","      encodeur.compile(optimizer=opt_G, loss=loss)\n","      \n","      #Vector_c\n","      \n","      vector_c_input = Input(shape=(randomVectorSizeC+randomVectorSize,), name=\"vector_c_input\")\n","      vector_c_output = Dense(randomVectorSizeC, name=\"vector_c_output\") (vector_c_input)  #activation=\"sigmoid\"\n","      \n","      vector_c = Model(input = vector_c_input, output = vector_c_output)\n","      vector_c.compile(optimizer=opt_G, loss=loss)\n","      \n","      #Vector_v\n","      \n","      vector_v_input = Input(shape=(randomVectorSizeC+randomVectorSize,), name=\"vector_v_input\")\n","      vector_v_output = Dense(randomVectorSize, name=\"vector_v_output\") (vector_v_input)  #activation=\"sigmoid\"\n","      \n","      vector_v = Model(input = vector_v_input, output = vector_v_output)\n","      vector_v.compile(optimizer=opt_G, loss=loss)\n","      \n","      \n","      #Generateur\n","\n","      g_input_c = Input(shape=(randomVectorSizeC,),name=\"g_input_c\")\n","      g_input_v = Input(shape=(randomVectorSize,),name=\"g_input_v\")\n","      x  = Reshape((randomVectorSizeC,)) (g_input_c)\n","      y = Reshape((randomVectorSize,)) (g_input_v)\n","      x = concatenate([x,y])\n","      x = Dense(7*7*128) (x)\n","      x = Reshape((7,7,128)) (x)\n","      x = Conv2DTranspose(64,kernel_size=(5,5),strides=(2,2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (x)\n","      x = BatchNormalization() (x)\n","      x = Activation('relu') (x)\n","      x = Conv2DTranspose(1,kernel_size=(5,5), strides=(2,2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (x)\n","      x = BatchNormalization() (x)\n","      g_prediction = Activation('tanh') (x)\n","\n","      generator = Model(input = [g_input_c, g_input_v], output = g_prediction)\n","      generator.compile(optimizer=opt_G, loss=loss)\n","      \n","      #Predicteur\n","\n","      p_input = Input(shape=(28,28,1),name=\"p_input\")\n","      x = Conv2D(32, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (p_input)\n","      x = LeakyReLU(0.2)                                                 (x)\n","      x = Conv2D(64, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (x)\n","      x = LeakyReLU(0.2)                                                 (x)\n","      x = Flatten()                                                      (x)\n","      p_prediction = Dense(randomVectorSize, name='p_prediction')        (x)\n","\n","      predicteur = Model(input = p_input, output = p_prediction)\n","      predicteur.compile(optimizer=opt_P, loss=loss)\n","      \n","      #Classifieur\n","\n","      c_input = Input(shape=(28,28,1), name='c_input')\n","      x = Conv2D(32, kernel_size=(3,3), kernel_initializer=initializers.RandomNormal(stddev=0.02))(c_input)\n","      x = Activation('relu')                                        (x)\n","      x = MaxPooling2D(pool_size=(2,2))                             (x)\n","      x = Conv2D(32, kernel_size=(3,3), kernel_initializer=initializers.RandomNormal(stddev=0.02))(x)\n","      x = Activation('relu')                                        (x)\n","      x = MaxPooling2D(pool_size=(2,2))                             (x)\n","      x = Dropout(0.3)                                              (x)\n","      x = Flatten()                                                 (x)\n","      x = Dense(100)                                                (x)\n","      x = Activation('relu')                                        (x)\n","      x = Dropout(0.3)                                              (x)\n","      x = Dense(10)                                                 (x)\n","      c_prediction = Activation('softmax', name='c_prediction')     (x)\n","\n","      classifieur = Model(input = c_input, output = c_prediction)\n","      classifieur.compile(optimizer=opt_D, loss=loss)\n","      \n","      #Discriminateur  \n","      \n","      d_input_x = Input(shape=(28,28,1),name=\"d_input_x\")\n","      d_input_class = Input(shape=(28,28,10),name=\"d_input_class\")\n","      x = concatenate([d_input_x, d_input_class])\n","      x = Conv2D(64, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (x)\n","      x = BatchNormalization() (x)\n","      x = LeakyReLU(0.2)                                                 (x)\n","      x = Conv2D(128, kernel_size=(4, 4), strides=(2, 2), padding='same')(x)\n","      x = BatchNormalization() (x)\n","      x = LeakyReLU(0.2)                                                 (x)\n","      x = Flatten()                                                      (x)\n","      d_prediction = Dense(1, activation='sigmoid', name='d_output')     (x)\n","\n","      discriminator = Model(input = [d_input_x,d_input_class], output = d_prediction)\n","      discriminator.compile(optimizer=opt_D, loss=loss)\n","      \n","      \n","      #Auto-encodeur AE\n","            \n","      AE_input_x = Input(shape=(28,28,1), name='AE_input_x')\n","      \n","      CV = encodeur(AE_input_x) \n","      C = vector_c(CV)\n","      V = vector_v(CV)\n","\n","      AE_prediction = generator([C, V])\n","\n","      AE = Model(input = AE_input_x, output = AE_prediction)\n","      AE.compile(optimizer=opt_G, loss=loss)\n","      \n","      #Auto-encodeur + Classifieur AEC\n","      \n","      discriminator.trainable = False\n","      #encodeur.trainable = False\n","      #vector_c.trainable = False\n","      \n","      AEC_input_x = Input(shape=(28,28,1), name='AEC_input_x')           #image\n","      AEC_input_v = Input(shape=(randomVectorSize,), name='AEC_input_v') #bruit\n","      AEC_input_class = Input(shape=(28,28,10), name='AEC_input_class')\n","      \n","      CV = encodeur(AEC_input_x)\n","      C = vector_c(CV)\n","      img_generees = generator([C, AEC_input_v])\n","      AEC_prediction = discriminator([img_generees, AEC_input_class])\n","      \n","      AEC = Model(input = [AEC_input_x, AEC_input_v, AEC_input_class], output = AEC_prediction)\n","      AEC.compile(optimizer=opt_G, loss=loss)\n","      \n","      #Auto-encodeur + Predicteur AEP\n","      #encodeur.trainable = False\n","      #vector_c.trainable = False\n","      predicteur.trainable = False\n","      AEP_input_x = Input(shape=(28,28,1), name='AEP_input_x')             #image\n","      AEP_input_v = Input(shape=(randomVectorSize,), name='AEP_input_v')   #bruit\n","      CV = encodeur(AEP_input_x)\n","      C = vector_c(CV)\n","      y = generator([C, AEP_input_v])\n","      AEP_prediction = predicteur(y)\n","\n","      AEP = Model(input = [AEP_input_x, AEP_input_v], output = AEP_prediction)\n","      AEP.compile(optimizer=opt_G, loss=loss)\n","      \n","      print(\"Modele \"+model+\" genere pour \"+bdd)\n","      print(\"list_models = (encodeur, vector_c, vector_v, generator, discriminator, predicteur, AE, AEC, AEP)\")\n","      return(encodeur, vector_c, vector_v, generator, discriminator, predicteur, AE, AEC, AEP)\n","      \n","      \n","  elif bdd == \"3Dchairs\":\n","    \n","    if model == \"GMV\" or model == \"CGMV\":\n","\n","      # Generator\n","\n","      g_input_c = Input(shape=(randomVectorSize,),name=\"g_input_c\")\n","      g_input_v = Input(shape=(randomVectorSize,),name=\"g_input_v\")\n","      x = concatenate([g_input_c,g_input_v])\n","      x = Dense(4*4*400) (x)\n","      x = Reshape((4,4,400)) (x)\n","      x = Conv2DTranspose(250,kernel_size=(4,4),strides=(2,2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (x)\n","      x = BatchNormalization() (x)\n","      x = Activation('relu') (x)\n","      #x = Dropout(0.3) (x)\n","      x = Conv2DTranspose(125,kernel_size=(4,4), strides=(2,2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (x)\n","      x = BatchNormalization() (x)\n","      x = Activation('relu') (x)\n","      x = Conv2DTranspose(60,kernel_size=(4,4), strides=(2,2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (x)\n","      x = BatchNormalization() (x)\n","      x = Activation('relu') (x)\n","      x = Conv2DTranspose(3,kernel_size=(4,4), strides=(2,2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (x)\n","      g_prediction = Activation('tanh') (x)\n","\n","      generator = Model(input = [g_input_c,g_input_v], output = g_prediction)\n","      generator.compile(optimizer=opt_G, loss=loss)\n","\n","      # Discriminator\n","\n","      d_input_1 = Input(shape=(64,64,3),name=\"d_input_1\")\n","      d_input_2 = Input(shape=(64,64,3),name=\"d_input_2\")\n","      x = concatenate([d_input_1, d_input_2])\n","      x = Reshape((64,64,6)) (x)\n","      x = Conv2D(64, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (x)\n","      x = LeakyReLU(0.2)                                                 (x)\n","      #x = Dropout(0.3)                                                   (x)\n","      x = Conv2D(128, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02))(x)\n","      x = BatchNormalization()                                           (x)\n","      x = LeakyReLU(0.2)                                                 (x)\n","      x = Conv2D(256, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02))(x)\n","      x = BatchNormalization()                                           (x)\n","      x = LeakyReLU(0.2)                                                 (x)\n","      x = Conv2D(512, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02))(x)\n","      x = BatchNormalization()                                           (x)\n","      #x = Dropout(0.3)                                                   (x)\n","      x = Flatten()                                                      (x)\n","      d_prediction = Dense(nb_sorties, activation='sigmoid', name='d_output')     (x)\n","\n","      discriminator = Model(input = [d_input_1,d_input_2], output = d_prediction)\n","      discriminator.compile(optimizer=opt_D, loss=loss)\n","\n","      if model == \"GMV\":\n","\n","        #GAN\n","\n","        discriminator.trainable = False\n","        gan_input_c = Input(shape=(randomVectorSize,))\n","        gan_input_v1 = Input(shape=(randomVectorSize,))\n","        gan_input_v2 = Input(shape=(randomVectorSize,))\n","        x = generator ([gan_input_c, gan_input_v1])\n","        y = generator ([gan_input_c, gan_input_v2])\n","        gan_prediction = discriminator ([x, y])\n","\n","        gan = Model(input = [gan_input_c, gan_input_v1, gan_input_v2], output = gan_prediction)\n","        gan.compile(optimizer=opt_G, loss=loss)\n","\n","      elif model == \"CGMV\":\n","        print(\"Modele pas implemente:(\")\n","        #Encodeur\n","        #GAN\n","        \n","    if model == \"AECP\":\n","      \n","      #Encodeur\n","\n","      e_input = Input(shape=(64,64,3),name=\"e_input\")\n","      x = Conv2D(32, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (e_input)\n","      x = LeakyReLU(0.2)                                                 (x)\n","      x = Conv2D(64, kernel_size=(4, 4), strides=(2, 2), padding='same') (x)\n","      x = LeakyReLU(0.2)                                                 (x)\n","      x = Conv2D(128, kernel_size=(4, 4), strides=(2, 2), padding='same') (x)\n","      x = LeakyReLU(0.2)                                                 (x)\n","      x = Flatten()                                           (x)\n","      e_prediction = Dense(randomVectorSizeC+randomVectorSize) (x)\n","\n","      encodeur = Model(input = e_input, output = e_prediction)\n","      encodeur.compile(optimizer=opt_G, loss=loss)\n","\n","      #Vector_c\n","\n","      vector_c_input = Input(shape=(randomVectorSizeC+randomVectorSize,), name=\"vector_c_input\")\n","      vector_c_output = Dense(randomVectorSizeC, name=\"vector_c_output\") (vector_c_input)  #activation=\"sigmoid\"\n","\n","      vector_c = Model(input = vector_c_input, output = vector_c_output)\n","      vector_c.compile(optimizer=opt_G, loss=loss)\n","\n","      #Vector_v\n","\n","      vector_v_input = Input(shape=(randomVectorSizeC+randomVectorSize,), name=\"vector_v_input\")\n","      vector_v_output = Dense(randomVectorSize, name=\"vector_v_output\") (vector_v_input)  #activation=\"sigmoid\"\n","\n","      vector_v = Model(input = vector_v_input, output = vector_v_output)\n","      vector_v.compile(optimizer=opt_G, loss=loss)\n","\n","      # Generator\n","\n","      g_input_c = Input(shape=(randomVectorSizeC,),name=\"g_input_c\")\n","      g_input_v = Input(shape=(randomVectorSize,),name=\"g_input_v\")\n","      x = concatenate([g_input_c,g_input_v])\n","      x = Dense(4*4*400) (x)\n","      x = Reshape((4,4,400)) (x)\n","      x = Conv2DTranspose(250,kernel_size=(4,4),strides=(2,2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (x)\n","      x = BatchNormalization() (x)\n","      x = Activation('relu') (x)\n","      #x = Dropout(0.3) (x)\n","      x = Conv2DTranspose(125,kernel_size=(4,4), strides=(2,2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (x)\n","      x = BatchNormalization() (x)\n","      x = Activation('relu') (x)\n","      x = Conv2DTranspose(60,kernel_size=(4,4), strides=(2,2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (x)\n","      x = BatchNormalization() (x)\n","      x = Activation('relu') (x)\n","      x = Conv2DTranspose(3,kernel_size=(4,4), strides=(2,2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (x)\n","      g_prediction = Activation('tanh') (x)\n","\n","      generator = Model(input = [g_input_c,g_input_v], output = g_prediction)\n","      generator.compile(optimizer=opt_G, loss=loss)\n","\n","      # Discriminator\n","\n","      d_input_x = Input(shape=(64,64,3),name=\"d_input_x\")\n","      d_input_class = Input(shape=(64,64,1393),name=\"d_input_class\")\n","      x = concatenate([d_input_x, d_input_class])\n","      x = Conv2D(64, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (x)\n","      x = LeakyReLU(0.2)                                                 (x)\n","      #x = Dropout(0.3)                                                   (x)\n","      x = Conv2D(128, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02))(x)\n","      x = BatchNormalization()                                           (x)\n","      x = LeakyReLU(0.2)                                                 (x)\n","      x = Conv2D(256, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02))(x)\n","      x = BatchNormalization()                                           (x)\n","      x = LeakyReLU(0.2)                                                 (x)\n","      x = Conv2D(512, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02))(x)\n","      x = BatchNormalization()                                           (x)\n","      #x = Dropout(0.3)                                                   (x)\n","      x = Flatten()                                                      (x)\n","      d_prediction = Dense(1, activation='sigmoid', name='d_output')     (x)\n","\n","      discriminator = Model(input = [d_input_x,d_input_class], output = d_prediction)\n","      discriminator.compile(optimizer=opt_D, loss=loss)\n","\n","      #Predicteur\n","\n","      p_input = Input(shape=(64,64,3),name=\"p_input\")\n","      x = Conv2D(32, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (p_input)\n","      x = LeakyReLU(0.2)                                                 (x)\n","      x = Conv2D(64, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (x)\n","      x = LeakyReLU(0.2)                                                 (x)\n","      x = Conv2D(128, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (x)\n","      x = LeakyReLU(0.2)                                                 (x)\n","      x = Flatten()                                                      (x)\n","      p_prediction = Dense(randomVectorSize, name='p_prediction')        (x)\n","\n","      predicteur = Model(input = p_input, output = p_prediction)\n","      predicteur.compile(optimizer=opt_P, loss=loss)\n","\n","\n","\n","      #Classifieur\n","\n","      c_input = Input(shape=(64,64,3), name='c_input')\n","      x = Conv2D(32, kernel_size=(3,3), kernel_initializer=initializers.RandomNormal(stddev=0.02))(c_input)\n","      x = Activation('relu')                                        (x)\n","      x = MaxPooling2D(pool_size=(2,2))                             (x)\n","      x = Conv2D(64, kernel_size=(3,3), kernel_initializer=initializers.RandomNormal(stddev=0.02))(x)\n","      x = Activation('relu')                                        (x)\n","      x = MaxPooling2D(pool_size=(2,2))                             (x)\n","      x = Conv2D(128, kernel_size=(3,3), kernel_initializer=initializers.RandomNormal(stddev=0.02))(x)\n","      x = Activation('relu')                                        (x)\n","      x = MaxPooling2D(pool_size=(2,2))                             (x)\n","      x = Dropout(0.3)                                              (x)\n","      x = Flatten()                                                 (x)\n","      x = Dense(100)                                                (x)\n","      x = Activation('relu')                                        (x)\n","      x = Dropout(0.3)                                              (x)\n","      x = Dense(1393)                                               (x)\n","      c_prediction = Activation('softmax', name='c_prediction')     (x)\n","\n","      classifieur = Model(input = c_input, output = c_prediction)\n","      classifieur.compile(optimizer=opt_D, loss=loss)\n","\n","      #Auto-encodeur AE\n","\n","      AE_input_x = Input(shape=(64,64,3), name='AE_input_x')\n","\n","      CV = encodeur(AE_input_x) \n","      C = vector_c(CV)\n","      V = vector_v(CV)\n","\n","      AE_prediction = generator([C, V])\n","\n","      AE = Model(input = AE_input_x, output = AE_prediction)\n","      AE.compile(optimizer=opt_G, loss=loss)\n","\n","      #Auto-encodeur + Classifieur AEC\n","\n","      discriminator.trainable = False\n","      #encodeur.trainable = False\n","      #vector_c.trainable = False\n","\n","      AEC_input_x = Input(shape=(64,64,3), name='AEC_input_x')           #image\n","      AEC_input_v = Input(shape=(randomVectorSize,), name='AEC_input_v') #bruit\n","      AEC_input_class = Input(shape=(64,64,1393), name='AEC_input_class')\n","\n","      CV = encodeur(AEC_input_x)\n","      C = vector_c(CV)\n","      img_generees = generator([C, AEC_input_v])\n","      AEC_prediction = discriminator([img_generees, AEC_input_class])\n","\n","      AEC = Model(input = [AEC_input_x, AEC_input_v, AEC_input_class], output = AEC_prediction)\n","      AEC.compile(optimizer=opt_G, loss=loss)\n","\n","      #Auto-encodeur + Predicteur AEP\n","      #encodeur.trainable = False\n","      #vector_c.trainable = False\n","      predicteur.trainable = False\n","      AEP_input_x = Input(shape=(64,64,3), name='AEP_input_x')             #image\n","      AEP_input_v = Input(shape=(randomVectorSize,), name='AEP_input_v')   #bruit\n","      CV = encodeur(AEP_input_x)\n","      C = vector_c(CV)\n","      y = generator([C, AEP_input_v])\n","      AEP_prediction = predicteur(y)\n","\n","      AEP = Model(input = [AEP_input_x, AEP_input_v], output = AEP_prediction)\n","      AEP.compile(optimizer=opt_G, loss=loss)\n","      \n","      print(\"Modele \"+model+\" genere pour \"+bdd)\n","      print(\"list_models = (encodeur, vector_c, vector_v, generator, discriminator, predicteur, AE, AEC, AEP)\")\n","      \n","      \n","      \n","      \n","  print(\"Modele \"+model+\" genere pour \"+bdd)\n","  if model == \"GMV\":\n","    return(generator, discriminator, gan)\n","  elif model == \"CGMV\":\n","    return(encodeur, generator, discriminator, gan)\n","  elif model == \"AECP\":\n","    return(encodeur, vector_c, vector_v, generator, discriminator, predicteur, AE, AEC, AEP)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"u6wXxH5ZiGhg","colab_type":"text"},"cell_type":"markdown","source":["## Entraînement"]},{"metadata":{"id":"Ms382XsbiGhi","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def train(model, bdd, list_models, X_train_2=[], X_train=[], y_train=[], epochs=10, batchSize=128, epochs_regen=5, data_discriminator=False, data_predicteur=False, nb_sorties=1):\n","  if model == \"GMV\":\n","    return(train_GMV(bdd, list_models, X_train_2, epochs=epochs, batchSize=batchSize, epochs_regen=epochs_regen, data_discriminator=data_discriminator, nb_sorties=nb_sorties))\n","  elif model == \"CGMV\":\n","    return(train_CGMV(bdd, list_models, X_train_2, epochs=epochs, batchSize=batchSize, epochs_regen=epochs_regen, data_discriminator=data_discriminator))\n","  elif model == \"AECP\":\n","    return(train_AECP(bdd, list_models, X_train=X_train, y_train=y_train, epochs=epochs, batchSize=batchSize, data_discriminator=data_discriminator, data_predicteur=data_predicteur))\n","\n","def train_GMV(bdd, list_models, X_train_2, epochs=100, batchSize=128, epochs_regen=5, data_discriminator=False, nb_sorties=1):\n","  \n","  generator, discriminator, gan = list_models\n","  if bdd == \"MNIST\":\n","    global dimensions_MNIST\n","    dim = dimensions_MNIST\n","  elif bdd == \"3Dchairs\":\n","    global dimensions_3Dchairs\n","    dim = dimensions_3Dchairs\n","  \n","  batchCount = int(len(X_train_2) / batchSize)\n","  print ('Epochs :', epochs)\n","  print ('Batch size :', batchSize)\n","  print ('Batches per epoch :', batchCount)\n","  d_losses = []\n","  g_losses = []\n","  if data_discriminator:\n","    d_moy_pos=[]\n","    d_moy_neg=[]\n","  for e in range(1, epochs+1):\n","    if e%epochs_regen==0:\n","      print(\"Regeneration de la base de donnees...\")\n","      X_train_2 = faire_des_paires(bdd, new=False, X_train_2=X_train_2)\n","        \n","    print ('\\n','-'*15, 'Epoch %d' % e, '-'*15)\n","    dloss=0\n","    gloss=0\n","\n","    for _ in tqdm(range(batchCount)):\n","      # DISCRIMINATOR\n","      #generation des vecteurs de bruit\n","      vector_c = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","      vector_v1 = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","      vector_v2 = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","      \n","      #passage dans le generateur\n","      generatedImages_1 = generator.predict([vector_c,vector_v1])\n","      generatedImages_2 = generator.predict([vector_c,vector_v2])\n","      \n","      #entraînement du discriminateur\n","      indices=np.random.randint(0, len(X_train_2), size=batchSize)\n","      imageBatch_1 = np.zeros((len(indices),dim[0],dim[1],dim[2]))\n","      imageBatch_2 = np.zeros((len(indices),dim[0],dim[1],dim[2]))\n","      for i in range(len(indices)):\n","        imageBatch_1[i]=X_train_2[indices[i]][0]\n","        imageBatch_2[i]=X_train_2[indices[i]][1]\n","      XDis1=np.concatenate((imageBatch_1,generatedImages_1))\n","      XDis2=np.concatenate((imageBatch_2,generatedImages_2))\n","      if nb_sorties==1:\n","        yDis = np.zeros(2*batchSize)\n","        yDis[:batchSize] = 0.9\n","        yDis[batchSize:] = 0.1\n","      elif nb_sorties==2:\n","        yDis = np.zeros((2*batchSize,2))\n","        yDis[:batchSize] = [1,0]\n","        yDis[batchSize:]=[0,1]\n","      \n","      if data_discriminator:\n","        #Recuperation des valeurs des predictions du discriminateur\n","        predictions=discriminator.predict([XDis1, XDis2])\n","        if nb_sorties==1:\n","          moy_pos=0\n","          moy_neg=0\n","          for z in range(len(predictions)):\n","            if z < batchSize:\n","              moy_pos +=predictions[z]\n","            else:\n","              moy_neg +=predictions[z]\n","\n","          moy_pos=2*float(moy_pos)/len(predictions)\n","          moy_neg=2*float(moy_neg)/len(predictions)\n","        elif nb_sorties==2:\n","          moy_pos=[0,0]\n","          moy_neg=[0,0]\n","          for z in range(len(predictions)):\n","            if z < batchSize:\n","              moy_pos[0]+=predictions[z][0]\n","              moy_pos[1]+=predictions[z][1]\n","            else:\n","              moy_neg[0]+=predictions[z][0]\n","              moy_neg[1]+=predictions[z][1]\n","\n","          moy_pos[0]=2*float(moy_pos[0])/len(predictions)\n","          moy_pos[1]=2*float(moy_pos[1])/len(predictions)\n","          moy_neg[0]=2*float(moy_neg[0])/len(predictions)\n","          moy_neg[1]=2*float(moy_neg[1])/len(predictions)\n","        d_moy_pos.append(moy_pos)\n","        d_moy_neg.append(moy_neg)\n","        \n","      dloss += discriminator.train_on_batch([XDis1,XDis2], yDis)\n","\n","      # GENERATOR\n","      vector_c = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","      vector_v1 = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","      vector_v2 = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","      if nb_sorties==1:\n","        yGen=np.zeros(batchSize)\n","        yGen[:]=0.9\n","      elif nb_sorties==2:\n","        yGen = np.zeros((batchSize,2))\n","        yGen[:]=[1,0]\n","      discriminator.trainable = False\n","      gloss += gan.train_on_batch([vector_c,vector_v1,vector_v2], yGen)\n","    \n","    \n","    # TESTS ET EVALUATIONS\n","    d_losses.append(dloss)\n","    g_losses.append(gloss)\n","    \n","    print (\"Losses : DISCRIMINATOR \", d_losses[-1],\" GENERATOR \", g_losses[-1])\n","    \n","  if data_discriminator:\n","    return(g_losses,d_losses,d_moy_pos,d_moy_neg)\n","  else:\n","    return(g_losses, d_losses)\n","\n","  \n","def train_CGMV(bdd, list_models, X_train_2, epochs=10, batchSize=128, epochs_regen=5, data_discriminator=False):\n","  \n","  encodeur, generator, discriminator, gan = list_models\n","  \n","  if bdd == \"MNIST\":\n","    global dimensions_MNIST\n","    dim = dimensions_MNIST\n","  elif bdd == \"3Dchairs\":\n","    global dimensions_3Dchairs\n","    dim = dimensions_3Dchairs\n","    \n","  batchCount = int(len(X_train_2) / batchSize)\n","  print ('Epochs :', epochs)\n","  print ('Batch size :', batchSize)\n","  print ('Batches per epoch :', batchCount)\n","  d_losses = []\n","  g_losses = []\n","  if data_discriminator:\n","    d_moy_pos=[]\n","    d_moy_neg=[]\n","    \n","  for e in range(1, epochs+1):\n","    if e%epochs_regen==0:\n","      print(\"Regeneration de la base de donnees...\")\n","      #creation des couples d'images\n","      X_train_2 = faire_des_paires(bdd, new=False, X_train_2=X_train_2)\n","          \n","    print ('-'*15, 'Epoch %d' % e, '-'*15)\n","    dloss=0\n","    gloss=[0,0]\n","    for _ in tqdm(range(batchCount)):\n","      # DISCRIMINATOR\n","      #generation des vecteurs de bruit\n","      vector_v1 = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","      vector_v2 = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","      vector_v3 = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","      \n","      #choix des images\n","      img_x1=np.zeros((batchSize, dim[0], dim[1], dim[2]))\n","      img_x2=np.zeros((batchSize, dim[0], dim[1], dim[2]))\n","      for k in range(batchSize):\n","        indice = rd.randint(0,len(X_train_2)-1)\n","        img_x1[k]=X_train_2[indice][0]\n","        img_x2[k]=X_train_2[indice][1]\n","        \n","      #passage dans l'encodeur\n","      vector_c1=encodeur.predict(img_x1)\n","      \n","      #passage dans le generateur\n","      generatedImages_1 = generator.predict([vector_c1,vector_v1])\n","      generatedImages_2 = generator.predict([vector_c1,vector_v2])\n","      generatedImages_3 = generator.predict([vector_c1,vector_v3])\n","      \n","      #entraînement du discriminateur\n","      XDis1=np.concatenate((img_x1,generatedImages_1,img_x1))\n","      XDis2=np.concatenate((img_x2,generatedImages_2,generatedImages_3))\n","      yDis = np.zeros(3*batchSize)\n","      yDis[:batchSize] = 0.9\n","      \n","      if data_discriminator:\n","        #Recuperation des valeurs des predictions du discriminateur\n","        predictions=discriminator.predict([XDis1, XDis2])\n","        if nb_sorties==1:\n","          moy_pos=0\n","          moy_neg=0\n","          for z in range(len(predictions)):\n","            if z < batchSize:\n","              moy_pos +=predictions[z]\n","            else:\n","              moy_neg +=predictions[z]\n","\n","          moy_pos=2*float(moy_pos)/len(predictions)\n","          moy_neg=2*float(moy_neg)/len(predictions)\n","        elif nb_sorties==2:\n","          moy_pos=[0,0]\n","          moy_neg=[0,0]\n","          for z in range(len(predictions)):\n","            if z < batchSize:\n","              moy_pos[0]+=predictions[z][0]\n","              moy_pos[1]+=predictions[z][1]\n","            else:\n","              moy_neg[0]+=predictions[z][0]\n","              moy_neg[1]+=predictions[z][1]\n","\n","          moy_pos[0]=2*float(moy_pos[0])/len(predictions)\n","          moy_pos[1]=2*float(moy_pos[1])/len(predictions)\n","          moy_neg[0]=2*float(moy_neg[0])/len(predictions)\n","          moy_neg[1]=2*float(moy_neg[1])/len(predictions)\n","        d_moy_pos.append(moy_pos)\n","        d_moy_neg.append(moy_neg)\n","      \n","      dloss += discriminator.train_on_batch([XDis1,XDis2], yDis)\n","\n","      # GENERATEUR et ENCODEUR\n","      vector_v1 = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","      vector_v2 = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","      vector_v3 = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","      \n","      yGen1 = np.ones(batchSize)\n","      yGen2 = np.ones(batchSize)\n","      discriminator.trainable = False\n","      val = gan.train_on_batch([img_x1,img_x1,vector_v1,vector_v2,vector_v3], [yGen1,yGen2])\n","      val = gan.train_on_batch([img_x1,img_x1,vector_v1,vector_v2,vector_v3], [yGen1,yGen2])\n","      gloss[0] = gloss[0] + val[0]\n","      gloss[1] = gloss[1] + val[1]\n","      \n","    # TESTS ET EVALUATIONS\n","    d_losses.append(dloss)\n","    g_losses.append(gloss)\n","    print (\"Losses : DISCRIMINATOR \", d_losses[-1],\" GENERATOR \", g_losses[-1])\n","    \n","  if data_discriminator:\n","    return(g_losses,d_losses,d_moy_pos,d_moy_neg)\n","  else:\n","    return(g_losses, d_losses)\n","\n","  \n","  \n","def train_AECP(bdd, list_models, X_train, y_train, epochs=10, batchSize=128, data_discriminator=False, data_predicteur=False):\n","  \n","  if len(list_models) != 9:\n","    print(\"Liste des modeles incorrecte\")\n","    return([[],[],[]])\n","  \n","  \n","  encodeur, vector_c, vector_v, generator, discriminator, predicteur, AE, AEC, AEP = list_models\n","  \n","  if bdd == \"MNIST\":  \n","  \n","    global dimensions_MNIST\n","    dim = dimensions_MNIST\n","    global nb_classes_MNIST\n","    nb_classes = nb_classes_MNIST\n","    batchCount = int(len(X_train) / batchSize)\n","  \n","  if bdd == \"3Dchairs\":  \n","    global dimensions_3Dchairs\n","    dim = dimensions_3Dchairs\n","    global nb_classes_3Dchairs\n","    nb_classes = nb_classes_3Dchairs\n","    global nb_images_par_classes_3Dchairs\n","    nb_images_par_classe = nb_images_par_classes_3Dchairs\n","    batchCount = 10\n","    \n","  print ('Epochs :', epochs)\n","  print ('Batch size :', batchSize)\n","  print ('Batches per epoch :', batchCount)\n","  ae_losses = []\n","  d_losses = []\n","  aec_losses = []\n","  aep_losses = []\n","  p_losses = []\n","  if data_discriminator:\n","    data = np.zeros((epochs*batchCount, 3, nb_classes))  #batchs, [nb_img, pred_T, pred_F], classes\n","  if data_predicteur:\n","    data_p = []\n","  for e in range(1, epochs+1):\n","    print ('-'*15, 'Epoch %d' % e, '-'*15)\n","    ae_loss=0\n","    aec_loss=0\n","    aep_loss=0\n","    d_loss=0\n","    p_loss=0\n","    \n","    if bdd == \"3Dchairs\":\n","      class_x = np.random.randint(0, nb_classes, batchSize)\n","      imgs = np.random.randint(0, nb_images_par_classe, batchSize)\n","      images = [[class_x[g], imgs[g]] for g in range(batchSize)]\n","      X_train, class_x = bdd_chaises(bdd, images)\n","      img_x = X_train\n","    \n","    for batch in tqdm(range(batchCount)):\n","      if bdd == \"MNIST\":        \n","        indices = np.random.randint(0, len(X_train), 2*batchSize)\n","        img_x = X_train[indices]\n","        class_x = y_train[indices]\n","        \n","      \n","      vector_random = random_vector(2*batchSize, randomVectorSize)\n","      \n","      #Auto-encodeur AE\n","      \n","      #ae_loss += AE.train_on_batch(img_x, img_x)\n","      \n","      C = vector_c.predict(encodeur.predict(img_x))\n","      img_generees = generator.predict([C, vector_random])\n","      \n","      \n","      #Discriminateur\n","      if bdd == \"MNIST\":\n","        classes = np.zeros((2*batchSize, 28, 28, 10))\n","        \n","      elif bdd == \"3Dchairs\":\n","        classes = np.zeros((batchSize, 64, 64, 1393))\n","        \n","      for k in range(2*batchSize):\n","        classes[k,:,:] = class_x[k]\n","      \n","      img_xd = img_x[:batchSize]\n","      img_genereesd = img_generees[:batchSize]\n","      classesd = classes[:batchSize]\n","      XDis = np.concatenate((img_xd, img_genereesd))\n","      CDis = np.concatenate((classesd, classesd))\n","      YDis = np.zeros(batchSize*2)\n","      YDis[:batchSize] = 0.9\n","      \n","      if data_discriminator:\n","        pred = discriminator.predict([XDis, CDis])\n","        pred_T = pred[:batchSize]\n","        pred_F = pred[batchSize:]\n","        num_batch = (e-1)*batchCount+batch\n","        \n","        for k in range(nb_classes):\n","          nb_img = 0\n","          moy_T = 0\n","          moy_F = 0\n","          for s in range(batchSize):\n","            if class_x[s][k] == 1:\n","              nb_img += 1\n","              moy_T += pred_T[s]\n","              moy_F += pred_F[s]\n","          data[num_batch,0,k] = nb_img\n","          if nb_img != 0:\n","            data[num_batch,1,k] = moy_T / float(nb_img)\n","            data[num_batch,2,k] = moy_F / float(nb_img)\n","              \n","      d_loss += discriminator.train_on_batch([XDis,CDis],YDis)\n","            \n","      #Auto-encodeur + Classifieur AEC\n","      Y_AEC = np.zeros(2*batchSize)\n","      Y_AEC[:] = 0.9\n","      aec_loss += AEC.train_on_batch([img_x, vector_random, classes], Y_AEC)\n","      \n","      \n","      #Predicteur\n","      \n","      predicteur.train_on_batch(img_generees, vector_random)\n","      p_loss += predicteur.train_on_batch(img_generees, vector_random)\n","      \n","      if data_predicteur:\n","        examplesp = batchSize\n","        indicesp = np.random.randint(0, len(X_train), examplesp)\n","        img_xp = X_train[indicesp]\n","        Cp = vector_c.predict(encodeur.predict(img_xp))\n","        Vp = np.random.normal(0, 1, size=[examplesp, randomVectorSize])\n","        img_genp = generator.predict([Cp, Vp])\n","        V_pred = predicteur.predict(img_genp)\n","        erreurs = np.zeros((examplesp))\n","        for f in range(examplesp):\n","          err = 0\n","          for g in range(randomVectorSize):\n","            err += (V_pred[f,g] - Vp[f,g])**2\n","          erreurs[f] = err\n","        err_moy = np.sum(erreurs) / erreurs.shape[0]\n","        data_p.append(err_moy)\n","      \n","      #Auto-encodeur + Predicteur AEP\n","      \n","      aep_loss += AEP.train_on_batch([img_x, vector_random], vector_random)\n","      \n","    \n","    ae_losses.append(ae_loss)\n","    aec_losses.append(aec_loss)\n","    aep_losses.append(aep_loss)\n","    d_losses.append(d_loss)\n","    p_losses.append(p_loss)\n","    print (\"Losses : AE \", ae_losses[-1],\"D \", d_losses[-1],\" AEC \", aec_losses[-1], \"P \", p_losses[-1], \" AEP \", aep_losses[-1])\n","    retour = [ae_losses, d_losses, aec_losses, p_losses, aep_losses]\n","  \n","  if data_discriminator:\n","    retour.append(data)\n","  if data_predicteur:\n","    data_p = np.asarray(data_p)\n","    retour.append(data_p)\n","  return(retour)  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"4A0XDyFFiGho","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def change_lr(model, list_models, name, value):\n","  if name == \"discriminator\":\n","    if model == \"GMV\":\n","      discriminator = list_models[1]\n","      lr = K.get_value(discriminator.optimizer.lr)\n","      K.set_value(discriminator.optimizer.lr, lr*value)\n","      print(\"Learning rate du discriminateur :\")\n","      print(\"changed to {}\".format(lr*value))\n","      return(list_models[0], discriminator, list_models[2])\n","    \n","    elif model == \"CGMV\":\n","      discriminator = list_models[2]\n","      lr = K.get_value(discriminator.optimizer.lr)\n","      K.set_value(discriminator.optimizer.lr, lr*value)\n","      print(\"Learning rate du discriminateur :\")\n","      print(\"changed to {}\".format(lr*value))\n","      return(list_models[0], list_models[1], discriminator, list_models[3])\n","    \n","    if model == \"AECP\":\n","      discriminator = list_models[4]\n","      lr = K.get_value(discriminator.optimizer.lr)\n","      K.set_value(discriminator.optimizer.lr, lr*value)\n","      print(\"Learning rate du discriminateur :\")\n","      print(\"changed to {}\".format(lr*value))\n","      return(list_models[0], list_models[1], list_models[2], list_models[3], discriminator, list_models[5], list_models[6], list_models[7], list_models[8])\n","    \n","  elif name == \"generator\" or name == \"gan\" or name == \"encodeur\":\n","    if model == \"GMV\":\n","      generator, gan = list_models[0], list_models[2]\n","      lr = K.get_value(generator.optimizer.lr)\n","      K.set_value(generator.optimizer.lr, lr*value)\n","      K.set_value(gan.optimizer.lr, lr*value)\n","      print(\"Learning rate du generateur et du gan :\")\n","      print(\"changed to {}\".format(lr*value))\n","      return(generator, list_models[1], gan)\n","    \n","    elif model == \"CGMV\":\n","      encodeur, generator, gan = list_models[0], list_models[1], list_models[3]\n","      lr = K.get_value(generator.optimizer.lr)\n","      K.set_value(encodeur.optimizer.lr, lr*value)\n","      K.set_value(generator.optimizer.lr, lr*value)\n","      K.set_value(gan.optimizer.lr, lr*value)\n","      print(\"Learning rate de l'encodeur, du generateur et du gan :\")\n","      print(\"changed to {}\".format(lr*value))\n","      return(encodeur, generator, list_models[2], gan)\n","    \n","    if model == \"AECP\":\n","      encodeur, vector_c, vector_v, generator, discriminator, predicteur, AE, AEC, AEP = list_models\n","      \n","      lr = K.get_value(encodeur.optimizer.lr)\n","      K.set_value(encodeur.optimizer.lr, lr*value)\n","      K.set_value(vector_c.optimizer.lr, lr*value)\n","      K.set_value(vector_v.optimizer.lr, lr*value)\n","      K.set_value(generator.optimizer.lr, lr*value)\n","      K.set_value(AE.optimizer.lr, lr*value)\n","      K.set_value(AEC.optimizer.lr, lr*value)\n","      K.set_value(AEP.optimizer.lr, lr*value)\n","      print(\"Learning rate de l'encodeur, du generateur, etc. :\")\n","      print(\"changed to {}\".format(lr*value))\n","      return(encodeur, vector_c, vector_v, generator, discriminator, predicteur, AE, AEC, AEP)\n","    \n","  elif name == \"predicteur\":\n","    if model != \"AECP\":\n","      print(\"Pas de predicteur dans ce modele !\")\n","    else:\n","      encodeur, vector_c, vector_v, generator, discriminator, predicteur, AE, AEC, AEP = list_models\n","      \n","      lr = K.get_value(predicteur.optimizer.lr)\n","      K.set_value(predicteur.optimizer.lr, lr*value)\n","      print(\"Learning rate du predicteur :\")\n","      print(\"changed to {}\".format(lr*value))\n","      return(encodeur, vector_c, vector_v, generator, discriminator, predicteur, AE, AEC, AEP)\n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"U9jEJCKJiGhy","colab_type":"text"},"cell_type":"markdown","source":["## Graphiques"]},{"metadata":{"id":"weORV_JOiGhz","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def graph_loss(model, g_losses=[], d_losses=[], ae_losses=[], aec_losses=[], p_losses=[], aep_losses=[], figsize=(10,10)):\n","  if model == \"GMV\":\n","    A=np.linspace(1,len(d_losses),len(d_losses))\n","    plt.figure(figsize=figsize)\n","    plt.subplot(2,1,1)\n","    plt.plot(A,d_losses,color='blue',label=\"Discriminateur\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend()\n","    plt.subplot(2,1,2)\n","    plt.plot(A,g_losses, color='green',label=\"Generateur\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend()\n","    \n","  elif model == \"CGMV\":\n","    A=np.linspace(1,len(d_losses),len(d_losses))\n","    g_losses_C_1 = []\n","    g_losses_C_2 = []\n","    for i in range(len(g_losses)):\n","      g_losses_C_1.append(g_losses[i][0])\n","      g_losses_C_2.append(g_losses[i][1])\n","\n","    plt.figure(figsize=figsize)\n","    plt.subplot(3,1,1)\n","    plt.plot(A,d_losses,color='blue',label=\"Discriminateur\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend()\n","    plt.subplot(3,1,2)\n","    plt.plot(A,g_losses_C_1, color='red',label=\"Generateur et Encodeur : 2 images generees\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend()\n","    plt.subplot(3,1,3)\n","    plt.plot(A,g_losses_C_2, color='green',label=\"Generateur et Encodeur : 1 image generee\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend()\n","    \n","  elif model == \"AECP\":\n","    A=np.linspace(1,len(ae_losses),len(ae_losses))\n","    plt.figure(figsize=figsize)\n","    plt.subplot(5,1,1)\n","    plt.plot(A,ae_losses,color='dodgerblue',label=\"AE\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend()\n","    plt.subplot(5,1,2)\n","    plt.plot(A,d_losses, color='limegreen',label=\"Discriminateur\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend()\n","    plt.subplot(5,1,3)\n","    plt.plot(A,aec_losses, color='orangered',label=\"AEC\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend()\n","    plt.subplot(5,1,4)\n","    plt.plot(A,p_losses, color='midnightblue',label=\"Predicteur\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend()\n","    plt.subplot(5,1,5)\n","    plt.plot(A,aep_losses, color='darkmagenta',label=\"AEP\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"d34egxDIiGh7","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def graph_discriminator(d_moy_pos, d_moy_neg, nb_sorties):\n","  A=np.linspace(1,len(d_moy_pos),len(d_moy_pos))\n","  plt.figure(figsize=(12,12))\n","\n","  if nb_sorties==1:\n","    Vrai=[]\n","    Fake=[]\n","    for i in range(len(d_moy_pos)):\n","      Vrai.append(d_moy_pos[i])\n","      Fake.append(d_moy_neg[i])\n","\n","    plt.subplot(2,1,1)\n","    plt.plot(A,Vrai,color='blue')\n","    plt.ylabel(\"Predictions du discriminateur\")\n","    plt.xlabel(\"Batch\")\n","    plt.title(\"Exemples True\")\n","    plt.subplot(2,1,2)\n","    plt.plot(A,Fake,color='green')\n","    plt.ylabel(\"Predictions du discriminateur\")\n","    plt.xlabel(\"Batch\")\n","    plt.title(\"Exemples Fake\")\n","    plt.show()\n","\n","\n","  elif nb_sorties==2:\n","    True_0=[]\n","    True_1=[]\n","    Fake_0=[]\n","    Fake_1=[]\n","    for i in range(len(d_moy_pos)):\n","      True_0.append(d_moy_pos[i][0])\n","      True_1.append(d_moy_pos[i][1])\n","      Fake_0.append(d_moy_neg[i][0])\n","      Fake_1.append(d_moy_neg[i][1])\n","\n","    plt.subplot(2,1,1)\n","    plt.plot(A,True_0,color='blue',label=\"1ere valeur\")\n","    plt.plot(A,True_1,color='green',label=\"2eme valeur\")\n","    plt.ylabel(\"Predictions du discriminateur\")\n","    plt.xlabel(\"Batch\")\n","    plt.title(\"Exemples True\")\n","    plt.legend()\n","    plt.subplot(2,1,2)\n","    plt.plot(A,Fake_0,color='blue',label=\"1ere valeur\")\n","    plt.plot(A,Fake_1,color='green',label=\"2eme valeur\")\n","    plt.ylabel(\"Predictions du discriminateur\")\n","    plt.xlabel(\"Batch\")\n","    plt.title(\"Exemples Fake\")\n","    plt.legend()\n","    plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"D30URLORiGiD","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def data_discriminator_AECP(bdd, data, figsize=(10,10)):\n","  if bdd == \"MNIST\":\n","    #print(\"data.shape : \",data.shape)\n","    A = np.linspace(1,data.shape[0], data.shape[0])\n","    nb_classes = data.shape[2]\n","    colors = [\"yellow\", \"orange\", \"orangered\", \"red\", \"deeppink\", \"fuchsia\", \"darkmagenta\", \"midnightblue\", \"dodgerblue\", \"limegreen\"]\n","    plt.figure(figsize=figsize)\n","    plt.subplot(3,1,1)\n","    for k in range(nb_classes):\n","      x=[]\n","      for b in range(data.shape[0]):\n","        x.append(data[b,1,k])\n","      #print(\"x : \",x)\n","      plt.plot(A,x, label=str(k), color=colors[k])\n","    plt.ylabel(\"Predictions du discriminateur\")\n","    plt.xlabel(\"Batch\")\n","    plt.title(\"Exemples True\")\n","    plt.legend()\n","    plt.subplot(3,1,2)\n","    for k in range(nb_classes):\n","      x=[]\n","      for b in range(data.shape[0]):\n","        x.append(data[b,2,k])\n","      plt.plot(A,x, label=str(k), color=colors[k])\n","    plt.ylabel(\"Predictions du discriminateur\")\n","    plt.xlabel(\"Batch\")\n","    plt.title(\"Exemples Fake\")\n","    plt.legend()\n","\n","    moy_T = []\n","    moy_F = []\n","    for b in range(data.shape[0]):\n","      m_T = 0\n","      m_F = 0\n","      N = 0\n","      for c in range(nb_classes):\n","        n = data[b,0,c]\n","        N += n\n","        m_T += n*data[b,1,c]\n","        m_F += n*data[b,2,c]\n","      m_T = m_T / float(N)\n","      m_F = m_F / float(N)\n","      moy_T.append(m_T)\n","      moy_F.append(m_F)\n","    #print(\"moy_T : \",moy_T)\n","    plt.subplot(3,1,3)\n","    plt.plot(A, moy_T, label=\"True\")\n","    plt.plot(A, moy_F, label=\"Fake\")\n","    plt.legend()\n","    plt.show()\n","    \n","  elif bdd == \"3Dchairs\":\n","    A = np.linspace(1,data.shape[0], data.shape[0])\n","    nb_classes = data.shape[2]\n","    plt.figure(figsize=figsize)\n","    moy_T = []\n","    moy_F = []\n","    for b in range(data.shape[0]):\n","      m_T = 0\n","      m_F = 0\n","      N = 0\n","      for c in range(nb_classes):\n","        n = data[b,0,c]\n","        N += n\n","        m_T += n*data[b,1,c]\n","        m_F += n*data[b,2,c]\n","      m_T = m_T / float(N)\n","      m_F = m_F / float(N)\n","      moy_T.append(m_T)\n","      moy_F.append(m_F)\n","    #print(\"moy_T : \",moy_T)\n","    plt.plot(A, moy_T, label=\"True\")\n","    plt.plot(A, moy_F, label=\"Fake\")\n","    plt.legend()\n","    plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PjyqPTBdiGiS","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def graph_predicteur(data_p, figsize=(10,10)):\n","  A=np.linspace(1,len(data_p),len(data_p))\n","  plt.figure(figsize=figsize)\n","  plt.plot(A, data_p, color=\"red\", label=\"Erreur du predicteur\")\n","  plt.xlabel(\"Batch\")\n","  plt.legend()\n","  plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PAKEJRPSiGiZ","colab_type":"text"},"cell_type":"markdown","source":["## Sauvegarde et restauration des poids"]},{"metadata":{"id":"7NN5vzp2iGia","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def sauvegarde(model, bdd, list_models, name_enc=\"encodeur\", name_gen=\"generator\", name_dis=\"discriminator\", name_gan=\"gan\", name_vec_c=\"vector_c\", name_vec_v=\"vector_v\", name_class=\"classifieur\", name_pred=\"predicteur\", name_AE=\"AE\", name_AEC=\"AEC\", name_AEP=\"AEP\"):\n","  path = 'drive/Sauvegardes_poids/'+bdd+'/'+model+'/'\n","  if model == \"GMV\":\n","    generator, discriminator, gan = list_models\n","    \n","    generator.save(path+name_gen+'.h5')\n","    discriminator.save(path+name_dis+'.h5')\n","    gan.save(path+name_gan+'.h5')\n","    print(\"Poids sauvegardes dans le Drive, dans le dossier Sauvegardes_poids/\"+bdd+\"/\"+model+\", sous le nom :\")\n","    print(name_gen+\".h5\")\n","    print(name_dis+\".h5\")\n","    print(name_gan+\".h5\")\n","    \n","  elif model == \"CGMV\":\n","    encodeur, generator, discriminator, gan = list_models\n","    \n","    encodeur.save(path+name_enc+'.h5')\n","    generator.save(path+name_gen+'.h5')\n","    discriminator.save(path+name_dis+'.h5')\n","    gan.save(path+name_gan+'.h5')\n","    print(\"Poids sauvegardes dans le Drive, dans le dossier Sauvegardes_poids/\"+bdd+\"/\"+model+\", sous le nom :\")\n","    print(name_enc+\".h5\")\n","    print(name_gen+\".h5\")\n","    print(name_dis+\".h5\")\n","    print(name_gan+\".h5\")\n","    \n","  elif model == \"AECP\":\n","    encodeur, vector_c, vector_v, generator, discriminator, predicteur, AE, AEC, AEP = list_models\n","    \n","    encodeur.save(path+name_enc+'.h5')\n","    vector_c.save(path+name_vec_c+'.h5')\n","    vector_v.save(path+name_vec_v+'.h5')\n","    generator.save(path+name_gen+'.h5')\n","    discriminator.save(path+name_dis+'.h5')\n","    predicteur.save(path+name_pred+'.h5')\n","    AE.save(path+name_AE+'.h5')\n","    AEC.save(path+name_AEC+'.h5')\n","    AEP.save(path+name_AEP+'.h5')\n","    print(\"Poids sauvegardes dans le Drive, dans le dossier Sauvegardes_poids/\"+bdd+\"/\"+model+\", sous le nom :\")\n","    print(name_enc+\".h5\")\n","    print(name_vec_c+\".h5\")\n","    print(name_vec_v+\".h5\")\n","    print(name_gen+\".h5\")\n","    print(name_dis+\".h5\")\n","    print(name_pred+\".h5\")\n","    print(name_AE+\".h5\")\n","    print(name_AEC+\".h5\")\n","    print(name_AEP+\".h5\")\n","\n","def restauration_poids(model, bdd, list_models, name_enc=\"encodeur\", name_gen=\"generator\", name_dis=\"discriminator\", name_gan=\"gan\", name_vec_c=\"vector_c\", name_vec_v=\"vector_v\", name_class=\"classifieur\", name_pred=\"predicteur\", name_AE=\"AE\", name_AEC=\"AEC\", name_AEP=\"AEP\"):\n","  path = 'drive/Sauvegardes_poids/'+bdd+'/'+model+'/'\n","  from keras.models import load_model\n","  if model == \"GMV\":\n","    generator, discriminator, gan = list_models\n","    \n","    generator = load_model(path+name_gen+'.h5')\n","    discriminator = load_model(path+name_dis+'.h5')\n","    gan = load_model(path+name_gan+'.h5')\n","    print(\"Poids restaures depuis le Drive, du dossier Sauvegardes_poids/\"+bdd+\"/\"+model+\", avec les noms :\")\n","    print(name_gen+\".h5\")\n","    print(name_dis+\".h5\")\n","    print(name_gan+\".h5\")\n","    return(generator, discriminator, gan)\n","  \n","  \n","  elif model == \"CGMV\":\n","    encodeur, generator, discriminator, gan = list_models\n","    \n","    encodeur = load_model(path+name_enc+'.h5')\n","    generator = load_model(path+name_gen+'.h5')\n","    discriminator = load_model(path+name_dis+'.h5')\n","    gan = load_model(path+name_gan+'.h5')\n","    print(\"Poids restaures depuis le Drive, du dossier Sauvegardes_poids/\"+bdd+\"/\"+model+\", avec les noms :\")\n","    print(name_enc+\".h5\")\n","    print(name_gen+\".h5\")\n","    print(name_dis+\".h5\")\n","    print(name_gan+\".h5\")\n","    return(encodeur, generator, discriminator, gan)\n","\n","  elif model == \"AECP\":\n","    encodeur, vector_c, vector_v, generator, discriminator, predicteur, AE, AEC, AEP = list_models\n","    \n","    encodeur = load_model(path+name_enc+'.h5')\n","    vector_c = load_model(path+name_vec_c+'.h5')\n","    vector_v = load_model(path+name_vec_v+'.h5')\n","    generator = load_model(path+name_gen+'.h5')\n","    discriminator = load_model(path+name_dis+'.h5')\n","    predicteur = load_model(path+name_pred+'.h5')\n","    AE = load_model(path+name_AE+'.h5')\n","    AEC = load_model(path+name_AEC+'.h5')\n","    AEP = load_model(path+name_AEP+'.h5')\n","    \n","    print(\"Poids restaures depuis le Drive, du dossier Sauvegardes_poids/\"+bdd+\"/\"+model+\", avec les noms :\")\n","    print(name_enc+\".h5\")\n","    print(name_vec_c+\".h5\")\n","    print(name_vec_v+\".h5\")\n","    print(name_gen+\".h5\")\n","    print(name_dis+\".h5\")\n","    print(name_pred+\".h5\")\n","    print(name_AE+\".h5\")\n","    print(name_AEC+\".h5\")\n","    print(name_AEP+\".h5\")\n","    return(encodeur, vector_c, vector_v, generator, discriminator, predicteur, AE, AEC ,AEP)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gJEq8znDiGio","colab_type":"text"},"cell_type":"markdown","source":["## Tests"]},{"metadata":{"id":"6byPBUkMiGiw","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#Affichage de 10 couples d'images generees\n","\n","def renormalise(bdd, image): #plt.imshow() veut des valeurs entre 0 et 1 et pas entre -1 et 1 lorsque les images sont en couleurs\n","  if bdd == \"MNIST\": #en noir et blanc\n","    return(image)\n","  elif bdd == \"3Dchairs\": #en couleurs\n","    return((image+1)/float(2))\n","\n","def plotGeneratedImages(model, bdd, list_models, examples=10, figsize=(15, 15)):\n","  \n","  if bdd == \"MNIST\":\n","    global dimensions_MNIST\n","    dim = dimensions_MNIST\n","  elif bdd == \"3Dchairs\":\n","    global dimensions_3Dchairs\n","    dim = dimensions_3Dchairs\n","    \n","  if model == \"GMV\":\n","    generator = list_models[0]\n","    vector_c = np.random.normal(0, 1, size=[examples,randomVectorSize])\n","    vector_v1 = np.random.normal(0, 1, size=[examples,randomVectorSize])\n","    vector_v2 = np.random.normal(0, 1, size=[examples,randomVectorSize])\n","    if dim[2] == 1:\n","      generatedImages_1 = generator.predict([vector_c,vector_v1]).reshape(examples, dim[0], dim[1])\n","      generatedImages_2 = generator.predict([vector_c,vector_v2]).reshape(examples, dim[0], dim[1])\n","    else:\n","      generatedImages_1 = generator.predict([vector_c,vector_v1]).reshape(examples, dim[0], dim[1], dim[2])\n","      generatedImages_2 = generator.predict([vector_c,vector_v2]).reshape(examples, dim[0], dim[1], dim[2])\n","    print(\"Couples d'images generees par le GMV :\")\n","    plt.figure(figsize=figsize)\n","    for i in range(examples):\n","        plt.subplot(examples, 2, 2*i+1)\n","        plt.imshow(renormalise(bdd, generatedImages_1[i]), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","        plt.subplot(examples, 2, 2*i+2)\n","        plt.imshow(renormalise(bdd, generatedImages_2[i]), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","    plt.tight_layout()\n","    plt.show()\n","    #plt.savefig('gan_generated_image.png')\n","    \n","    \n","  elif model == \"CGMV\":\n","    encodeur, generator = list_models[:2]\n","    vector_v1 = np.random.normal(0, 1, size=[examples,randomVectorSize])\n","    vector_v2 = np.random.normal(0, 1, size=[examples,randomVectorSize])\n","\n","    #choix des images\n","    img_x1=np.zeros((examples, dim[0], dim[1], dim[2]))\n","    for k in range(examples):\n","      indice1=rd.randint(0,len(X_train_2)-1)\n","      img_x1[k]=X_train_2[indice1][0]\n","\n","    #passage dans l'encodeur\n","    vector_c1=encodeur.predict(img_x1)\n","\n","    #passage dans le generateur\n","    generatedImages_1 = generator.predict([vector_c1,vector_v1]).reshape(examples, dim[0], dim[1])\n","    generatedImages_2 = generator.predict([vector_c1,vector_v2]).reshape(examples, dim[0], dim[1])\n","    print(\"image d'origine - image generee 1 - image generee 2\")\n","    plt.figure(figsize=figsize)\n","    for i in range(examples):\n","        plt.subplot(examples, 3, 3*i+1)\n","        plt.imshow(renormalise(bdd, img_x1[i].reshape((dim[0], dim[1]))), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","        plt.subplot(examples, 3, 3*i+2)\n","        plt.imshow(renormalise(bdd, generatedImages_1[i]), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","        plt.subplot(examples, 3, 3*i+3)\n","        plt.imshow(renormalise(bdd, generatedImages_2[i]), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","    plt.tight_layout()\n","    plt.show()\n","    #plt.savefig('gan_generated_image.png')\n","    \n","\n","  elif model == \"AECP\":\n","    encodeur, vector_c, vector_v, generator, discriminator, predicteur, AE, AEC, AEP = list_models\n","\n","    \n","    if bdd == \"MNIST\":\n","      indices = np.random.randint(0, len(X_train), examples)\n","      img_x = X_train[indices]\n","      class_x = y_train[indices]\n","      vector_random = np.random.normal(0, 1, size=[examples, randomVectorSize])\n","\n","      CV = encodeur.predict(img_x)\n","      C = vector_c.predict(CV)\n","      V = vector_v.predict(CV)\n","      img_AE = generator.predict([C,V])\n","      img_AEV = generator.predict([C, vector_random])\n","\n","      img_AE = img_AE.reshape((examples, dim[0], dim[1]))\n","      img_AEV = img_AEV.reshape((examples, dim[0], dim[1]))\n","      img_x = img_x.reshape((examples, dim[0], dim[1]))\n","\n","      print(\"Image d'origine - image generee avec AE - image generee avec variations\")\n","      plt.figure(figsize=figsize)\n","      for i in range(examples):\n","          plt.subplot(examples, 3, 3*i+1)\n","          plt.imshow(renormalise(bdd, img_x[i]))\n","          plt.axis('off')\n","          plt.subplot(examples, 3, 3*i+2)\n","          plt.imshow(renormalise(bdd, img_AE[i]))\n","          plt.axis('off')\n","          plt.subplot(examples, 3, 3*i+3)\n","          plt.imshow(renormalise(bdd, img_AEV[i]))\n","          plt.axis('off')\n","      plt.tight_layout()\n","      plt.show()\n","      \n","    elif bdd == \"3Dchairs\":\n","      \n","      global nb_classes_3Dchairs\n","      nb_classes = nb_classes_3Dchairs\n","      global nb_images_par_classes_3Dchairs\n","      nb_images_par_classe = nb_images_par_classes_3Dchairs\n","    \n","      class_x = np.random.randint(0, nb_classes, examples)\n","      imgs = np.random.randint(0, nb_images_par_classe, examples)\n","      images = [[class_x[g], imgs[g]] for g in range(examples)]\n","      img_x, class_x = bdd_chaises(bdd, images)\n","      \n","      vector_random = np.random.normal(0, 1, size=[examples, randomVectorSize])\n","\n","      CV = encodeur.predict(img_x)\n","      C = vector_c.predict(CV)\n","      V = vector_v.predict(CV)\n","      img_AE = generator.predict([C,V])\n","      img_AEV = generator.predict([C, vector_random])\n","\n","      print(\"Image d'origine - image generee avec AE - image generee avec variations\")\n","      plt.figure(figsize=figsize)\n","      for i in range(examples):\n","          plt.subplot(examples, 3, 3*i+1)\n","          plt.imshow(renormalise(bdd, img_x[i]))\n","          plt.axis('off')\n","          plt.subplot(examples, 3, 3*i+2)\n","          plt.imshow(renormalise(bdd, img_AE[i]))\n","          plt.axis('off')\n","          plt.subplot(examples, 3, 3*i+3)\n","          plt.imshow(renormalise(bdd, img_AEV[i]))\n","          plt.axis('off')\n","      plt.tight_layout()\n","      plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cMhyRigniGi2","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def tableau(model, bdd, list_models, examples=10, figsize=(10,10)):\n","  \n","  if bdd == \"MNIST\":\n","    global dimensions_MNIST\n","    dim = dimensions_MNIST\n","    global nb_classes_MNIST\n","    nb_classes = nb_classes_MNIST\n","    global X_train, Y_train\n","    X = []\n","    for n in range(nb_classes):\n","      X_0=X_train[np.where(Y_train==n)]\n","      X.append(X_0)\n","  elif bdd == \"3Dchairs\":\n","    global dimensions_3Dchairs\n","    dim = dimensions_3Dchairs\n","    global nb_classes_3Dchairs\n","    nb_classes = nb_classes_3Dchairs\n","    \n","  if model == \"GMV\":\n","    generator = list_models[0]\n","    vector_c = np.random.normal(0, 1, size=[examples,randomVectorSize])\n","    vector_v = np.random.normal(0, 1, size=[examples,randomVectorSize])\n","    plt.figure(figsize=figsize)\n","    for i in range(examples):\n","      C=np.zeros((examples,randomVectorSize))\n","      C[:]=vector_c[i]\n","      generatedImages=generator.predict([C,vector_v]).reshape(examples, dim[0], dim[1])\n","      for j in range(examples):\n","        plt.subplot(examples, examples, 10*i+j+1)\n","        plt.imshow(renormalise(bdd, generatedImages[j]), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","    plt.tight_layout()\n","    plt.show()\n","    \n","    \n","  elif model == \"CGMV\":\n","    encodeur, generator = list_models[:2]\n","\n","    #choix des images\n","    img_x=np.zeros((nb_classes,dim[0], dim[1], dim[2]))\n","    for v in range(nb_classes):\n","      img_x[v]=X[v][np.random.randint(0,len(X[v]))]\n","\n","    #generation des vecteurs c et v\n","    vector_c = encodeur.predict(img_x)\n","    vector_v = np.random.normal(0, 1, size=[examples,randomVectorSize])\n","    if nb_classes > examples:\n","      nb_classes = examples\n","\n","    plt.figure(figsize=figsize)\n","    for i in range(nb_classes):\n","      C=np.zeros((examples,randomVectorSize))\n","      C[:]=vector_c[i]\n","      generatedImages=generator.predict([C,vector_v]).reshape(examples, dim[0], dim[1])\n","      plt.subplot(nb_classes, examples+1, (examples+1)*i+1)\n","      plt.imshow(renormalise(bdd, img_x[i].reshape(dim[0], dim[1])))\n","      plt.axis('off')\n","      for j in range(examples):\n","        plt.subplot(nb_classes, examples+1, (examples+1)*i+j+2)\n","        plt.imshow(renormalise(bdd, generatedImages[j]), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","    plt.tight_layout()\n","    plt.show()\n","    \n","  elif model == \"AECP\":\n","    encodeur, vector_c, vector_v, generator, discriminator, predicteur, AE, AEC, AEP = list_models\n","\n","\n","    #choix des images\n","    img_x=np.zeros((nb_classes, dim[0], dim[1], dim[2]))\n","    for v in range(nb_classes):\n","      img_x[v]=X[v][np.random.randint(0,len(X[v]))]\n","\n","    #generation des vecteurs c et v\n","    CC = vector_c.predict(encodeur.predict(img_x))\n","    V = np.random.normal(0, 1, size=[examples,randomVectorSize])\n","    if nb_classes > examples:\n","      nb_classes = examples\n","\n","    plt.figure(figsize=figsize)\n","    for i in range(nb_classes):\n","      C=np.zeros((examples,randomVectorSizeC))\n","      C[:]=CC[i]\n","      generatedImages=generator.predict([C,V]).reshape(examples, dim[0], dim[1])\n","      plt.subplot(nb_classes, examples+1, (examples+1)*i+1)\n","      plt.imshow(renormalise(bdd, img_x[i].reshape(dim[0], dim[1])))\n","      plt.axis('off')\n","      for j in range(examples):\n","        plt.subplot(nb_classes, examples+1, (examples+1)*i+j+2)\n","        plt.imshow(renormalise(bdd, generatedImages[j]), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","    plt.tight_layout()\n","    plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2CI8IOvDiGi9","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#Test de l'accuracy du discriminateur et du generateur\n","\n","def test(model, bdd, list_models, X_test_2=[], X_test=[], y_test=[], seuil=0.45, nb_sorties=1, batchSize=100):\n","  \n","  if bdd == \"MNIST\":\n","    global dimensions_MNIST\n","    dim = dimensions_MNIST\n","    global nb_classes_MNIST\n","    nb_classes = nb_classes_MNIST\n","  elif bdd == \"3Dchairs\":\n","    global dimensions_3Dchairs\n","    dim = dimensions_3Dchairs\n","    global nb_classes_3Dchairs\n","    nb_classes = nb_classes_3Dchairs\n","  \n","  if model == \"GMV\":\n","    generator, discriminator, gan = list_models\n","    #generation des vecteurs de bruit\n","    vector_c = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","    vector_v1 = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","    vector_v2 = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","\n","    #passage dans le generateur\n","    generatedImages_1 = generator.predict([vector_c,vector_v1])\n","    generatedImages_2 = generator.predict([vector_c,vector_v2])\n","\n","    #test du discriminateur\n","    indices=np.random.randint(0, len(X_test_2), size=batchSize)\n","    imageBatch_1 = np.zeros((len(indices), dim[0], dim[1], dim[2]))\n","    imageBatch_2 = np.zeros((len(indices), dim[0], dim[1], dim[2]))\n","    for i in range(len(indices)):\n","      imageBatch_1[i]=X_test_2[indices[i]][0]\n","      imageBatch_2[i]=X_test_2[indices[i]][1]\n","    XDis1=np.concatenate((imageBatch_1,generatedImages_1))\n","    XDis2=np.concatenate((imageBatch_2,generatedImages_2))\n","    predictions = discriminator.predict([XDis1,XDis2])\n","    score_d=0\n","    if nb_sorties==1:\n","      for j in range(len(predictions)):\n","        if j<batchSize and predictions[j]>seuil:\n","          score_d+=1\n","        elif j>= batchSize and predictions[j]<seuil:\n","          score_d+=1\n","    elif nb_sorties==2:\n","      for j in range(len(predictions)):\n","        if j<batchSize and np.argmax(predictions[j])==0:\n","          score_d+=1\n","        elif j>= batchSize and np.argmax(predictions[j])==1:\n","          score_d+=1\n","    score_d=float(score_d)/len(predictions)\n","    print(\"Seuil = \",seuil)\n","    print(\"Score du discriminateur : \"+str(score_d*100)+\"%\")\n","\n","    #test du generateur\n","    predictions_g = gan.predict([vector_c,vector_v1,vector_v2])\n","    score_g=0\n","    if nb_sorties==1:\n","      for j in range(len(predictions_g)):\n","        if predictions_g[j]>seuil:\n","          score_g+=1\n","    elif nb_sorties==2:\n","      for j in range(len(predictions_g)):\n","        if np.argmax(predictions_g[j])==0:\n","          score_g+=1\n","    score_g=float(score_g)/len(predictions_g)\n","    print(\"Score du generateur : \"+str(score_g*100)+\"%\")\n","\n","    #Affichage d'exemples\n","    if dim[2] == 1:\n","      generatedImages_1.reshape(batchSize, dim[0], dim[1])      \n","      generatedImages_2.reshape(batchSize, dim[0], dim[1])\n","      imageBatch_1.reshape(batchSize, dim[0], dim[1])\n","      imageBatch_2.reshape(batchSize, dim[0], dim[1])\n","    else:\n","      generatedImages_1.reshape(batchSize, dim[0], dim[1], dim[2])      \n","      generatedImages_2.reshape(batchSize, dim[0], dim[1], dim[2])\n","      \n","    plt.figure(figsize=(10,10))\n","    for i in range(4):\n","        plt.subplot(8, 2, 2*i+1)\n","        plt.imshow(renormalise(bdd, generatedImages_1[i]), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","        plt.subplot(8, 2, 2*i+2)\n","        plt.imshow(renormalise(bdd, generatedImages_2[i]), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","    for i in range(4):\n","        plt.subplot(8, 2, 2*i+9)\n","        plt.imshow(renormalise(bdd, imageBatch_1[i]), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","        plt.subplot(8, 2, 2*i+10)\n","        plt.imshow(renormalise(bdd, imageBatch_2[i]), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","    plt.tight_layout()\n","    plt.show()\n","    print(\"Predictions pour les images generees : \",predictions[batchSize:batchSize+4])\n","    print(\"Predictions pour les images de \"+bdd+\" : \",predictions[:4])\n","    print(\"Predictions du gan : \", predictions_g[:4])\n","    \n","    \n","    \n","  elif model == \"CGMV\":\n","    encodeur, generator, discriminator, gan = list_models\n","\n","    #generation des vecteurs de bruit\n","    vector_v1 = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","    vector_v2 = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","    vector_v3 = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","\n","    #choix des images\n","    img_x1=np.zeros((batchSize, dim[0], dim[1], dim[2]))\n","    img_x2=np.zeros((batchSize, dim[0], dim[1], dim[2]))\n","    for k in range(batchSize):\n","      indice1=rd.randint(0,len(X_test_2)-1)\n","      #indice2=rd.randint(0,len(X_test_2)-1)\n","      img_x1[k]=X_test_2[indice1][0]\n","      img_x2[k]=X_test_2[indice1][1]\n","\n","    #passage dans l'encodeur\n","    vector_c1=encodeur.predict(img_x1)\n","    vector_c2=encodeur.predict(img_x2)\n","\n","    #passage dans le generateur\n","    generatedImages_1 = generator.predict([vector_c1,vector_v1])\n","    generatedImages_2 = generator.predict([vector_c1,vector_v2])\n","    generatedImages_3 = generator.predict([vector_c1,vector_v3])\n","\n","    #test du discriminateur\n","    \"\"\"indices=np.random.randint(0, len(X_test_2), size=batchSize)\n","    imageBatch_1 = np.zeros((batchSize, dim[0], dim[1], dim[2]))\n","    imageBatch_2 = np.zeros((batchSize, dim[0], dim[1], dim[2]))\n","    for i in range(len(indices)):\n","      imageBatch_1[i]=X_test_2[indices[i]][0]\n","      imageBatch_2[i]=X_test_2[indices[i]][1]\"\"\"\n","    XDis1=np.concatenate((img_x1,generatedImages_1,img_x1))\n","    XDis2=np.concatenate((img_x2,generatedImages_2,generatedImages_3))\n","    predictions = discriminator.predict([XDis1,XDis2])\n","    score_d=0\n","    for j in range(len(predictions)):\n","      if j<batchSize and predictions[j]>seuil:\n","        score_d+=1\n","      elif j>= batchSize and predictions[j]<seuil:\n","        score_d+=1\n","    score_d=float(score_d)/len(predictions)\n","    print(\"Seuil = \",seuil)\n","    print(\"Score du discriminateur : \"+str(score_d*100)+\"%\")\n","    moy=0\n","    for pred in predictions:\n","      moy+=pred\n","    moy=float(moy)/len(predictions)\n","    M=max(predictions)\n","    m=min(predictions)\n","    print(\"Moyenne : \",moy)\n","    print(\"Maximum : \",M)\n","    print(\"Minimum : \",m)\n","\n","    #test du generateur\n","    predictions_g = gan.predict([img_x1,img_x1,vector_v1,vector_v2,vector_v3])\n","    score_g=0\n","    #print(predictions_g)\n","    for j in range(len(predictions_g)):\n","      if predictions_g[j][0]>seuil:\n","        score_g+=1\n","      if predictions_g[j][1]>seuil:\n","        score_g+=1\n","    score_g=float(score_g)/(2*len(predictions_g))\n","    print(\"Score du generateur : \"+str(score_g*100)+\"%\")\n","\n","    #Affichage d'exemples\n","    plt.figure(figsize=(10,10))\n","    for i in range(4):\n","        plt.subplot(8, 2, 2*i+1)\n","        plt.imshow(np.reshape(renormalise(bdd, generatedImages_1[i]),(dim[0], dim[1])), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","        plt.subplot(8, 2, 2*i+2)\n","        plt.imshow(np.reshape(renormalise(bdd, generatedImages_2[i]),(dim[0], dim[1])), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","    for i in range(4):\n","        plt.subplot(8, 2, 2*i+9)\n","        plt.imshow(np.reshape(renormalise(bdd, img_x1[i]),(dim[0], dim[1])), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","        plt.subplot(8, 2, 2*i+10)\n","        plt.imshow(np.reshape(renormalise(bdd, img_x2[i]),(dim[0], dim[1])), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","    plt.tight_layout()\n","    plt.show()\n","    print(\"Predictions pour les images generees : \",predictions[batchSize:batchSize+4])\n","    print(\"Predictions pour les images de \"+bdd+\" : \",predictions[:4])\n","    print(\"Valeurs du gan : \", predictions_g[0][:4])\n","    print(predictions_g[1][:4])\n","    \n","  elif model == \"AECP\":\n","    encodeur, vector_c, vector_v, generator, discriminator, predicteur, AE, AEC, AEP = list_models\n","    \n","    N = len(X_test)\n","    img_x = X_test\n","    class_x = y_test\n","    vector_random = np.random.normal(0, 1, size=[N, randomVectorSize])\n","\n","    CV = encodeur.predict(img_x)\n","    C = vector_c.predict(CV)\n","    V = vector_v.predict(CV)\n","    img_AEV = generator.predict([C, vector_random])\n","    \n","    classes = np.zeros((N, 28, 28, 10))\n","    for k in range(N):\n","      classes[k,:,:] = class_x[k]\n","    \n","    Pred_T = discriminator.predict([img_x, classes])\n","    Pred_F = discriminator.predict([img_AEV, classes])\n","    \n","    score_dis = 0\n","    for k in range(N):\n","      if Pred_T[k] > seuil:\n","        score_dis += 1\n","      if Pred_F[k] < seuil:\n","        score_dis += 1\n","    score_dis = score_dis / float(2*N)\n","    print(\"Score du discriminateur : \"+str(100*score_dis)+\"%\")\n","    \n","    pred_AEC = AEC.predict([img_x, vector_random, classes])\n","    score_AEC = 0\n","    for k in range(N):\n","      if pred_AEC[k] > seuil:\n","        score_AEC += 1\n","    score_AEC = score_AEC / float(N)\n","    print(\"Score de AEC : \"+str(100*score_AEC)+\"%\")\n","    \n","    examples = 10\n","    print(\"Image d'origine - image generee avec variations\")\n","    plt.figure(figsize=(15,15))\n","    for i in range(examples):\n","      plt.subplot(examples, 2, 2*i+1)\n","      plt.imshow(renormalise(bdd, img_x[i].reshape((dim[0], dim[1]))), interpolation='nearest', cmap='gray_r')\n","      plt.axis('off')\n","      plt.subplot(examples, 2, 2*i+2)\n","      plt.imshow(renormalise(bdd, img_AEV[i].reshape((dim[0], dim[1]))), interpolation='nearest', cmap='gray_r')\n","      plt.axis('off')\n","    plt.tight_layout()\n","    plt.show()    \n","    \n","    for k in range(examples):\n","      print(\"Image n°\"+str(k+1)+\"; Score du discriminateur :\")\n","      print(\"True / Fake\")\n","      print(Pred_T[k], \" / \", Pred_F[k])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HElRAJCHiGjA","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def interpolation_lineaire(v1, v2, n=10):\n","  #n = nombre de points, v1 et v2 inclus\n","  vecteur=np.zeros((n,len(v1)))\n","  vecteur[0]=v1\n","  vecteur[n-1]=v2\n","  for p in range(1,n-1):\n","    x=np.zeros(len(v1))\n","    for q in range(len(v1)):\n","      x[q]=v1[q]+(float(p)/float(n-1))*(v2[q]-v1[q])\n","    vecteur[p]=x\n","  return(vecteur)\n","  \n","\n","def interpolation(model, bdd, list_models, n1, n2, examples=10, figsize=(10,10)):\n","  \n","  if bdd == \"MNIST\":\n","    global dimensions_MNIST\n","    global X_train, Y_train\n","    global nb_classes_MNIST\n","  \n","  if model == \"GMV\":\n","    print(\"pas d'interpolation pour ce modele :(\")\n","  elif model == \"CGMV\":\n","    encodeur, generator = list_models[:2]\n","    \n","    if bdd == \"MNIST\":\n","      dim = dimensions_MNIST\n","    elif bdd == \"3Dchairs\":\n","      global dimensions_3Dchairs\n","      dim = dimensions_3Dchairs\n","    \n","    if bdd == \"MNIST\":\n","\n","      nb_classes = nb_classes_MNIST\n","      X = []\n","      for n in range(nb_classes):\n","        X_0=X_train[np.where(Y_train==n)]\n","        X.append(X_0)\n","      k1=rd.randint(0, len(X[n1]))\n","      k2=rd.randint(0, len(X[n2]))\n","      img_x1=X[n1][k1]\n","      img_x2=X[n2][k2]\n","\n","      plt.figure(figsize=(3,3))\n","      plt.subplot(1,2,1)\n","      plt.imshow(img_x1.reshape(28,28))\n","      plt.axis('off')\n","      plt.subplot(1,2,2)\n","      plt.imshow(img_x2.reshape(28,28))\n","      plt.axis('off')\n","\n","\n","    #passage dans l'encodeur et interpolations\n","    images=np.zeros((2,dim[0], dim[1], dim[2]))\n","    images[0]=img_x1\n","    images[1]=img_x2\n","    vector_c1, vector_c2 = encodeur.predict(images)\n","    Vecteur_c=interpolation_lineaire(vector_c1, vector_c2, n=examples)\n","\n","    vector_v1 = np.random.normal(0, 1, size=randomVectorSize)\n","    vector_v2 = np.random.normal(0, 1, size=randomVectorSize)\n","    Vecteur_v = interpolation_lineaire(vector_v1, vector_v2, n=examples)\n","\n","    #Affichage\n","    plt.figure(figsize=figsize)\n","    for i in range(examples):\n","      C=np.zeros((examples,randomVectorSize))\n","      C[:]=Vecteur_c[i]\n","      generatedImages=generator.predict([C,Vecteur_v]).reshape(examples, dim[0], dim[1])\n","      for j in range(examples):\n","        plt.subplot(examples, examples, examples*i+j+1)\n","        plt.imshow(renormalise(bdd, generatedImages[j]), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","    plt.tight_layout()\n","    plt.show()\n","    \n","    \n","  elif model == \"AECP\":\n","    encodeur, vector_c, vector_v, generator, discriminator, predicteur, AE, AEC, AEP = list_models\n","    \n","  if bdd == \"MNIST\":\n","\n","    dim = dimensions_MNIST\n","\n","    nb_classes = nb_classes_MNIST\n","    X = []\n","    for n in range(nb_classes):\n","      X_0=X_train[np.where(Y_train==n)]\n","      X.append(X_0)\n","    k1=rd.randint(0, len(X[n1]))\n","    k2=rd.randint(0, len(X[n2]))\n","    img_x1=X[n1][k1]\n","    img_x2=X[n2][k2]\n","\n","    plt.figure(figsize=(3,3))\n","    plt.subplot(1,2,1)\n","    plt.imshow(img_x1.reshape(28,28))\n","    plt.axis('off')\n","    plt.subplot(1,2,2)\n","    plt.imshow(img_x2.reshape(28,28))\n","    plt.axis('off')\n","    \n","    #passage dans l'encodeur et interpolations    \n","    images=np.zeros((2,dim[0], dim[1], dim[2]))\n","    images[0]=img_x1\n","    images[1]=img_x2\n","    vector_c1, vector_c2 = vector_c.predict(encodeur.predict(images))\n","    Vecteur_c=interpolation_lineaire(vector_c1, vector_c2, n=examples)\n","\n","    vector_v1 = np.random.normal(0, 1, size=randomVectorSize)\n","    vector_v2 = np.random.normal(0, 1, size=randomVectorSize)\n","    Vecteur_v = interpolation_lineaire(vector_v1, vector_v2, n=examples)\n","    \n","    #Affichage\n","    plt.figure(figsize=figsize)\n","    for i in range(examples):\n","      C=np.zeros((examples,randomVectorSizeC))\n","      C[:]=Vecteur_c[i]\n","      generatedImages=generator.predict([C,Vecteur_v]).reshape(examples, dim[0], dim[1])\n","      for j in range(examples):\n","        plt.subplot(examples, examples, examples*i+j+1)\n","        plt.imshow(renormalise(bdd, generatedImages[j]), interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","    plt.tight_layout()\n","    plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FxDGailmlJWY","colab_type":"text"},"cell_type":"markdown","source":["# Entraînement"]},{"metadata":{"id":"IrT0p1B-dHNS","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["model = \"GMV\"\n","bdd = \"MNIST\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"DYASBEYViPqd","colab_type":"text"},"cell_type":"markdown","source":["**Génération de la base de données : utile pour GMV et CGMV uniquement (fait des paires d'images)**\n","\n","faire_des_paires(bdd, new=True, nb_paires=0, X_train_2=[], test=False)\n","\n","*   bdd = \"MNIST\" ou \"3Dchairs\"\n","*   new = True => X_train_2 va être entièrement créé avec le nombre de paires indiqué\n","*   new = False => X_train_2 va juste être réactualisé avec de nouvelles paires\n","*   nb_paires : seulement utile si new = True\n","*   X_train_2 : seulement utile si new = False\n","*   test = True => génère ou actualise X_test_2 (que pour MNIST)\n","\n","return(X_train_2)\n","    "]},{"metadata":{"id":"cvqikBcNiPqn","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["X_train_2 = faire_des_paires(bdd, new=True, nb_paires=40000, X_train_2=[], test=False) "],"execution_count":0,"outputs":[]},{"metadata":{"id":"sbUEfXS1N9nC","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#Affichage d'une image\n","plt.imshow(renormalise(bdd, X_train_2[1][0]))\n","plt.axis('off')\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LpC1PVzmiPrH","colab_type":"text"},"cell_type":"markdown","source":["**Initialisation du modèle**\n","\n","create_model(model, bdd, opt_G = adam_1, opt_D = adam_2, nb_sorties = 1, loss = 'mse')\n","\n","*   model = \"GMV\", \"CGMV\" ou \"AECP\"\n","*   bdd = \"MNIST\" ou \"3Dchairs\"\n","*   opt_G et opt_D : optimizers du générateur et du discriminateur\n","*   nb_sorties = 1 ou 2 (pour AECP : 1)\n","*   loss : la loss de tous les réseaux\n","\n","return((encodeur,) generator, discriminator, gan)"]},{"metadata":{"id":"o5igTfcmH578","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["list_models = create_model(model, bdd, opt_G = adam_1, opt_D = adam_2, opt_P = adam_3, nb_sorties = 1, loss = 'mse')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iIQKp985kNRq","colab_type":"text"},"cell_type":"markdown","source":["### **Restauration des poids des réseaux**"]},{"metadata":{"id":"B4TouS9piX5A","colab_type":"text"},"cell_type":"markdown","source":["restauration_poids(model, bdd, list_models, name_enc=\"encodeur\", name_gen=\"generator\", name_dis=\"discriminator\", name_gan=\"gan\")\n","\n","*   model = \"GMV\" ou \"CGMV\"\n","*   bdd = \"MNIST\" ou \"3Dchairs\"\n","*   list_models = [(encodeur,)generator,discriminator,gan]\n","*   name_enc = nom donné à la sauvegarde des poids de l'encodeur (ne pas mettre d'extension)\n","\n","return((encodeur,) generator, discriminator, gan)"]},{"metadata":{"id":"_kjGJa91QMG8","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["list_models = restauration_poids(model, bdd, list_models)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"X-84L8bUkTtk","colab_type":"text"},"cell_type":"markdown","source":["### **Entraînement**"]},{"metadata":{"id":"R-_qp4e_ieBj","colab_type":"text"},"cell_type":"markdown","source":["train(model, bdd, list_models, X_train_2, epochs=10, batchSize=128, epochs_regen=5, data_discriminator=False)\n","\n","*   model = \"GMV\", \"CGMV\" ou \"AECP\"\n","*   bdd = \"MNIST\" ou \"3Dchairs\"\n","*   list_models = [(encodeur,)generator,discriminator,gan]\n","*   epochs_regen = nombre d'epochs séparant chaque régénération de la base de données\n","*   data_discriminator = True => récupération des valeurs moyennes des prédictions du discriminateur\n","*  data_predicteur = True => récupération de l'erreur moyenne du prédicteur au cours de l'entraînement (valable que pour AECP)\n","\n","return(g_losses,d_losses(,d_moy_pos,d_moy_neg))\n","ou return(ae_losses, c_losses, aec_losses, aep_losses)"]},{"metadata":{"id":"NZZ0URZQlJx4","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["g_losses1, d_losses1 = train(model, bdd, list_models, X_train=X_train, y_train=y_train, epochs=3, batchSize=128, epochs_regen=5, nb_sorties=1, data_discriminator=True)    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"La8b6sTKPmIg","colab_type":"text"},"cell_type":"markdown","source":["### **Sauvegarde des poids**"]},{"metadata":{"id":"4R-4S-Xsik5f","colab_type":"text"},"cell_type":"markdown","source":["sauvegarde(model, bdd, list_models, name_enc=\"encodeur\", name_gen=\"generator\", name_dis=\"discriminator\", name_gan=\"gan\")\n","\n","*   model = \"GMV\", \"CGMV\" ou \"AECP\"\n","*   bdd = \"MNIST\" ou \"3Dchairs\"\n","*   list_models = [(encodeur,)generator,discriminator,gan]\n","*   name_enc = nom donné à la sauvegarde des poids de l'encodeur (ne pas mettre d'extension)\n"]},{"metadata":{"id":"XUBJKKM7OYN2","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["sauvegarde(model, bdd, list_models)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"V95IKh7DZ9AQ","colab_type":"text"},"cell_type":"markdown","source":["### **Changer le learning rate**"]},{"metadata":{"id":"xJGyfXiiiFgg","colab_type":"text"},"cell_type":"markdown","source":["change_lr(name, value)\n","\n","*   name = \"generator\", \"discriminator\" ou \"predicteur\"\n","*   value = valeur par laquelle le learning rate actuel va être multiplié (ex : 0.9)"]},{"metadata":{"id":"MASlSytpZ6a-","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#Discriminator\n","list_models = change_lr(model, list_models, \"discriminator\", 1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oR5s3ub6Wk6Q","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#Encodeur, generator\n","list_models = change_lr(model, list_models, \"encodeur\", 1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4OvRGNL9ccAu","colab_type":"text"},"cell_type":"markdown","source":["### Graphiques"]},{"metadata":{"id":"AgnUT6KVivS7","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#Lancer si c'est un nouvel entrainement\n","g_losses = g_losses1\n","d_losses = d_losses1"],"execution_count":0,"outputs":[]},{"metadata":{"id":"owjRw-f_ivTX","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#Lancer si c'est la suite d'un entraînement\n","g_losses = np.concatenate((g_losses, g_losses1))\n","d_losses = np.concatenate((d_losses, d_losses1))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pwK2o4KwPdg0","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["graph_loss(model, g_losses=g_losses, d_losses=d_losses)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YCitu3XVxHH8","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["graph_discriminator(d_moy_pos, d_moy_neg, 1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RX3ksEp5ZmFs","colab_type":"text"},"cell_type":"markdown","source":["# Tests"]},{"metadata":{"id":"cLKE8COlMZjA","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["plotGeneratedImages(model, bdd, list_models, examples=10, figsize=(15,15)) #Affichage de 10 couples d'images generees"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wlOyMFt8-ApO","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["X_test_2 = faire_des_paires(bdd, new=True, nb_paires=50, test=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eViWVvd8MmXa","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["test(model, bdd, list_models, X_test=X_test, y_test=y_test, seuil=0.45, batchSize=10, nb_sorties=1) # Calcul de l'accuracy du generateur et du discriminateur"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WIeL8T2-q_Ji","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["tableau(model, bdd, list_models)"],"execution_count":0,"outputs":[]}]}